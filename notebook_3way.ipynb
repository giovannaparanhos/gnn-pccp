{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0df9abff-b363-48fd-8cd4-873ce5a7145c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "from os.path import join, basename, splitext\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.transforms import RadiusGraph\n",
    "from skimage import io, img_as_float\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from torchvision import models\n",
    "import torch.nn as nn\n",
    "from random import shuffle\n",
    "from sklearn.metrics import roc_auc_score, f1_score, balanced_accuracy_score\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "from torch_geometric.data import Data\n",
    "from models import GraphNet, Preprocess, AttentionPool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fdb892be-fc61-43b4-befb-80b56c5f9d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions for visualization.\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "import networkx as nx\n",
    "\n",
    "def visualize_points(pos, edge_index=None, index=None, edge_weights=None, node_weights=None, show=True, img=None, patch_size=1, fig_text=None, box_color='palegreen'):\n",
    "    fig = plt.figure(figsize=(torch.max(pos, 0)[0].numpy()[0], torch.max(pos, 0)[0].numpy()[1]))\n",
    "        # fig_w, fig_h = plt.gcf().get_size_inches()\n",
    "    if edge_index is not None:\n",
    "        if edge_weights is None:\n",
    "            for (src, dst) in edge_index.t().tolist():\n",
    "                src = pos[src].tolist()\n",
    "                dst = pos[dst].tolist()\n",
    "                plt.plot([src[0]*patch_size+int(patch_size/2), dst[0]*patch_size+int(patch_size/2)], [src[1]*patch_size+int(patch_size/2), dst[1]*patch_size+int(patch_size/2)], linewidth=3, color='royalblue')\n",
    "        else:\n",
    "            i = 0\n",
    "            for (s, d) in edge_index.t().tolist():\n",
    "                src = pos[s].tolist()\n",
    "                dst = pos[d].tolist()\n",
    "                plt.plot([src[0]*patch_size+int(patch_size/2), dst[0]*patch_size+int(patch_size/2)], [src[1]*patch_size+int(patch_size/2), dst[1]*patch_size+int(patch_size/2)], linewidth=widths[i]*3, color='royalblue')\n",
    "                i+=1\n",
    "    if index is None:\n",
    "        if node_weights is not None:\n",
    "            for p, w in zip(pos, node_weights):\n",
    "                plt.scatter(p[0]*patch_size+int(patch_size/2), p[1]*patch_size+int(patch_size/2), s=w*500, zorder=1000, color='red')\n",
    "        else:\n",
    "            plt.scatter(pos[:, 0]*patch_size+int(patch_size/2), pos[:, 1]*patch_size+int(patch_size/2), s=500, zorder=1000, color='red')\n",
    "    else:\n",
    "        mask = torch.zeros(pos.size(0), dtype=torch.bool)\n",
    "        mask[index] = True\n",
    "        plt.scatter(pos[~mask, 0], pos[~mask, 1], s=50, color='lightgray', zorder=1000)\n",
    "        plt.scatter(pos[mask, 0], pos[mask, 1], s=50, zorder=1000)\n",
    "    plt.axis('off')\n",
    "    plt.gca().invert_yaxis()\n",
    "    if img is not None:\n",
    "        im = plt.imread(img)\n",
    "        plt.imshow(im, alpha=0.5)\n",
    "    if fig_text is not None:\n",
    "        plt.figtext(0.5, 0.1, fig_text, ha=\"center\", fontsize=18, bbox={\"facecolor\":box_color, \"alpha\":0.5, \"pad\":5} )\n",
    "    if show:\n",
    "        plt.show()\n",
    "    plt.close()\n",
    "    return fig\n",
    "    \n",
    "def visualize_projection(h, color):\n",
    "    z = TSNE(n_components=2).fit_transform(h.detach().cpu().numpy())\n",
    "\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "\n",
    "    plt.scatter(z[:, 0], z[:, 1], s=70, c=color, cmap=\"Set2\")\n",
    "    plt.show()\n",
    "    \n",
    "def visualize_grid(pos, color):\n",
    "    color = color.detach().cpu().numpy()\n",
    "    pos = pos.detach().cpu().numpy()\n",
    "    # plt.figure(figsize=(10,10))\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "\n",
    "    plt.scatter(pos[:, 0], pos[:, 1], s=70, c=color, cmap=\"Set2\")\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.show()\n",
    "    \n",
    "def visualize_graph(h, color, epoch=None, loss=None):\n",
    "    plt.figure(figsize=(7,7))\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "\n",
    "    if torch.is_tensor(h):\n",
    "        h = h.detach().cpu().numpy()\n",
    "        plt.scatter(h[:, 0], h[:, 1], s=140, c=color, cmap=\"Set2\")\n",
    "        if epoch is not None and loss is not None:\n",
    "            plt.xlabel(f'Epoch: {epoch}, Loss: {loss.item():.4f}', fontsize=16)\n",
    "    else:\n",
    "        nx.draw_networkx(h, pos=nx.spring_layout(h, seed=42), with_labels=False,\n",
    "                         node_color=color, cmap=\"Set2\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "38d25aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_dataset_to_graph(csv, class_codes, num_feat=(1, 513), compute_edges=False, weight_func=cosine_similarity, radius=1.5, remove_empty=False):\n",
    "    df = pd.read_csv(csv)\n",
    "    feat_arr = np.array(df.iloc[:, num_feat[0]:num_feat[1]])\n",
    "    pos_arr = np.array(df.iloc[:, -3:-1])\n",
    "    label_arr = np.array(class_codes.get(df.iloc[0, -1]))\n",
    "\n",
    "    # if remove_empty and os.path.splitext(os.path.basename(csv))[0]:\n",
    "    #     empty_idx = np.where( np.sum(feat_arr, 1) == 0)[0]\n",
    "    #     feat_arr[empty_idx, :] = 0\n",
    "    #     if len(empty_idx)>0: pos_arr[empty_idx, :] = pos_arr[empty_idx[0], :]\n",
    "\n",
    "    data = Data(x=torch.tensor(feat_arr, dtype=torch.float), pos=torch.tensor(pos_arr, dtype=torch.long), y=torch.tensor(label_arr, dtype=torch.long))\n",
    "    radius_graph = RadiusGraph(radius, loop=True)\n",
    "    data = radius_graph(data) # 1.5 or 2\n",
    "    if compute_edges:\n",
    "        weights = []\n",
    "        for i in range(data.edge_index.shape[1]):\n",
    "            edge = data.edge_index[:, i]\n",
    "            weight = weight_func( data.x[edge[0]].view(1, -1), data.x[edge[1]].view(1, -1) )\n",
    "            weights.append(weight)\n",
    "        # weights = exposure.rescale_intensity( np.vstack(weights).squeeze(), in_range=(0.5, 1), out_range=(0, 1) )\n",
    "        weights = np.vstack(weights).squeeze()\n",
    "        data.edge_weight = torch.tensor(weights, dtype=torch.float)\n",
    "    data.name = os.path.splitext(os.path.basename(csv))[0]\n",
    "    data.label = df.iloc[0, -1]\n",
    "    return data\n",
    "\n",
    "def compute_dataset(embedder, path, class_codes, ext='png', opt_folder=False):\n",
    "    classes = glob(join(path, '*'))\n",
    "    # print(classes) \n",
    "    data_list = [] # pos, feats, node labels, node_mask, graph label\n",
    "    # return None\n",
    "    for idx, c in enumerate(classes):\n",
    "        class_name = basename(c)\n",
    "        if opt_folder:\n",
    "            regions = glob(join(path, class_name, '*', '*'))\n",
    "        else:\n",
    "            regions = glob(join(path, class_name, '*'))\n",
    "        regions = [x for x in regions if os.path.isdir(x)]\n",
    "        for region in tqdm(regions):\n",
    "            try:\n",
    "                pos_list = []\n",
    "                feat_list = []\n",
    "                node_list = []\n",
    "                mask_list = []\n",
    "                patches = glob(join(region, '*.'+ext))\n",
    "                for patch in patches:\n",
    "                    ### patch pos -> x, y\n",
    "                    patch_data = img_as_float(io.imread(patch))\n",
    "\n",
    "                    # feat = np.mean(patch_data)\n",
    "                    with torch.no_grad():\n",
    "                        patch_tensor = torch.FloatTensor(patch_data.transpose(2, 0, 1))[None, :].cuda()\n",
    "                        feat_tensor = embedder(patch_tensor)\n",
    "                        feat = feat_tensor.detach().cpu().numpy().squeeze()\n",
    "                    masked = False\n",
    "                    node_label = np.nan\n",
    "\n",
    "                    x = int(splitext(basename(patch))[0].split('_')[0])\n",
    "                    y = int(splitext(basename(patch))[0].split('_')[1])\n",
    "                    pos_list.append((x, y))\n",
    "                    feat_list.append(feat)  \n",
    "                    node_list.append(node_label)\n",
    "                    mask_list.append(masked)\n",
    "                pos_arr = np.vstack(pos_list)\n",
    "                feat_arr = np.vstack(feat_list)\n",
    "                node_arr = np.vstack(node_list)\n",
    "                mask_arr = np.vstack(mask_list)\n",
    "                graph_label = np.array(class_codes.get(class_name))\n",
    "                graph_name = region\n",
    "                data_list.append((pos_arr, feat_arr, node_arr, mask_arr, graph_label, graph_name))\n",
    "            except:\n",
    "                print(region)\n",
    "    return data_list\n",
    "\n",
    "def sample_weights(graph_train):\n",
    "    n_sample = []\n",
    "    for graph in graph_train:\n",
    "        n_sample.append(graph.y.numpy())\n",
    "    n_sample = np.asarray(n_sample)\n",
    "    _, counts = np.unique(n_sample, return_counts=True)\n",
    "    counts = counts.max() / (10e-3+counts)\n",
    "    return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bd710312-a6b0-4d71-aff4-bd597820fe2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "distributed_codes = {\n",
    "    'normal': 0,\n",
    "    'pancreatitis': 1,\n",
    "    'pdac': 2\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a669b6",
   "metadata": {},
   "source": [
    "### Compute representations of patches of every ROI, save them as csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5851a6b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_path = 'data_bags/TMA_v2'\n",
    "patch_ext = 'png'\n",
    "resnet = models.resnet18(pretrained=True)\n",
    "resnet.fc = nn.Identity()\n",
    "resnet = resnet.cuda()\n",
    "resnet.eval()\n",
    "data_list = compute_dataset(resnet, bag_path, distributed_codes, ext='png')\n",
    "out_path = 'dataframes/TMA_v2'\n",
    "for data in tqdm(data_list):\n",
    "    pos = data[0]\n",
    "    feat = data[1]\n",
    "    label = data[5].split(os.sep)[-2]\n",
    "    name = os.sep.join(data[5].split(os.sep)[-2:])\n",
    "    save_name = os.path.join(out_path, name)\n",
    "    os.makedirs(os.sep.join(save_name.split(os.sep)[:-1]), exist_ok=True)\n",
    "    df = pd.DataFrame(data=feat)\n",
    "    df = df.assign(pos_x=pd.Series(pos[:, 0]).values)\n",
    "    df = df.assign(pos_y=pd.Series(pos[:, 1]).values)\n",
    "    df = df.assign(label=pd.Series([label]*feat.shape[0]).values)\n",
    "    df.to_csv(save_name+'.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e2144da",
   "metadata": {},
   "source": [
    "### Read representations of ROIs from csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3e5e2c6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 786/786 [02:15<00:00,  5.81it/s]\n"
     ]
    }
   ],
   "source": [
    "csv_path = 'dataframes/TMA'  \n",
    "num_feat = (1, 513)\n",
    "csvs = glob(os.path.join(csv_path, '*', '*.csv'))\n",
    "train_graph_list = []\n",
    "fov_normal = []\n",
    "fov_pancreatitis = []\n",
    "fov_pdac = []\n",
    "for csv in tqdm(csvs):\n",
    "    data = read_dataset_to_graph(csv, distributed_codes, num_feat, compute_edges=True, weight_func=cosine_similarity, radius=1.5)\n",
    "    train_graph_list.append(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27182867",
   "metadata": {},
   "source": [
    "### Core level separation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d6a1355d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 936x864 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_fold = 4\n",
    "while(True):\n",
    "    weight_mean = max(sample_weights(train_graph_list))\n",
    "    shuffle(train_graph_list)\n",
    "    cores_per_chunk = int(len(train_graph_list)/num_fold)\n",
    "    chunks_data = [train_graph_list[i*cores_per_chunk:(i+1)*cores_per_chunk] for i in range(num_fold)]\n",
    "    weights = []\n",
    "    for chunk_data in chunks_data:\n",
    "        weights.append(sample_weights(chunk_data))\n",
    "    max_weight = max([max(i) for i in weights])\n",
    "    if max_weight<weight_mean*1.2: break\n",
    "fig = visualize_points(train_graph_list[0].pos, edge_index=train_graph_list[0].edge_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "d2db9d53-7fce-43b2-adb2-48ca6a1295fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(loader):\n",
    "    model.train()\n",
    "    loss_avg = 0\n",
    "    count = 0\n",
    "    for data in loader: \n",
    "        optimizer.zero_grad()  \n",
    "        out = model(\n",
    "            data.x.float().cuda(), \n",
    "            edge_index=data.edge_index.cuda(), \n",
    "            batch=torch.LongTensor(np.zeros(data.x.shape[0])).cuda(), \n",
    "            edge_weight=data.edge_weight.squeeze().cuda() if data.edge_weight is not None else None,\n",
    "            )  \n",
    "        loss = criterion(out, data.y.long().cuda().view(1))  \n",
    "        loss.backward()  \n",
    "        optimizer.step() \n",
    "        loss_avg += loss.item()\n",
    "        count += 1\n",
    "    return loss_avg / count\n",
    "\n",
    "def test(loader):\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        labels = []\n",
    "        preds = []\n",
    "        outs = []\n",
    "        one_hots = []\n",
    "        loss_avg = 0\n",
    "        for data in loader: \n",
    "            out = model(\n",
    "                data.x.float().cuda(), \n",
    "                edge_index=data.edge_index.long().cuda(), \n",
    "                batch=torch.LongTensor(np.zeros(data.x.shape[0])).cuda(), \n",
    "                edge_weight=data.edge_weight.squeeze().cuda() if data.edge_weight is not None else None,\n",
    "                )\n",
    "            outs.append(nn.functional.softmax(out, dim=1).cpu().numpy())\n",
    "            one_hot = np.zeros((3))\n",
    "            one_hot[data.y.long().numpy()]=1\n",
    "            # one_hot = data.y.numpy().squeeze()\n",
    "            one_hots.append(one_hot)  \n",
    "            loss = criterion(out, data.y.long().cuda().view(1))  # Compute the loss.\n",
    "            preds.append(torch.argmax(out).cpu().numpy().squeeze())\n",
    "            labels.append(data.y.numpy().squeeze())\n",
    "            # labels.append(np.argmax(data.y.numpy()).squeeze())\n",
    "            loss_avg += loss.item()\n",
    "        preds = np.array(preds)\n",
    "        labels = np.array(labels)\n",
    "        outs = np.array(outs).squeeze()\n",
    "        one_hots = np.array(one_hots).squeeze()\n",
    "        # print(one_hots.shape)\n",
    "        # print(outs.shape)\n",
    "        aucs = roc_auc_score(one_hots, outs, average=None, multi_class='ovo')\n",
    "        avg_acc = balanced_accuracy_score(labels, preds)\n",
    "        avg_f1 = f1_score(labels, preds, average='weighted')\n",
    "    return loss_avg / len(loader), avg_acc, avg_f1, aucs  # Derive ratio of correct predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "1a2bfcff-147b-49dc-9a57-05963a3cca25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Preprocess(\n",
       "  (lin1): Sequential(\n",
       "    (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (1): PReLU(num_parameters=1)\n",
       "  )\n",
       "  (layers): Sequential(\n",
       "    (0): Sequential(\n",
       "      (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (1): PReLU(num_parameters=1)\n",
       "    )\n",
       "  )\n",
       "  (lin2): Sequential(\n",
       "    (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (1): PReLU(num_parameters=1)\n",
       "  )\n",
       "  (classifier): Linear(in_features=512, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_epochs = 150\n",
    "weight_decay = 0.0005\n",
    "weights_name = 'MLP_TMA_3way'\n",
    "weights_path = 'weights_cv'\n",
    "os.makedirs('weights_cv', exist_ok=True)\n",
    "model = GraphNet(\n",
    "        in_channels=512, \n",
    "        out_channels=3, \n",
    "        hidden_channels=512, \n",
    "        gcn_layer='GCN', \n",
    "        graph_pool='mean', \n",
    "        drop_p0=0.0,\n",
    "        drop_p1=0.25, \n",
    "        preprocess=Preprocess(in_channels=512, hidden_channels=512, out_channels=512, n_layers=0),\n",
    "        )\n",
    "model = Preprocess(in_channels=512, hidden_channels=512, out_channels=512, out_class=3, n_layers=1, head=True)\n",
    "lr = 0.0005\n",
    "model = model.cuda()\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "af793c0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running cross-validation fold: 00\n",
      "Epoch: 001, Train Loss: 0.7545, Test Loss: 0.7888,  Train Acc: 0.6120, Test Acc: 0.6418, Test f1: 0.5920, AUC: class-0>>0.9331638755980861|class-1>>0.6663148172582134|class-2>>0.9252316984275748\n",
      "Saved model!\n",
      "Epoch: 002, Train Loss: 0.5495, Test Loss: 0.6742,  Train Acc: 0.6743, Test Acc: 0.7020, Test f1: 0.7339, AUC: class-0>>0.9451255980861243|class-1>>0.7680432774772398|class-2>>0.937936061647402\n",
      "Saved model!\n",
      "Epoch: 003, Train Loss: 0.4707, Test Loss: 0.5634,  Train Acc: 0.7269, Test Acc: 0.7135, Test f1: 0.7281, AUC: class-0>>0.944377990430622|class-1>>0.8561815542947618|class-2>>0.9449130480058314\n",
      "Saved model!\n",
      "Epoch: 004, Train Loss: 0.4575, Test Loss: 0.5661,  Train Acc: 0.7246, Test Acc: 0.7235, Test f1: 0.7500, AUC: class-0>>0.9452751196172249|class-1>>0.8498482649426046|class-2>>0.9488701447464335\n",
      "Saved model!\n",
      "Epoch: 005, Train Loss: 0.4154, Test Loss: 0.4663,  Train Acc: 0.7942, Test Acc: 0.8009, Test f1: 0.8368, AUC: class-0>>0.9529007177033493|class-1>>0.8683203588863967|class-2>>0.95261897323753\n",
      "Saved model!\n",
      "Epoch: 006, Train Loss: 0.4191, Test Loss: 0.4909,  Train Acc: 0.7595, Test Acc: 0.7549, Test f1: 0.7861, AUC: class-0>>0.9514055023923446|class-1>>0.8584245942736508|class-2>>0.9558471311048631\n",
      "Epoch: 007, Train Loss: 0.4199, Test Loss: 0.5573,  Train Acc: 0.7550, Test Acc: 0.7285, Test f1: 0.7602, AUC: class-0>>0.947218899521531|class-1>>0.8683203588863966|class-2>>0.9577215453504114\n",
      "Epoch: 008, Train Loss: 0.4773, Test Loss: 0.6466,  Train Acc: 0.7413, Test Acc: 0.6757, Test f1: 0.6774, AUC: class-0>>0.9415370813397129|class-1>>0.8811188811188813|class-2>>0.9567843382276373\n",
      "Epoch: 009, Train Loss: 0.3961, Test Loss: 0.5225,  Train Acc: 0.7785, Test Acc: 0.7540, Test f1: 0.7932, AUC: class-0>>0.9505083732057417|class-1>>0.8544662884285525|class-2>>0.9591794230969488\n",
      "Epoch: 010, Train Loss: 0.3671, Test Loss: 0.4723,  Train Acc: 0.7784, Test Acc: 0.7830, Test f1: 0.8144, AUC: class-0>>0.951555023923445|class-1>>0.8849452434358095|class-2>>0.9605331667187338\n",
      "Epoch: 011, Train Loss: 0.3613, Test Loss: 0.4838,  Train Acc: 0.7958, Test Acc: 0.7654, Test f1: 0.7933, AUC: class-0>>0.9502093301435406|class-1>>0.8832299775696002|class-2>>0.9592835572217016\n",
      "Epoch: 012, Train Loss: 0.3623, Test Loss: 0.4457,  Train Acc: 0.8262, Test Acc: 0.7843, Test f1: 0.8139, AUC: class-0>>0.952153110047847|class-1>>0.8936535162950257|class-2>>0.9605331667187337\n",
      "Epoch: 013, Train Loss: 0.4056, Test Loss: 0.4544,  Train Acc: 0.8212, Test Acc: 0.7962, Test f1: 0.8312, AUC: class-0>>0.954994019138756|class-1>>0.8794036152526719|class-2>>0.9592835572217016\n",
      "Epoch: 014, Train Loss: 0.3334, Test Loss: 0.4533,  Train Acc: 0.8254, Test Acc: 0.7717, Test f1: 0.8006, AUC: class-0>>0.954096889952153|class-1>>0.8984034833091437|class-2>>0.9626158492137873\n",
      "Epoch: 015, Train Loss: 0.3410, Test Loss: 0.4707,  Train Acc: 0.8009, Test Acc: 0.7528, Test f1: 0.7799, AUC: class-0>>0.952153110047847|class-1>>0.897611822140124|class-2>>0.9641778610850775\n",
      "Epoch: 016, Train Loss: 0.3733, Test Loss: 0.4717,  Train Acc: 0.8213, Test Acc: 0.7680, Test f1: 0.7913, AUC: class-0>>0.9526016746411482|class-1>>0.8841535822667899|class-2>>0.9612621055920025\n",
      "Epoch: 017, Train Loss: 0.3920, Test Loss: 0.5186,  Train Acc: 0.7867, Test Acc: 0.7414, Test f1: 0.7804, AUC: class-0>>0.9503588516746411|class-1>>0.8584245942736509|class-2>>0.965635738831615\n",
      "Epoch: 018, Train Loss: 0.3682, Test Loss: 0.5312,  Train Acc: 0.7932, Test Acc: 0.7365, Test f1: 0.7540, AUC: class-0>>0.9502093301435406|class-1>>0.9063200949993403|class-2>>0.9687597625741955\n",
      "Epoch: 019, Train Loss: 0.4180, Test Loss: 0.4581,  Train Acc: 0.8391, Test Acc: 0.8282, Test f1: 0.8439, AUC: class-0>>0.9548444976076554|class-1>>0.895500725689405|class-2>>0.9678225554514214\n",
      "Saved model!\n",
      "Epoch: 020, Train Loss: 0.3694, Test Loss: 0.4875,  Train Acc: 0.8207, Test Acc: 0.7651, Test f1: 0.8150, AUC: class-0>>0.9542464114832536|class-1>>0.8817785987597309|class-2>>0.9703217744454857\n",
      "Epoch: 021, Train Loss: 0.3654, Test Loss: 0.4848,  Train Acc: 0.8080, Test Acc: 0.7693, Test f1: 0.7892, AUC: class-0>>0.9503588516746412|class-1>>0.8903549280907772|class-2>>0.9678225554514215\n",
      "Epoch: 022, Train Loss: 0.3168, Test Loss: 0.4185,  Train Acc: 0.8487, Test Acc: 0.7943, Test f1: 0.8278, AUC: class-0>>0.9524521531100479|class-1>>0.9084311914500593|class-2>>0.971883786316776\n",
      "Epoch: 023, Train Loss: 0.3228, Test Loss: 0.4907,  Train Acc: 0.8098, Test Acc: 0.7691, Test f1: 0.8029, AUC: class-0>>0.9509569377990431|class-1>>0.8964243303865945|class-2>>0.9709465791940018\n",
      "Epoch: 024, Train Loss: 0.3026, Test Loss: 0.4179,  Train Acc: 0.8621, Test Acc: 0.8074, Test f1: 0.8329, AUC: class-0>>0.9540968899521531|class-1>>0.9166116901965959|class-2>>0.9716755180672707\n",
      "Epoch: 025, Train Loss: 0.3106, Test Loss: 0.4433,  Train Acc: 0.8408, Test Acc: 0.7780, Test f1: 0.8072, AUC: class-0>>0.9485645933014354|class-1>>0.9106742314289483|class-2>>0.9711548474435072\n",
      "Epoch: 026, Train Loss: 0.3011, Test Loss: 0.4691,  Train Acc: 0.8328, Test Acc: 0.7944, Test f1: 0.8154, AUC: class-0>>0.9524521531100478|class-1>>0.9027576197387519|class-2>>0.9709465791940018\n",
      "Epoch: 027, Train Loss: 0.3355, Test Loss: 0.4857,  Train Acc: 0.8200, Test Acc: 0.7690, Test f1: 0.8141, AUC: class-0>>0.9523026315789473|class-1>>0.8787438976118221|class-2>>0.970634176819744\n",
      "Epoch: 028, Train Loss: 0.2886, Test Loss: 0.4770,  Train Acc: 0.8372, Test Acc: 0.7730, Test f1: 0.7990, AUC: class-0>>0.9534988038277512|class-1>>0.9193825042881647|class-2>>0.9732375299385609\n",
      "Epoch: 029, Train Loss: 0.2928, Test Loss: 0.4365,  Train Acc: 0.8496, Test Acc: 0.7793, Test f1: 0.8051, AUC: class-0>>0.951555023923445|class-1>>0.9081673043937195|class-2>>0.9724044569405393\n",
      "Epoch: 030, Train Loss: 0.3294, Test Loss: 0.4568,  Train Acc: 0.8308, Test Acc: 0.7798, Test f1: 0.7976, AUC: class-0>>0.9534988038277512|class-1>>0.9193825042881647|class-2>>0.972508591065292\n",
      "Epoch: 031, Train Loss: 0.3125, Test Loss: 0.4668,  Train Acc: 0.8163, Test Acc: 0.7679, Test f1: 0.8042, AUC: class-0>>0.9481160287081339|class-1>>0.9106742314289484|class-2>>0.9710507133187546\n",
      "Epoch: 032, Train Loss: 0.2906, Test Loss: 0.4190,  Train Acc: 0.8591, Test Acc: 0.7917, Test f1: 0.8299, AUC: class-0>>0.9514055023923446|class-1>>0.9080353608655496|class-2>>0.9723003228157867\n",
      "Epoch: 033, Train Loss: 0.3012, Test Loss: 0.5218,  Train Acc: 0.8071, Test Acc: 0.7680, Test f1: 0.7900, AUC: class-0>>0.9536483253588517|class-1>>0.9156880854994062|class-2>>0.9732375299385608\n",
      "Epoch: 034, Train Loss: 0.3377, Test Loss: 0.5652,  Train Acc: 0.8136, Test Acc: 0.7611, Test f1: 0.8058, AUC: class-0>>0.9499102870813397|class-1>>0.8700356247526059|class-2>>0.9716755180672706\n",
      "Epoch: 035, Train Loss: 0.3427, Test Loss: 0.6422,  Train Acc: 0.8173, Test Acc: 0.7440, Test f1: 0.7766, AUC: class-0>>0.9466208133971292|class-1>>0.8922021374851563|class-2>>0.9717796521920233\n",
      "Epoch: 036, Train Loss: 0.2753, Test Loss: 0.4281,  Train Acc: 0.8620, Test Acc: 0.8011, Test f1: 0.8272, AUC: class-0>>0.9533492822966507|class-1>>0.9195144478163346|class-2>>0.9736540664375715\n",
      "Epoch: 037, Train Loss: 0.3328, Test Loss: 0.4135,  Train Acc: 0.8701, Test Acc: 0.8248, Test f1: 0.8597, AUC: class-0>>0.955293062200957|class-1>>0.925451906583982|class-2>>0.9750078100593564\n",
      "Saved model!\n",
      "Epoch: 038, Train Loss: 0.2981, Test Loss: 0.5468,  Train Acc: 0.7956, Test Acc: 0.7403, Test f1: 0.7666, AUC: class-0>>0.949013157894737|class-1>>0.9145005937458768|class-2>>0.9751119441841092\n",
      "Epoch: 039, Train Loss: 0.2733, Test Loss: 0.4643,  Train Acc: 0.8555, Test Acc: 0.7640, Test f1: 0.8072, AUC: class-0>>0.9517045454545454|class-1>>0.9020979020979021|class-2>>0.9741747370613351\n",
      "Epoch: 040, Train Loss: 0.3470, Test Loss: 0.6214,  Train Acc: 0.7995, Test Acc: 0.7580, Test f1: 0.7715, AUC: class-0>>0.9505083732057416|class-1>>0.9191186172318249|class-2>>0.9733416640633135\n",
      "Epoch: 041, Train Loss: 0.2526, Test Loss: 0.4301,  Train Acc: 0.8680, Test Acc: 0.7843, Test f1: 0.8135, AUC: class-0>>0.9545454545454545|class-1>>0.9250560759994723|class-2>>0.9756326148078726\n",
      "Epoch: 042, Train Loss: 0.3267, Test Loss: 0.4475,  Train Acc: 0.8345, Test Acc: 0.8003, Test f1: 0.8226, AUC: class-0>>0.9555921052631579|class-1>>0.9039451114922813|class-2>>0.9768822243049048\n",
      "Epoch: 043, Train Loss: 0.2448, Test Loss: 0.4308,  Train Acc: 0.8687, Test Acc: 0.7793, Test f1: 0.8058, AUC: class-0>>0.9527511961722488|class-1>>0.9287504947882306|class-2>>0.9763615536811414\n",
      "Epoch: 044, Train Loss: 0.3058, Test Loss: 0.4042,  Train Acc: 0.8638, Test Acc: 0.8209, Test f1: 0.8598, AUC: class-0>>0.9531997607655502|class-1>>0.9230769230769231|class-2>>0.9770904925544102\n",
      "Epoch: 045, Train Loss: 0.3623, Test Loss: 0.4517,  Train Acc: 0.7996, Test Acc: 0.7557, Test f1: 0.8083, AUC: class-0>>0.950956937799043|class-1>>0.9051326032458108|class-2>>0.976778090180152\n",
      "Epoch: 046, Train Loss: 0.3285, Test Loss: 0.6141,  Train Acc: 0.8177, Test Acc: 0.7196, Test f1: 0.7575, AUC: class-0>>0.9472188995215312|class-1>>0.8933896292386858|class-2>>0.9729251275643028\n",
      "Epoch: 047, Train Loss: 0.3467, Test Loss: 0.4584,  Train Acc: 0.8555, Test Acc: 0.8039, Test f1: 0.8183, AUC: class-0>>0.9593301435406698|class-1>>0.8890354928090778|class-2>>0.9710507133187546\n",
      "Epoch: 048, Train Loss: 0.2751, Test Loss: 0.4529,  Train Acc: 0.8615, Test Acc: 0.8016, Test f1: 0.8198, AUC: class-0>>0.9539473684210527|class-1>>0.9158200290275762|class-2>>0.97709049255441\n",
      "Epoch: 049, Train Loss: 0.2317, Test Loss: 0.4727,  Train Acc: 0.8639, Test Acc: 0.7793, Test f1: 0.8062, AUC: class-0>>0.9520035885167464|class-1>>0.9220213748515635|class-2>>0.9756326148078726\n",
      "Epoch: 050, Train Loss: 0.2623, Test Loss: 0.4789,  Train Acc: 0.8499, Test Acc: 0.7633, Test f1: 0.7893, AUC: class-0>>0.9488636363636364|class-1>>0.9192505607599947|class-2>>0.9753202124336144\n",
      "Epoch: 051, Train Loss: 0.3151, Test Loss: 0.6520,  Train Acc: 0.8283, Test Acc: 0.7348, Test f1: 0.7678, AUC: class-0>>0.9493122009569378|class-1>>0.8994590315345032|class-2>>0.9758408830573779\n",
      "Epoch: 052, Train Loss: 0.2388, Test Loss: 0.4540,  Train Acc: 0.8901, Test Acc: 0.8182, Test f1: 0.8428, AUC: class-0>>0.9520035885167465|class-1>>0.9155561419712362|class-2>>0.9747995418098511\n",
      "Epoch: 053, Train Loss: 0.2419, Test Loss: 0.5009,  Train Acc: 0.8700, Test Acc: 0.7664, Test f1: 0.8144, AUC: class-0>>0.9455741626794258|class-1>>0.8977437656682938|class-2>>0.9756326148078726\n",
      "Epoch: 054, Train Loss: 0.2024, Test Loss: 0.4191,  Train Acc: 0.8993, Test Acc: 0.7846, Test f1: 0.8176, AUC: class-0>>0.9506578947368421|class-1>>0.9230769230769231|class-2>>0.9775070290534208\n",
      "Epoch: 055, Train Loss: 0.2375, Test Loss: 0.6195,  Train Acc: 0.8590, Test Acc: 0.7578, Test f1: 0.7892, AUC: class-0>>0.9464712918660287|class-1>>0.9105422879007785|class-2>>0.9756326148078726\n",
      "Epoch: 056, Train Loss: 0.2227, Test Loss: 0.4064,  Train Acc: 0.8907, Test Acc: 0.8072, Test f1: 0.8430, AUC: class-0>>0.9460227272727273|class-1>>0.9204380525135242|class-2>>0.9775070290534209\n",
      "Epoch: 057, Train Loss: 0.2051, Test Loss: 0.4544,  Train Acc: 0.9041, Test Acc: 0.8035, Test f1: 0.8357, AUC: class-0>>0.9515550239234449|class-1>>0.9148964243303865|class-2>>0.9760491513068832\n",
      "Epoch: 058, Train Loss: 0.2122, Test Loss: 0.5817,  Train Acc: 0.8715, Test Acc: 0.7793, Test f1: 0.8058, AUC: class-0>>0.9500598086124402|class-1>>0.9156880854994063|class-2>>0.9764656878058939\n",
      "Epoch: 059, Train Loss: 0.2984, Test Loss: 0.4893,  Train Acc: 0.8543, Test Acc: 0.7705, Test f1: 0.8206, AUC: class-0>>0.9419856459330144|class-1>>0.9080353608655496|class-2>>0.9776111631781735\n",
      "Epoch: 060, Train Loss: 0.1945, Test Loss: 0.4059,  Train Acc: 0.9219, Test Acc: 0.7985, Test f1: 0.8297, AUC: class-0>>0.9503588516746412|class-1>>0.9272991159783612|class-2>>0.9769863584296574\n",
      "Epoch: 061, Train Loss: 0.2289, Test Loss: 0.4716,  Train Acc: 0.8759, Test Acc: 0.7956, Test f1: 0.8176, AUC: class-0>>0.9530502392344498|class-1>>0.923208866605093|class-2>>0.9780276996771842\n",
      "Epoch: 062, Train Loss: 0.1754, Test Loss: 0.4938,  Train Acc: 0.9215, Test Acc: 0.7922, Test f1: 0.8232, AUC: class-0>>0.9502093301435407|class-1>>0.9123894972951576|class-2>>0.9753202124336146\n",
      "Epoch: 063, Train Loss: 0.1849, Test Loss: 0.4928,  Train Acc: 0.9066, Test Acc: 0.7809, Test f1: 0.8101, AUC: class-0>>0.9476674641148325|class-1>>0.9229449795487532|class-2>>0.9762574195563887\n",
      "Epoch: 064, Train Loss: 0.2402, Test Loss: 0.5545,  Train Acc: 0.8775, Test Acc: 0.8032, Test f1: 0.8226, AUC: class-0>>0.9512559808612441|class-1>>0.913972819633197|class-2>>0.9765698219306467\n",
      "Epoch: 065, Train Loss: 0.1718, Test Loss: 0.4652,  Train Acc: 0.9150, Test Acc: 0.7998, Test f1: 0.8288, AUC: class-0>>0.9491626794258373|class-1>>0.925583850112152|class-2>>0.9769863584296574\n",
      "Epoch: 066, Train Loss: 0.2021, Test Loss: 0.6142,  Train Acc: 0.8794, Test Acc: 0.7535, Test f1: 0.8005, AUC: class-0>>0.9413875598086126|class-1>>0.895236838633065|class-2>>0.9727689263771737\n",
      "Epoch: 067, Train Loss: 0.1773, Test Loss: 0.5651,  Train Acc: 0.9082, Test Acc: 0.7725, Test f1: 0.8081, AUC: class-0>>0.9431818181818181|class-1>>0.9080353608655496|class-2>>0.9749036759346037\n",
      "Epoch: 068, Train Loss: 0.3056, Test Loss: 0.6021,  Train Acc: 0.8575, Test Acc: 0.7668, Test f1: 0.8160, AUC: class-0>>0.9360047846889953|class-1>>0.90697981264019|class-2>>0.9779235655524315\n",
      "Epoch: 069, Train Loss: 0.2124, Test Loss: 0.4505,  Train Acc: 0.9256, Test Acc: 0.8123, Test f1: 0.8411, AUC: class-0>>0.9458732057416267|class-1>>0.9199102784008444|class-2>>0.9775070290534208\n",
      "Epoch: 070, Train Loss: 0.1701, Test Loss: 0.4808,  Train Acc: 0.9114, Test Acc: 0.7809, Test f1: 0.8100, AUC: class-0>>0.9425837320574163|class-1>>0.916215859612086|class-2>>0.9754243465583672\n",
      "Epoch: 071, Train Loss: 0.1474, Test Loss: 0.5671,  Train Acc: 0.9282, Test Acc: 0.7595, Test f1: 0.7929, AUC: class-0>>0.9418361244019139|class-1>>0.9098825702599288|class-2>>0.9758929501197543\n",
      "Epoch: 072, Train Loss: 0.1552, Test Loss: 0.4619,  Train Acc: 0.9332, Test Acc: 0.8114, Test f1: 0.8444, AUC: class-0>>0.940938995215311|class-1>>0.9295421559572503|class-2>>0.978756638550453\n",
      "Epoch: 073, Train Loss: 0.3338, Test Loss: 0.7945,  Train Acc: 0.8662, Test Acc: 0.7671, Test f1: 0.7918, AUC: class-0>>0.9407894736842105|class-1>>0.886396622245679|class-2>>0.9779235655524315\n",
      "Epoch: 074, Train Loss: 0.1474, Test Loss: 0.5867,  Train Acc: 0.9242, Test Acc: 0.7754, Test f1: 0.8100, AUC: class-0>>0.9395933014354066|class-1>>0.9109381184852883|class-2>>0.9765698219306467\n",
      "Epoch: 075, Train Loss: 0.1515, Test Loss: 0.5094,  Train Acc: 0.9287, Test Acc: 0.7918, Test f1: 0.8206, AUC: class-0>>0.9392942583732058|class-1>>0.9167436337247659|class-2>>0.9761532854316359\n",
      "Epoch: 076, Train Loss: 0.1424, Test Loss: 0.5360,  Train Acc: 0.9345, Test Acc: 0.7671, Test f1: 0.7983, AUC: class-0>>0.9461722488038278|class-1>>0.9180630690064653|class-2>>0.9755284806831198\n",
      "Epoch: 077, Train Loss: 0.1848, Test Loss: 0.7561,  Train Acc: 0.8928, Test Acc: 0.7485, Test f1: 0.7932, AUC: class-0>>0.9395933014354068|class-1>>0.8894313233935875|class-2>>0.9752160783088618\n",
      "Epoch: 078, Train Loss: 0.1899, Test Loss: 0.4800,  Train Acc: 0.9154, Test Acc: 0.8189, Test f1: 0.8561, AUC: class-0>>0.9400418660287082|class-1>>0.925715793640322|class-2>>0.9783401020514423\n",
      "Epoch: 079, Train Loss: 0.1391, Test Loss: 0.5019,  Train Acc: 0.9391, Test Acc: 0.8106, Test f1: 0.8392, AUC: class-0>>0.94063995215311|class-1>>0.9226810924924133|class-2>>0.9777152973029262\n",
      "Epoch: 080, Train Loss: 0.1859, Test Loss: 0.6869,  Train Acc: 0.8821, Test Acc: 0.7659, Test f1: 0.7849, AUC: class-0>>0.9376495215311005|class-1>>0.9130492149360074|class-2>>0.9750078100593564\n",
      "Epoch: 081, Train Loss: 0.1229, Test Loss: 0.5927,  Train Acc: 0.9306, Test Acc: 0.7783, Test f1: 0.8118, AUC: class-0>>0.9385466507177033|class-1>>0.9096186832035889|class-2>>0.9747995418098511\n",
      "Epoch: 082, Train Loss: 0.1170, Test Loss: 0.5521,  Train Acc: 0.9478, Test Acc: 0.7909, Test f1: 0.8241, AUC: class-0>>0.9419856459330144|class-1>>0.914104763161367|class-2>>0.9752160783088618\n",
      "Epoch: 083, Train Loss: 0.1118, Test Loss: 0.6124,  Train Acc: 0.9450, Test Acc: 0.7846, Test f1: 0.8175, AUC: class-0>>0.9413875598086123|class-1>>0.9197783348726745|class-2>>0.9774549619910444\n",
      "Epoch: 084, Train Loss: 0.1195, Test Loss: 0.7588,  Train Acc: 0.9399, Test Acc: 0.7569, Test f1: 0.7952, AUC: class-0>>0.9380980861244019|class-1>>0.9003826362316928|class-2>>0.9744871394355931\n",
      "Epoch: 085, Train Loss: 0.1063, Test Loss: 0.6625,  Train Acc: 0.9519, Test Acc: 0.7758, Test f1: 0.8134, AUC: class-0>>0.9398923444976076|class-1>>0.9123894972951576|class-2>>0.9762053524940123\n",
      "Epoch: 086, Train Loss: 0.1123, Test Loss: 0.5928,  Train Acc: 0.9396, Test Acc: 0.7976, Test f1: 0.8325, AUC: class-0>>0.9382476076555024|class-1>>0.9158200290275762|class-2>>0.9772987608039155\n",
      "Epoch: 087, Train Loss: 0.1482, Test Loss: 0.7441,  Train Acc: 0.9025, Test Acc: 0.7806, Test f1: 0.8049, AUC: class-0>>0.9400418660287081|class-1>>0.9195144478163346|class-2>>0.9760491513068832\n",
      "Epoch: 088, Train Loss: 0.0935, Test Loss: 0.7314,  Train Acc: 0.9570, Test Acc: 0.7758, Test f1: 0.8140, AUC: class-0>>0.9385466507177033|class-1>>0.9125214408233275|class-2>>0.9775070290534208\n",
      "Epoch: 089, Train Loss: 0.0955, Test Loss: 0.6739,  Train Acc: 0.9610, Test Acc: 0.7758, Test f1: 0.8142, AUC: class-0>>0.9351076555023923|class-1>>0.9150283678585566|class-2>>0.9782359679266895\n",
      "Epoch: 090, Train Loss: 0.0927, Test Loss: 0.5960,  Train Acc: 0.9649, Test Acc: 0.7905, Test f1: 0.8211, AUC: class-0>>0.9363038277511961|class-1>>0.9177991819501252|class-2>>0.9759450171821306\n",
      "Epoch: 091, Train Loss: 0.1018, Test Loss: 0.8046,  Train Acc: 0.9506, Test Acc: 0.7464, Test f1: 0.7885, AUC: class-0>>0.9375|class-1>>0.8951048951048951|class-2>>0.9733937311256899\n",
      "Epoch: 092, Train Loss: 0.0933, Test Loss: 0.7245,  Train Acc: 0.9685, Test Acc: 0.7532, Test f1: 0.7874, AUC: class-0>>0.9331638755980861|class-1>>0.9075075867528697|class-2>>0.9752681453712382\n",
      "Epoch: 093, Train Loss: 0.0905, Test Loss: 0.7368,  Train Acc: 0.9601, Test Acc: 0.7637, Test f1: 0.7939, AUC: class-0>>0.9345095693779903|class-1>>0.9079034173373797|class-2>>0.9742788711860877\n",
      "Epoch: 094, Train Loss: 0.1240, Test Loss: 0.8043,  Train Acc: 0.9289, Test Acc: 0.7704, Test f1: 0.8028, AUC: class-0>>0.9318181818181818|class-1>>0.9060562079430003|class-2>>0.9744871394355931\n",
      "Epoch: 095, Train Loss: 0.0829, Test Loss: 0.6899,  Train Acc: 0.9570, Test Acc: 0.7908, Test f1: 0.8240, AUC: class-0>>0.9348086124401914|class-1>>0.920833883098034|class-2>>0.9775070290534209\n",
      "Epoch: 096, Train Loss: 0.0854, Test Loss: 0.6589,  Train Acc: 0.9661, Test Acc: 0.7720, Test f1: 0.8071, AUC: class-0>>0.9328648325358851|class-1>>0.9196463913445045|class-2>>0.9756326148078726\n",
      "Epoch: 097, Train Loss: 0.0783, Test Loss: 0.7020,  Train Acc: 0.9697, Test Acc: 0.7716, Test f1: 0.8030, AUC: class-0>>0.930023923444976|class-1>>0.9050006597176408|class-2>>0.9728209934395502\n",
      "Epoch: 098, Train Loss: 0.0768, Test Loss: 0.8472,  Train Acc: 0.9697, Test Acc: 0.7548, Test f1: 0.7894, AUC: class-0>>0.9324162679425837|class-1>>0.9014381844570525|class-2>>0.9719879204415286\n",
      "Epoch: 099, Train Loss: 0.0617, Test Loss: 0.7879,  Train Acc: 0.9749, Test Acc: 0.7740, Test f1: 0.8122, AUC: class-0>>0.9313696172248804|class-1>>0.9119936667106479|class-2>>0.975684681870249\n",
      "Epoch: 100, Train Loss: 0.0923, Test Loss: 0.9479,  Train Acc: 0.9606, Test Acc: 0.7577, Test f1: 0.8015, AUC: class-0>>0.9300239234449761|class-1>>0.8956326692175749|class-2>>0.9716234510048943\n",
      "Epoch: 101, Train Loss: 0.0625, Test Loss: 0.7670,  Train Acc: 0.9761, Test Acc: 0.7895, Test f1: 0.8244, AUC: class-0>>0.9288277511961722|class-1>>0.9105422879007785|class-2>>0.9755284806831199\n",
      "Epoch: 102, Train Loss: 0.0596, Test Loss: 0.8373,  Train Acc: 0.9744, Test Acc: 0.7745, Test f1: 0.8051, AUC: class-0>>0.9331638755980861|class-1>>0.9123894972951576|class-2>>0.9763615536811414\n",
      "Epoch: 103, Train Loss: 0.0601, Test Loss: 0.9128,  Train Acc: 0.9721, Test Acc: 0.7523, Test f1: 0.7918, AUC: class-0>>0.927781100478469|class-1>>0.9032853938514316|class-2>>0.9701135061959805\n",
      "Epoch: 104, Train Loss: 0.0594, Test Loss: 0.8339,  Train Acc: 0.9728, Test Acc: 0.7594, Test f1: 0.7960, AUC: class-0>>0.9280801435406699|class-1>>0.9100145137880987|class-2>>0.9735499323128189\n",
      "Epoch: 105, Train Loss: 0.0566, Test Loss: 0.8685,  Train Acc: 0.9736, Test Acc: 0.7670, Test f1: 0.8008, AUC: class-0>>0.9264354066985647|class-1>>0.9090909090909091|class-2>>0.9742788711860877\n",
      "Epoch: 106, Train Loss: 0.0713, Test Loss: 0.8198,  Train Acc: 0.9689, Test Acc: 0.7854, Test f1: 0.8153, AUC: class-0>>0.9286782296650717|class-1>>0.9102784008444386|class-2>>0.9743830053108404\n",
      "Epoch: 107, Train Loss: 0.0495, Test Loss: 0.8691,  Train Acc: 0.9816, Test Acc: 0.7678, Test f1: 0.8063, AUC: class-0>>0.9276315789473685|class-1>>0.9076395302810396|class-2>>0.9718317192543997\n",
      "Epoch: 108, Train Loss: 0.0583, Test Loss: 1.0504,  Train Acc: 0.9673, Test Acc: 0.7687, Test f1: 0.8020, AUC: class-0>>0.9283791866028709|class-1>>0.9027576197387519|class-2>>0.9719879204415287\n",
      "Epoch: 109, Train Loss: 0.0541, Test Loss: 0.9569,  Train Acc: 0.9765, Test Acc: 0.7728, Test f1: 0.8123, AUC: class-0>>0.9267344497607656|class-1>>0.9088929937986542|class-2>>0.9724565240029158\n",
      "Epoch: 110, Train Loss: 0.0701, Test Loss: 0.9693,  Train Acc: 0.9654, Test Acc: 0.7748, Test f1: 0.8169, AUC: class-0>>0.9238935406698564|class-1>>0.9041430267845362|class-2>>0.9729771946266792\n",
      "Epoch: 111, Train Loss: 0.0436, Test Loss: 0.9746,  Train Acc: 0.9821, Test Acc: 0.7740, Test f1: 0.8119, AUC: class-0>>0.9244916267942582|class-1>>0.9044728856049611|class-2>>0.9731333958138082\n",
      "Epoch: 112, Train Loss: 0.0461, Test Loss: 0.9877,  Train Acc: 0.9840, Test Acc: 0.7602, Test f1: 0.8004, AUC: class-0>>0.9247906698564593|class-1>>0.8995909750626732|class-2>>0.9705300426949912\n",
      "Epoch: 113, Train Loss: 0.0436, Test Loss: 1.0250,  Train Acc: 0.9869, Test Acc: 0.7698, Test f1: 0.8107, AUC: class-0>>0.9240430622009569|class-1>>0.8999868056471829|class-2>>0.9714151827553889\n",
      "Epoch: 114, Train Loss: 0.0549, Test Loss: 0.9880,  Train Acc: 0.9706, Test Acc: 0.7697, Test f1: 0.8097, AUC: class-0>>0.9213516746411483|class-1>>0.9063860667634254|class-2>>0.9706862438821202\n",
      "Epoch: 115, Train Loss: 0.0470, Test Loss: 1.0966,  Train Acc: 0.9784, Test Acc: 0.7674, Test f1: 0.8031, AUC: class-0>>0.9280801435406699|class-1>>0.9034173373796015|class-2>>0.9721441216286577\n",
      "Epoch: 116, Train Loss: 0.0372, Test Loss: 0.9975,  Train Acc: 0.9888, Test Acc: 0.7707, Test f1: 0.8077, AUC: class-0>>0.924043062200957|class-1>>0.9048027444253859|class-2>>0.9716234510048943\n",
      "Epoch: 117, Train Loss: 0.0408, Test Loss: 1.1649,  Train Acc: 0.9740, Test Acc: 0.7661, Test f1: 0.8044, AUC: class-0>>0.9279306220095693|class-1>>0.904538857369046|class-2>>0.9726127251900448\n",
      "Epoch: 118, Train Loss: 0.0472, Test Loss: 1.1603,  Train Acc: 0.9833, Test Acc: 0.7698, Test f1: 0.8103, AUC: class-0>>0.9300239234449761|class-1>>0.9013062409288826|class-2>>0.9742788711860877\n",
      "Epoch: 119, Train Loss: 0.0377, Test Loss: 1.0898,  Train Acc: 0.9832, Test Acc: 0.7598, Test f1: 0.7987, AUC: class-0>>0.9267344497607656|class-1>>0.9055284338303207|class-2>>0.9722482557534105\n",
      "Epoch: 120, Train Loss: 0.0414, Test Loss: 1.1570,  Train Acc: 0.9853, Test Acc: 0.7803, Test f1: 0.8169, AUC: class-0>>0.9267344497607656|class-1>>0.8983375115450587|class-2>>0.9736540664375716\n",
      "Epoch: 121, Train Loss: 0.0344, Test Loss: 1.1540,  Train Acc: 0.9908, Test Acc: 0.7502, Test f1: 0.7875, AUC: class-0>>0.924043062200957|class-1>>0.9007784668162028|class-2>>0.970946579194002\n",
      "Epoch: 122, Train Loss: 0.0319, Test Loss: 1.1114,  Train Acc: 0.9908, Test Acc: 0.7615, Test f1: 0.8011, AUC: class-0>>0.9222488038277512|class-1>>0.9055284338303207|class-2>>0.9721441216286577\n",
      "Epoch: 123, Train Loss: 0.0369, Test Loss: 1.1771,  Train Acc: 0.9853, Test Acc: 0.7803, Test f1: 0.8171, AUC: class-0>>0.926584928229665|class-1>>0.9036152526718564|class-2>>0.9740185358742061\n",
      "Epoch: 124, Train Loss: 0.0301, Test Loss: 1.1392,  Train Acc: 0.9928, Test Acc: 0.7489, Test f1: 0.7885, AUC: class-0>>0.9244916267942584|class-1>>0.9015041562211373|class-2>>0.971727585129647\n",
      "Epoch: 125, Train Loss: 0.0305, Test Loss: 1.1254,  Train Acc: 0.9893, Test Acc: 0.7866, Test f1: 0.8232, AUC: class-0>>0.9220992822966507|class-1>>0.9053964903021506|class-2>>0.9723003228157867\n",
      "Epoch: 126, Train Loss: 0.0280, Test Loss: 1.1047,  Train Acc: 0.9908, Test Acc: 0.7770, Test f1: 0.8132, AUC: class-0>>0.9244916267942584|class-1>>0.9080353608655495|class-2>>0.9724044569405395\n",
      "Epoch: 127, Train Loss: 0.0277, Test Loss: 1.2207,  Train Acc: 0.9912, Test Acc: 0.7803, Test f1: 0.8174, AUC: class-0>>0.9222488038277512|class-1>>0.8991291727140784|class-2>>0.9719358533791524\n",
      "Epoch: 128, Train Loss: 0.0311, Test Loss: 1.2206,  Train Acc: 0.9880, Test Acc: 0.7578, Test f1: 0.7927, AUC: class-0>>0.9232954545454546|class-1>>0.9030874785591767|class-2>>0.9693325002603352\n",
      "Epoch: 129, Train Loss: 0.0293, Test Loss: 1.2024,  Train Acc: 0.9893, Test Acc: 0.7803, Test f1: 0.8171, AUC: class-0>>0.9238935406698565|class-1>>0.9007124950521177|class-2>>0.9726647922524211\n",
      "Epoch: 130, Train Loss: 0.0253, Test Loss: 1.1830,  Train Acc: 0.9932, Test Acc: 0.7678, Test f1: 0.8067, AUC: class-0>>0.9238935406698564|class-1>>0.9071117561683599|class-2>>0.9724565240029157\n",
      "Epoch: 131, Train Loss: 0.0248, Test Loss: 1.1374,  Train Acc: 0.9928, Test Acc: 0.7707, Test f1: 0.8077, AUC: class-0>>0.9238935406698564|class-1>>0.906913840876105|class-2>>0.9721961886910341\n",
      "Epoch: 132, Train Loss: 0.0248, Test Loss: 1.2778,  Train Acc: 0.9928, Test Acc: 0.7552, Test f1: 0.7950, AUC: class-0>>0.9218002392344498|class-1>>0.9018340150415622|class-2>>0.9711548474435072\n",
      "Epoch: 133, Train Loss: 0.0228, Test Loss: 1.2262,  Train Acc: 0.9928, Test Acc: 0.7678, Test f1: 0.8059, AUC: class-0>>0.9238935406698564|class-1>>0.9028895632669218|class-2>>0.9722482557534105\n",
      "Epoch: 134, Train Loss: 0.0232, Test Loss: 1.2095,  Train Acc: 0.9928, Test Acc: 0.7615, Test f1: 0.8011, AUC: class-0>>0.9225478468899521|class-1>>0.9053964903021507|class-2>>0.9710507133187546\n",
      "Epoch: 135, Train Loss: 0.0335, Test Loss: 1.2254,  Train Acc: 0.9916, Test Acc: 0.7929, Test f1: 0.8284, AUC: class-0>>0.9232954545454545|class-1>>0.904670800897216|class-2>>0.9731333958138082\n",
      "Epoch: 136, Train Loss: 0.0227, Test Loss: 1.2479,  Train Acc: 0.9928, Test Acc: 0.7628, Test f1: 0.7999, AUC: class-0>>0.9241925837320574|class-1>>0.9051985750098958|class-2>>0.971571383942518\n",
      "Epoch: 137, Train Loss: 0.0209, Test Loss: 1.2283,  Train Acc: 0.9928, Test Acc: 0.7615, Test f1: 0.8011, AUC: class-0>>0.9240430622009569|class-1>>0.9071777279324449|class-2>>0.9723523898781631\n",
      "Epoch: 138, Train Loss: 0.0217, Test Loss: 1.2265,  Train Acc: 0.9936, Test Acc: 0.7803, Test f1: 0.8177, AUC: class-0>>0.923145933014354|class-1>>0.9061881514711703|class-2>>0.9726127251900448\n",
      "Epoch: 139, Train Loss: 0.0202, Test Loss: 1.2497,  Train Acc: 0.9932, Test Acc: 0.7615, Test f1: 0.8007, AUC: class-0>>0.9235944976076556|class-1>>0.9051985750098958|class-2>>0.9718317192543997\n",
      "Epoch: 140, Train Loss: 0.0213, Test Loss: 1.2430,  Train Acc: 0.9956, Test Acc: 0.7803, Test f1: 0.8177, AUC: class-0>>0.9232954545454546|class-1>>0.9061881514711704|class-2>>0.9732895970009372\n",
      "Epoch: 141, Train Loss: 0.0198, Test Loss: 1.2841,  Train Acc: 0.9932, Test Acc: 0.7740, Test f1: 0.8119, AUC: class-0>>0.9238935406698565|class-1>>0.9033513656155165|class-2>>0.9726647922524212\n",
      "Epoch: 142, Train Loss: 0.0196, Test Loss: 1.2611,  Train Acc: 0.9928, Test Acc: 0.7615, Test f1: 0.8011, AUC: class-0>>0.9229964114832536|class-1>>0.9062541232352552|class-2>>0.9720920545662813\n",
      "Epoch: 143, Train Loss: 0.0198, Test Loss: 1.2518,  Train Acc: 0.9956, Test Acc: 0.7803, Test f1: 0.8177, AUC: class-0>>0.9231459330143541|class-1>>0.9061881514711704|class-2>>0.9722482557534105\n",
      "Epoch: 144, Train Loss: 0.0196, Test Loss: 1.2816,  Train Acc: 0.9956, Test Acc: 0.7803, Test f1: 0.8177, AUC: class-0>>0.923744019138756|class-1>>0.9042749703127062|class-2>>0.9726647922524212\n",
      "Epoch: 145, Train Loss: 0.0190, Test Loss: 1.2527,  Train Acc: 0.9928, Test Acc: 0.7615, Test f1: 0.8011, AUC: class-0>>0.9231459330143541|class-1>>0.9061881514711704|class-2>>0.9719358533791524\n",
      "Epoch: 146, Train Loss: 0.0186, Test Loss: 1.2699,  Train Acc: 0.9932, Test Acc: 0.7615, Test f1: 0.8011, AUC: class-0>>0.9232954545454546|class-1>>0.9059902361789154|class-2>>0.9719879204415287\n",
      "Epoch: 147, Train Loss: 0.0185, Test Loss: 1.2756,  Train Acc: 0.9956, Test Acc: 0.7740, Test f1: 0.8122, AUC: class-0>>0.924043062200957|class-1>>0.9057263491225755|class-2>>0.9727168593147975\n",
      "Epoch: 148, Train Loss: 0.0191, Test Loss: 1.2644,  Train Acc: 0.9956, Test Acc: 0.7803, Test f1: 0.8177, AUC: class-0>>0.9231459330143541|class-1>>0.9055284338303206|class-2>>0.9725085910652922\n",
      "Epoch: 149, Train Loss: 0.0181, Test Loss: 1.2681,  Train Acc: 0.9956, Test Acc: 0.7740, Test f1: 0.8122, AUC: class-0>>0.9219497607655502|class-1>>0.9053964903021507|class-2>>0.9721961886910341\n",
      "Running cross-validation fold: 01\n",
      "Epoch: 001, Train Loss: 0.6526, Test Loss: 0.5329,  Train Acc: 0.6612, Test Acc: 0.6426, Test f1: 0.7248, AUC: class-0>>0.967013888888889|class-1>>0.7640978872169022|class-2>>0.9741532047941637\n",
      "Saved model!\n",
      "Epoch: 002, Train Loss: 0.7881, Test Loss: 0.7019,  Train Acc: 0.6423, Test Acc: 0.7227, Test f1: 0.7531, AUC: class-0>>0.9712873931623931|class-1>>0.7148502811977503|class-2>>0.9749869723814487\n",
      "Saved model!\n",
      "Epoch: 003, Train Loss: 0.5047, Test Loss: 0.4074,  Train Acc: 0.7256, Test Acc: 0.7846, Test f1: 0.8267, AUC: class-0>>0.9814369658119658|class-1>>0.9053047575619394|class-2>>0.9769671704012507\n",
      "Saved model!\n",
      "Epoch: 004, Train Loss: 0.5402, Test Loss: 0.4555,  Train Acc: 0.7064, Test Acc: 0.7288, Test f1: 0.7926, AUC: class-0>>0.9801014957264957|class-1>>0.8277853777169782|class-2>>0.9773840541948933\n",
      "Epoch: 005, Train Loss: 0.6015, Test Loss: 0.4891,  Train Acc: 0.6752, Test Acc: 0.6743, Test f1: 0.7530, AUC: class-0>>0.9817040598290598|class-1>>0.8116735066119471|class-2>>0.9817613340281397\n",
      "Epoch: 006, Train Loss: 0.4728, Test Loss: 0.3897,  Train Acc: 0.7631, Test Acc: 0.7949, Test f1: 0.8417, AUC: class-0>>0.984775641025641|class-1>>0.8790089679282566|class-2>>0.9830119854090672\n",
      "Saved model!\n",
      "Epoch: 007, Train Loss: 0.4887, Test Loss: 0.4158,  Train Acc: 0.7613, Test Acc: 0.7814, Test f1: 0.8336, AUC: class-0>>0.982905982905983|class-1>>0.8749050007599939|class-2>>0.9805106826472121\n",
      "Epoch: 008, Train Loss: 0.4590, Test Loss: 0.3599,  Train Acc: 0.7472, Test Acc: 0.7578, Test f1: 0.8102, AUC: class-0>>0.983440170940171|class-1>>0.8992248062015504|class-2>>0.9820739968733715\n",
      "Epoch: 009, Train Loss: 0.4503, Test Loss: 0.3728,  Train Acc: 0.7886, Test Acc: 0.8189, Test f1: 0.8518, AUC: class-0>>0.9821047008547008|class-1>>0.9084967320261437|class-2>>0.9812402292860865\n",
      "Saved model!\n",
      "Epoch: 010, Train Loss: 0.4306, Test Loss: 0.3570,  Train Acc: 0.7813, Test Acc: 0.8092, Test f1: 0.8486, AUC: class-0>>0.984375|class-1>>0.9265845873233014|class-2>>0.9842626367899948\n",
      "Epoch: 011, Train Loss: 0.4595, Test Loss: 0.4175,  Train Acc: 0.7864, Test Acc: 0.8220, Test f1: 0.8526, AUC: class-0>>0.9830395299145299|class-1>>0.92734458124335|class-2>>0.9839499739447628\n",
      "Saved model!\n",
      "Epoch: 012, Train Loss: 0.4305, Test Loss: 0.3842,  Train Acc: 0.8056, Test Acc: 0.8195, Test f1: 0.8538, AUC: class-0>>0.983840811965812|class-1>>0.9205046359629123|class-2>>0.9823866597186035\n",
      "Epoch: 013, Train Loss: 0.4893, Test Loss: 0.4545,  Train Acc: 0.7814, Test Acc: 0.7904, Test f1: 0.8158, AUC: class-0>>0.9845085470085471|class-1>>0.8802249582003344|class-2>>0.9789473684210527\n",
      "Epoch: 014, Train Loss: 0.4008, Test Loss: 0.3436,  Train Acc: 0.7846, Test Acc: 0.7888, Test f1: 0.8367, AUC: class-0>>0.9827724358974359|class-1>>0.9177686578507371|class-2>>0.9837415320479417\n",
      "Epoch: 015, Train Loss: 0.3997, Test Loss: 0.3246,  Train Acc: 0.7884, Test Acc: 0.7984, Test f1: 0.8417, AUC: class-0>>0.9823717948717948|class-1>>0.9317525459796322|class-2>>0.9865554976550286\n",
      "Epoch: 016, Train Loss: 0.5816, Test Loss: 0.4760,  Train Acc: 0.7317, Test Acc: 0.7244, Test f1: 0.7656, AUC: class-0>>0.9826388888888888|class-1>>0.8683690530475756|class-2>>0.9857217300677436\n",
      "Epoch: 017, Train Loss: 0.4117, Test Loss: 0.3540,  Train Acc: 0.8123, Test Acc: 0.8447, Test f1: 0.8753, AUC: class-0>>0.984909188034188|class-1>>0.9334245326037391|class-2>>0.9867639395518499\n",
      "Saved model!\n",
      "Epoch: 018, Train Loss: 0.4486, Test Loss: 0.4387,  Train Acc: 0.8109, Test Acc: 0.8458, Test f1: 0.8625, AUC: class-0>>0.9794337606837608|class-1>>0.922328621371029|class-2>>0.983637311099531\n",
      "Epoch: 019, Train Loss: 0.4100, Test Loss: 0.3558,  Train Acc: 0.7739, Test Acc: 0.7859, Test f1: 0.8407, AUC: class-0>>0.9833066239316239|class-1>>0.9019607843137256|class-2>>0.9908285565398645\n",
      "Epoch: 020, Train Loss: 0.3845, Test Loss: 0.3482,  Train Acc: 0.8149, Test Acc: 0.8518, Test f1: 0.8784, AUC: class-0>>0.9831730769230769|class-1>>0.937376500987992|class-2>>0.9880145909327775\n",
      "Saved model!\n",
      "Epoch: 021, Train Loss: 0.3778, Test Loss: 0.3323,  Train Acc: 0.8010, Test Acc: 0.8172, Test f1: 0.8590, AUC: class-0>>0.9863782051282052|class-1>>0.9408724730202158|class-2>>0.9907243355914539\n",
      "Epoch: 022, Train Loss: 0.3803, Test Loss: 0.3300,  Train Acc: 0.7892, Test Acc: 0.7972, Test f1: 0.8440, AUC: class-0>>0.9837072649572649|class-1>>0.9303845569235447|class-2>>0.989577905158937\n",
      "Epoch: 023, Train Loss: 0.3746, Test Loss: 0.3452,  Train Acc: 0.8242, Test Acc: 0.8414, Test f1: 0.8703, AUC: class-0>>0.984642094017094|class-1>>0.9439124487004105|class-2>>0.9879103699843668\n",
      "Epoch: 024, Train Loss: 0.3872, Test Loss: 0.3465,  Train Acc: 0.8160, Test Acc: 0.7827, Test f1: 0.8252, AUC: class-0>>0.984642094017094|class-1>>0.9285605715154278|class-2>>0.9886399166232414\n",
      "Epoch: 025, Train Loss: 0.3732, Test Loss: 0.3174,  Train Acc: 0.8176, Test Acc: 0.8311, Test f1: 0.8664, AUC: class-0>>0.9873130341880342|class-1>>0.9437604499164006|class-2>>0.9900990099009901\n",
      "Epoch: 026, Train Loss: 0.4179, Test Loss: 0.3641,  Train Acc: 0.7930, Test Acc: 0.7996, Test f1: 0.8459, AUC: class-0>>0.984775641025641|class-1>>0.9036327709378326|class-2>>0.9883272537780095\n",
      "Epoch: 027, Train Loss: 0.4832, Test Loss: 0.4788,  Train Acc: 0.7936, Test Acc: 0.8148, Test f1: 0.8303, AUC: class-0>>0.9865117521367521|class-1>>0.8969448244414044|class-2>>0.98843147472642\n",
      "Epoch: 028, Train Loss: 0.3744, Test Loss: 0.3615,  Train Acc: 0.8262, Test Acc: 0.8737, Test f1: 0.8932, AUC: class-0>>0.9707532051282051|class-1>>0.9458884328925369|class-2>>0.989577905158937\n",
      "Saved model!\n",
      "Epoch: 029, Train Loss: 0.3909, Test Loss: 0.3224,  Train Acc: 0.7890, Test Acc: 0.7859, Test f1: 0.8405, AUC: class-0>>0.9850427350427351|class-1>>0.9247606019151846|class-2>>0.9927045336112559\n",
      "Epoch: 030, Train Loss: 0.3554, Test Loss: 0.3132,  Train Acc: 0.8275, Test Acc: 0.8246, Test f1: 0.8624, AUC: class-0>>0.9869123931623932|class-1>>0.9325125398996809|class-2>>0.9895779051589371\n",
      "Epoch: 031, Train Loss: 0.3523, Test Loss: 0.3300,  Train Acc: 0.8393, Test Acc: 0.8602, Test f1: 0.8863, AUC: class-0>>0.9853098290598291|class-1>>0.9539443684450524|class-2>>0.9896821261073476\n",
      "Epoch: 032, Train Loss: 0.3521, Test Loss: 0.2976,  Train Acc: 0.8071, Test Acc: 0.7914, Test f1: 0.8398, AUC: class-0>>0.9870459401709402|class-1>>0.9506003951968385|class-2>>0.9919749869723815\n",
      "Epoch: 033, Train Loss: 0.3938, Test Loss: 0.3746,  Train Acc: 0.8179, Test Acc: 0.8333, Test f1: 0.8561, AUC: class-0>>0.9863782051282051|class-1>>0.9233926128590971|class-2>>0.9897863470557582\n",
      "Epoch: 034, Train Loss: 0.3574, Test Loss: 0.3029,  Train Acc: 0.8093, Test Acc: 0.8150, Test f1: 0.8620, AUC: class-0>>0.9869123931623931|class-1>>0.9308405532755738|class-2>>0.9924960917144346\n",
      "Epoch: 035, Train Loss: 0.3422, Test Loss: 0.3025,  Train Acc: 0.8108, Test Acc: 0.7782, Test f1: 0.8227, AUC: class-0>>0.984375|class-1>>0.9527283781729745|class-2>>0.9919749869723814\n",
      "Epoch: 036, Train Loss: 0.3447, Test Loss: 0.3208,  Train Acc: 0.8227, Test Acc: 0.8220, Test f1: 0.8590, AUC: class-0>>0.9862446581196581|class-1>>0.9562243502051984|class-2>>0.9918707660239708\n",
      "Epoch: 037, Train Loss: 0.3376, Test Loss: 0.3390,  Train Acc: 0.8378, Test Acc: 0.8702, Test f1: 0.8843, AUC: class-0>>0.9851762820512822|class-1>>0.9417844657242742|class-2>>0.9891610213652944\n",
      "Epoch: 038, Train Loss: 0.3316, Test Loss: 0.3025,  Train Acc: 0.7998, Test Acc: 0.8129, Test f1: 0.8614, AUC: class-0>>0.985576923076923|class-1>>0.9378324973400213|class-2>>0.992391870766024\n",
      "Epoch: 039, Train Loss: 0.3041, Test Loss: 0.2944,  Train Acc: 0.8450, Test Acc: 0.8540, Test f1: 0.8853, AUC: class-0>>0.984375|class-1>>0.9458884328925369|class-2>>0.9917665450755602\n",
      "Epoch: 040, Train Loss: 0.3425, Test Loss: 0.3281,  Train Acc: 0.8601, Test Acc: 0.8709, Test f1: 0.8904, AUC: class-0>>0.986244658119658|class-1>>0.9534883720930232|class-2>>0.9916623241271495\n",
      "Saved model!\n",
      "Epoch: 041, Train Loss: 0.3022, Test Loss: 0.2915,  Train Acc: 0.8359, Test Acc: 0.8245, Test f1: 0.8673, AUC: class-0>>0.984775641025641|class-1>>0.9349445204438365|class-2>>0.9917665450755602\n",
      "Epoch: 042, Train Loss: 0.3110, Test Loss: 0.2970,  Train Acc: 0.8704, Test Acc: 0.8407, Test f1: 0.8767, AUC: class-0>>0.984642094017094|class-1>>0.9396564827481381|class-2>>0.9883272537780095\n",
      "Epoch: 043, Train Loss: 0.2896, Test Loss: 0.2728,  Train Acc: 0.8427, Test Acc: 0.7939, Test f1: 0.8400, AUC: class-0>>0.9886485042735043|class-1>>0.9548563611491108|class-2>>0.9924960917144346\n",
      "Epoch: 044, Train Loss: 0.3297, Test Loss: 0.2897,  Train Acc: 0.8322, Test Acc: 0.8112, Test f1: 0.8588, AUC: class-0>>0.9881143162393162|class-1>>0.9270405836753306|class-2>>0.9935383011985409\n",
      "Epoch: 045, Train Loss: 0.3016, Test Loss: 0.3290,  Train Acc: 0.8163, Test Acc: 0.8269, Test f1: 0.8681, AUC: class-0>>0.9829059829059829|class-1>>0.9299285605715154|class-2>>0.9899947889525795\n",
      "Epoch: 046, Train Loss: 0.2729, Test Loss: 0.2823,  Train Acc: 0.8768, Test Acc: 0.8617, Test f1: 0.8910, AUC: class-0>>0.9850427350427351|class-1>>0.9461924304605562|class-2>>0.989265242313705\n",
      "Epoch: 047, Train Loss: 0.3679, Test Loss: 0.3571,  Train Acc: 0.8532, Test Acc: 0.8967, Test f1: 0.8940, AUC: class-0>>0.9878472222222222|class-1>>0.9516643866849066|class-2>>0.9913496612819177\n",
      "Saved model!\n",
      "Epoch: 048, Train Loss: 0.3190, Test Loss: 0.3319,  Train Acc: 0.8658, Test Acc: 0.8417, Test f1: 0.8704, AUC: class-0>>0.983707264957265|class-1>>0.9472564219486244|class-2>>0.990411672746222\n",
      "Epoch: 049, Train Loss: 0.3482, Test Loss: 0.3431,  Train Acc: 0.8428, Test Acc: 0.8519, Test f1: 0.8791, AUC: class-0>>0.9854433760683761|class-1>>0.9481684146526828|class-2>>0.9917665450755602\n",
      "Epoch: 050, Train Loss: 0.2705, Test Loss: 0.2900,  Train Acc: 0.8574, Test Acc: 0.8433, Test f1: 0.8741, AUC: class-0>>0.9857104700854701|class-1>>0.954704362365101|class-2>>0.9918707660239708\n",
      "Epoch: 051, Train Loss: 0.2981, Test Loss: 0.3405,  Train Acc: 0.8662, Test Acc: 0.8678, Test f1: 0.8873, AUC: class-0>>0.984642094017094|class-1>>0.9433044535643715|class-2>>0.9896821261073476\n",
      "Epoch: 052, Train Loss: 0.2669, Test Loss: 0.2753,  Train Acc: 0.8541, Test Acc: 0.8437, Test f1: 0.8818, AUC: class-0>>0.9878472222222222|class-1>>0.9407204742362061|class-2>>0.9902032308494008\n",
      "Epoch: 053, Train Loss: 0.2529, Test Loss: 0.2636,  Train Acc: 0.8687, Test Acc: 0.8435, Test f1: 0.8781, AUC: class-0>>0.9885149572649572|class-1>>0.9554643562851497|class-2>>0.9917665450755602\n",
      "Epoch: 054, Train Loss: 0.2529, Test Loss: 0.2747,  Train Acc: 0.8886, Test Acc: 0.8720, Test f1: 0.8936, AUC: class-0>>0.9859775641025641|class-1>>0.9533363733090134|class-2>>0.9905158936946327\n",
      "Epoch: 055, Train Loss: 0.3191, Test Loss: 0.3492,  Train Acc: 0.8174, Test Acc: 0.8011, Test f1: 0.8492, AUC: class-0>>0.9879807692307692|class-1>>0.9375284997720019|class-2>>0.991558103178739\n",
      "Epoch: 056, Train Loss: 0.2716, Test Loss: 0.2968,  Train Acc: 0.8470, Test Acc: 0.8222, Test f1: 0.8625, AUC: class-0>>0.9866452991452992|class-1>>0.9515123879008968|class-2>>0.9913496612819177\n",
      "Epoch: 057, Train Loss: 0.2412, Test Loss: 0.2347,  Train Acc: 0.8752, Test Acc: 0.8577, Test f1: 0.8892, AUC: class-0>>0.9887820512820512|class-1>>0.9598723210214318|class-2>>0.994059405940594\n",
      "Epoch: 058, Train Loss: 0.2649, Test Loss: 0.3421,  Train Acc: 0.8642, Test Acc: 0.8104, Test f1: 0.8471, AUC: class-0>>0.9810363247863247|class-1>>0.941328469372245|class-2>>0.9888483585200625\n",
      "Epoch: 059, Train Loss: 0.2393, Test Loss: 0.3105,  Train Acc: 0.8921, Test Acc: 0.8577, Test f1: 0.8890, AUC: class-0>>0.9823717948717948|class-1>>0.9425444596443229|class-2>>0.9908285565398646\n",
      "Epoch: 060, Train Loss: 0.2435, Test Loss: 0.2879,  Train Acc: 0.8773, Test Acc: 0.8391, Test f1: 0.8761, AUC: class-0>>0.9885149572649572|class-1>>0.9484724122207021|class-2>>0.9873892652423136\n",
      "Epoch: 061, Train Loss: 0.2687, Test Loss: 0.3457,  Train Acc: 0.8465, Test Acc: 0.8276, Test f1: 0.8675, AUC: class-0>>0.981971153846154|class-1>>0.9212646298829609|class-2>>0.9858259510161542\n",
      "Epoch: 062, Train Loss: 0.3671, Test Loss: 0.3832,  Train Acc: 0.8010, Test Acc: 0.8207, Test f1: 0.8681, AUC: class-0>>0.9890491452991453|class-1>>0.9195926432588539|class-2>>0.9903074517978113\n",
      "Epoch: 063, Train Loss: 0.2105, Test Loss: 0.2784,  Train Acc: 0.9006, Test Acc: 0.8557, Test f1: 0.8848, AUC: class-0>>0.985309829059829|class-1>>0.9443684450524397|class-2>>0.9883272537780095\n",
      "Epoch: 064, Train Loss: 0.2185, Test Loss: 0.2755,  Train Acc: 0.8989, Test Acc: 0.8546, Test f1: 0.8881, AUC: class-0>>0.9866452991452992|class-1>>0.941936464508284|class-2>>0.9894736842105264\n",
      "Epoch: 065, Train Loss: 0.2788, Test Loss: 0.3824,  Train Acc: 0.8711, Test Acc: 0.8616, Test f1: 0.8812, AUC: class-0>>0.9770299145299145|class-1>>0.9443684450524396|class-2>>0.9880145909327775\n",
      "Epoch: 066, Train Loss: 0.2960, Test Loss: 0.4097,  Train Acc: 0.8845, Test Acc: 0.8543, Test f1: 0.8531, AUC: class-0>>0.9779647435897436|class-1>>0.9411764705882353|class-2>>0.9873892652423137\n",
      "Epoch: 067, Train Loss: 0.2226, Test Loss: 0.2850,  Train Acc: 0.8849, Test Acc: 0.8556, Test f1: 0.8806, AUC: class-0>>0.9869123931623931|class-1>>0.953032375740994|class-2>>0.9905158936946326\n",
      "Epoch: 068, Train Loss: 0.1917, Test Loss: 0.2794,  Train Acc: 0.9003, Test Acc: 0.8666, Test f1: 0.8910, AUC: class-0>>0.984508547008547|class-1>>0.946344429244566|class-2>>0.9910369984366858\n",
      "Epoch: 069, Train Loss: 0.2544, Test Loss: 0.3775,  Train Acc: 0.8618, Test Acc: 0.8380, Test f1: 0.8754, AUC: class-0>>0.9805021367521367|class-1>>0.9136646906824745|class-2>>0.9861386138613861\n",
      "Epoch: 070, Train Loss: 0.2130, Test Loss: 0.3537,  Train Acc: 0.9048, Test Acc: 0.8706, Test f1: 0.8927, AUC: class-0>>0.9736912393162394|class-1>>0.9344885240918073|class-2>>0.9861386138613861\n",
      "Epoch: 071, Train Loss: 0.3369, Test Loss: 0.4073,  Train Acc: 0.8450, Test Acc: 0.7964, Test f1: 0.8341, AUC: class-0>>0.9660790598290598|class-1>>0.925672594619243|class-2>>0.9890568004168838\n",
      "Epoch: 072, Train Loss: 0.2325, Test Loss: 0.3374,  Train Acc: 0.8910, Test Acc: 0.8432, Test f1: 0.8738, AUC: class-0>>0.9817040598290598|class-1>>0.9355525155798754|class-2>>0.9846795205836374\n",
      "Epoch: 073, Train Loss: 0.1927, Test Loss: 0.3268,  Train Acc: 0.9053, Test Acc: 0.8544, Test f1: 0.8845, AUC: class-0>>0.9850427350427351|class-1>>0.9433044535643715|class-2>>0.9858259510161543\n",
      "Epoch: 074, Train Loss: 0.1801, Test Loss: 0.3291,  Train Acc: 0.9103, Test Acc: 0.8513, Test f1: 0.8836, AUC: class-0>>0.9819711538461539|class-1>>0.9417844657242742|class-2>>0.985617509119333\n",
      "Epoch: 075, Train Loss: 0.1971, Test Loss: 0.3200,  Train Acc: 0.8855, Test Acc: 0.8036, Test f1: 0.8502, AUC: class-0>>0.9878472222222223|class-1>>0.9436084511323909|class-2>>0.9878061490359562\n",
      "Epoch: 076, Train Loss: 0.1803, Test Loss: 0.3298,  Train Acc: 0.9112, Test Acc: 0.8720, Test f1: 0.8933, AUC: class-0>>0.9831730769230769|class-1>>0.9392004863961088|class-2>>0.9857217300677437\n",
      "Epoch: 077, Train Loss: 0.1543, Test Loss: 0.3392,  Train Acc: 0.9247, Test Acc: 0.8480, Test f1: 0.8789, AUC: class-0>>0.9834401709401709|class-1>>0.9384404924760602|class-2>>0.9863470557582074\n",
      "Epoch: 078, Train Loss: 0.2572, Test Loss: 0.4272,  Train Acc: 0.8580, Test Acc: 0.8393, Test f1: 0.8805, AUC: class-0>>0.983974358974359|class-1>>0.9115367077063384|class-2>>0.9846795205836373\n",
      "Epoch: 079, Train Loss: 0.1945, Test Loss: 0.3657,  Train Acc: 0.8788, Test Acc: 0.8114, Test f1: 0.8562, AUC: class-0>>0.9859775641025641|class-1>>0.9354005167958657|class-2>>0.9866597186034393\n",
      "Epoch: 080, Train Loss: 0.1804, Test Loss: 0.3278,  Train Acc: 0.9165, Test Acc: 0.8733, Test f1: 0.8941, AUC: class-0>>0.9772970085470085|class-1>>0.9373765009879922|class-2>>0.9867639395518499\n",
      "Epoch: 081, Train Loss: 0.1663, Test Loss: 0.3200,  Train Acc: 0.9102, Test Acc: 0.8280, Test f1: 0.8660, AUC: class-0>>0.9861111111111112|class-1>>0.9439124487004105|class-2>>0.9863470557582074\n",
      "Epoch: 082, Train Loss: 0.2240, Test Loss: 0.4312,  Train Acc: 0.8598, Test Acc: 0.8003, Test f1: 0.8453, AUC: class-0>>0.9859775641025641|class-1>>0.934640522875817|class-2>>0.9837415320479417\n",
      "Epoch: 083, Train Loss: 0.1396, Test Loss: 0.3283,  Train Acc: 0.9350, Test Acc: 0.8615, Test f1: 0.8866, AUC: class-0>>0.9818376068376069|class-1>>0.937984496124031|class-2>>0.9869723814486713\n",
      "Epoch: 084, Train Loss: 0.1353, Test Loss: 0.3151,  Train Acc: 0.9389, Test Acc: 0.8712, Test f1: 0.8961, AUC: class-0>>0.9890491452991452|class-1>>0.9398084815321478|class-2>>0.9870766023970818\n",
      "Epoch: 085, Train Loss: 0.1377, Test Loss: 0.3096,  Train Acc: 0.9404, Test Acc: 0.8695, Test f1: 0.8966, AUC: class-0>>0.9861111111111112|class-1>>0.9325125398996807|class-2>>0.987597707139135\n",
      "Epoch: 086, Train Loss: 0.1607, Test Loss: 0.3554,  Train Acc: 0.9193, Test Acc: 0.8511, Test f1: 0.8802, AUC: class-0>>0.9898504273504274|class-1>>0.9405684754521964|class-2>>0.9857217300677437\n",
      "Epoch: 087, Train Loss: 0.1225, Test Loss: 0.3451,  Train Acc: 0.9377, Test Acc: 0.8648, Test f1: 0.8911, AUC: class-0>>0.9817040598290598|class-1>>0.9323605411156711|class-2>>0.9862428348097968\n",
      "Epoch: 088, Train Loss: 0.1174, Test Loss: 0.3463,  Train Acc: 0.9501, Test Acc: 0.8648, Test f1: 0.8911, AUC: class-0>>0.9846420940170939|class-1>>0.9314485484116126|class-2>>0.985617509119333\n",
      "Epoch: 089, Train Loss: 0.1229, Test Loss: 0.3524,  Train Acc: 0.9425, Test Acc: 0.8675, Test f1: 0.8927, AUC: class-0>>0.9833066239316239|class-1>>0.9328165374677002|class-2>>0.9865554976550286\n",
      "Epoch: 090, Train Loss: 0.1213, Test Loss: 0.3601,  Train Acc: 0.9445, Test Acc: 0.8833, Test f1: 0.8976, AUC: class-0>>0.9875801282051282|class-1>>0.9407204742362061|class-2>>0.9869723814486712\n",
      "Epoch: 091, Train Loss: 0.1506, Test Loss: 0.4352,  Train Acc: 0.9181, Test Acc: 0.8158, Test f1: 0.8578, AUC: class-0>>0.9819711538461539|class-1>>0.9331205350357197|class-2>>0.9846795205836374\n",
      "Epoch: 092, Train Loss: 0.1483, Test Loss: 0.3832,  Train Acc: 0.9279, Test Acc: 0.8580, Test f1: 0.8875, AUC: class-0>>0.9865117521367521|class-1>>0.9294725642194862|class-2>>0.9855132881709224\n",
      "Epoch: 093, Train Loss: 0.1276, Test Loss: 0.3762,  Train Acc: 0.9450, Test Acc: 0.8811, Test f1: 0.8883, AUC: class-0>>0.985576923076923|class-1>>0.9405684754521964|class-2>>0.9865554976550287\n",
      "Epoch: 094, Train Loss: 0.0975, Test Loss: 0.3354,  Train Acc: 0.9671, Test Acc: 0.8976, Test f1: 0.9130, AUC: class-0>>0.9877136752136751|class-1>>0.9385924912600698|class-2>>0.9891610213652944\n",
      "Saved model!\n",
      "Epoch: 095, Train Loss: 0.0964, Test Loss: 0.4255,  Train Acc: 0.9498, Test Acc: 0.8435, Test f1: 0.8775, AUC: class-0>>0.9846420940170941|class-1>>0.9334245326037391|class-2>>0.985617509119333\n",
      "Epoch: 096, Train Loss: 0.1276, Test Loss: 0.4510,  Train Acc: 0.9358, Test Acc: 0.8759, Test f1: 0.9014, AUC: class-0>>0.9793002136752136|class-1>>0.922328621371029|class-2>>0.9825951016154246\n",
      "Epoch: 097, Train Loss: 0.0978, Test Loss: 0.3696,  Train Acc: 0.9522, Test Acc: 0.8797, Test f1: 0.8986, AUC: class-0>>0.983840811965812|class-1>>0.9372245022039823|class-2>>0.9877019280875456\n",
      "Epoch: 098, Train Loss: 0.0914, Test Loss: 0.4268,  Train Acc: 0.9576, Test Acc: 0.8711, Test f1: 0.8917, AUC: class-0>>0.9837072649572649|class-1>>0.9381364949080407|class-2>>0.987597707139135\n",
      "Epoch: 099, Train Loss: 0.0810, Test Loss: 0.3950,  Train Acc: 0.9595, Test Acc: 0.8468, Test f1: 0.8730, AUC: class-0>>0.9790331196581196|class-1>>0.9293205654354766|class-2>>0.9842626367899947\n",
      "Epoch: 100, Train Loss: 0.0808, Test Loss: 0.4425,  Train Acc: 0.9723, Test Acc: 0.8678, Test f1: 0.8868, AUC: class-0>>0.9799679487179487|class-1>>0.9378324973400213|class-2>>0.9861386138613861\n",
      "Epoch: 101, Train Loss: 0.0892, Test Loss: 0.5199,  Train Acc: 0.9656, Test Acc: 0.8466, Test f1: 0.8680, AUC: class-0>>0.9759615384615384|class-1>>0.9300805593555251|class-2>>0.9822824387701928\n",
      "Epoch: 102, Train Loss: 0.0755, Test Loss: 0.4225,  Train Acc: 0.9725, Test Acc: 0.8675, Test f1: 0.8927, AUC: class-0>>0.981971153846154|class-1>>0.9308405532755738|class-2>>0.984783741532048\n",
      "Epoch: 103, Train Loss: 0.0667, Test Loss: 0.4310,  Train Acc: 0.9736, Test Acc: 0.8834, Test f1: 0.9024, AUC: class-0>>0.9821047008547008|class-1>>0.9302325581395349|class-2>>0.9863470557582075\n",
      "Epoch: 104, Train Loss: 0.0906, Test Loss: 0.4877,  Train Acc: 0.9660, Test Acc: 0.8745, Test f1: 0.8786, AUC: class-0>>0.9825053418803419|class-1>>0.9357045143638851|class-2>>0.9862428348097967\n",
      "Epoch: 105, Train Loss: 0.0623, Test Loss: 0.4025,  Train Acc: 0.9690, Test Acc: 0.8757, Test f1: 0.8969, AUC: class-0>>0.9878472222222222|class-1>>0.9364645082839337|class-2>>0.9879103699843668\n",
      "Epoch: 106, Train Loss: 0.0872, Test Loss: 0.4974,  Train Acc: 0.9589, Test Acc: 0.8723, Test f1: 0.8804, AUC: class-0>>0.9759615384615383|class-1>>0.9350965192278462|class-2>>0.9863470557582075\n",
      "Epoch: 107, Train Loss: 0.0615, Test Loss: 0.4225,  Train Acc: 0.9836, Test Acc: 0.8803, Test f1: 0.9021, AUC: class-0>>0.9831730769230769|class-1>>0.9325125398996807|class-2>>0.9830119854090673\n",
      "Epoch: 108, Train Loss: 0.0638, Test Loss: 0.4799,  Train Acc: 0.9732, Test Acc: 0.8445, Test f1: 0.8699, AUC: class-0>>0.983440170940171|class-1>>0.9355525155798754|class-2>>0.9873892652423137\n",
      "Epoch: 109, Train Loss: 0.0638, Test Loss: 0.4143,  Train Acc: 0.9723, Test Acc: 0.8739, Test f1: 0.8975, AUC: class-0>>0.9850427350427351|class-1>>0.9343365253077975|class-2>>0.9881188118811882\n",
      "Epoch: 110, Train Loss: 0.0538, Test Loss: 0.4145,  Train Acc: 0.9869, Test Acc: 0.8770, Test f1: 0.8973, AUC: class-0>>0.984909188034188|class-1>>0.9357045143638851|class-2>>0.98843147472642\n",
      "Epoch: 111, Train Loss: 0.0465, Test Loss: 0.4337,  Train Acc: 0.9848, Test Acc: 0.8662, Test f1: 0.8920, AUC: class-0>>0.984375|class-1>>0.9352485180118558|class-2>>0.9874934861907243\n",
      "Epoch: 112, Train Loss: 0.0466, Test Loss: 0.5253,  Train Acc: 0.9869, Test Acc: 0.8419, Test f1: 0.8623, AUC: class-0>>0.9778311965811965|class-1>>0.9344885240918073|class-2>>0.9867639395518499\n",
      "Epoch: 113, Train Loss: 0.0455, Test Loss: 0.4865,  Train Acc: 0.9867, Test Acc: 0.8507, Test f1: 0.8808, AUC: class-0>>0.9811698717948718|class-1>>0.9317525459796321|class-2>>0.9866597186034394\n",
      "Epoch: 114, Train Loss: 0.0457, Test Loss: 0.4490,  Train Acc: 0.9869, Test Acc: 0.8706, Test f1: 0.8929, AUC: class-0>>0.9857104700854701|class-1>>0.9325125398996807|class-2>>0.9873892652423137\n",
      "Epoch: 115, Train Loss: 0.0490, Test Loss: 0.4376,  Train Acc: 0.9776, Test Acc: 0.8524, Test f1: 0.8800, AUC: class-0>>0.9873130341880342|class-1>>0.9366165070679434|class-2>>0.9888483585200625\n",
      "Epoch: 116, Train Loss: 0.0386, Test Loss: 0.4471,  Train Acc: 0.9891, Test Acc: 0.8565, Test f1: 0.8827, AUC: class-0>>0.9858440170940171|class-1>>0.9354005167958657|class-2>>0.9877019280875455\n",
      "Epoch: 117, Train Loss: 0.0448, Test Loss: 0.4833,  Train Acc: 0.9830, Test Acc: 0.8778, Test f1: 0.8944, AUC: class-0>>0.9819711538461539|class-1>>0.9322085423316614|class-2>>0.9879103699843668\n",
      "Epoch: 118, Train Loss: 0.0447, Test Loss: 0.4954,  Train Acc: 0.9821, Test Acc: 0.8524, Test f1: 0.8800, AUC: class-0>>0.9861111111111112|class-1>>0.9363125094999241|class-2>>0.9875977071391351\n",
      "Epoch: 119, Train Loss: 0.0349, Test Loss: 0.4709,  Train Acc: 0.9890, Test Acc: 0.8673, Test f1: 0.8880, AUC: class-0>>0.983840811965812|class-1>>0.9349445204438365|class-2>>0.9881188118811881\n",
      "Epoch: 120, Train Loss: 0.0389, Test Loss: 0.4994,  Train Acc: 0.9866, Test Acc: 0.8396, Test f1: 0.8706, AUC: class-0>>0.983974358974359|class-1>>0.9325125398996809|class-2>>0.9877019280875455\n",
      "Epoch: 121, Train Loss: 0.0342, Test Loss: 0.4714,  Train Acc: 0.9893, Test Acc: 0.8687, Test f1: 0.8885, AUC: class-0>>0.983974358974359|class-1>>0.93296853625171|class-2>>0.9886399166232412\n",
      "Epoch: 122, Train Loss: 0.0300, Test Loss: 0.5077,  Train Acc: 0.9911, Test Acc: 0.8485, Test f1: 0.8721, AUC: class-0>>0.9805021367521367|class-1>>0.9302325581395349|class-2>>0.9878061490359562\n",
      "Epoch: 123, Train Loss: 0.0678, Test Loss: 0.5440,  Train Acc: 0.9707, Test Acc: 0.8477, Test f1: 0.8747, AUC: class-0>>0.9822382478632479|class-1>>0.9319045447636419|class-2>>0.9866597186034393\n",
      "Epoch: 124, Train Loss: 0.0318, Test Loss: 0.5088,  Train Acc: 0.9936, Test Acc: 0.8640, Test f1: 0.8829, AUC: class-0>>0.9819711538461539|class-1>>0.9288645690834474|class-2>>0.9875977071391349\n",
      "Epoch: 125, Train Loss: 0.0296, Test Loss: 0.5387,  Train Acc: 0.9911, Test Acc: 0.8330, Test f1: 0.8611, AUC: class-0>>0.9821047008547008|class-1>>0.9309925520595835|class-2>>0.9865554976550287\n",
      "Epoch: 126, Train Loss: 0.0589, Test Loss: 0.5559,  Train Acc: 0.9728, Test Acc: 0.8574, Test f1: 0.8841, AUC: class-0>>0.9809027777777777|class-1>>0.9290165678674571|class-2>>0.985617509119333\n",
      "Epoch: 127, Train Loss: 0.0272, Test Loss: 0.5436,  Train Acc: 0.9912, Test Acc: 0.8499, Test f1: 0.8728, AUC: class-0>>0.9815705128205128|class-1>>0.9319045447636419|class-2>>0.9870766023970818\n",
      "Epoch: 128, Train Loss: 0.0304, Test Loss: 0.5418,  Train Acc: 0.9888, Test Acc: 0.8427, Test f1: 0.8705, AUC: class-0>>0.9835737179487181|class-1>>0.9323605411156711|class-2>>0.98843147472642\n",
      "Epoch: 129, Train Loss: 0.0255, Test Loss: 0.5131,  Train Acc: 0.9957, Test Acc: 0.8551, Test f1: 0.8816, AUC: class-0>>0.982905982905983|class-1>>0.9308405532755738|class-2>>0.9875977071391351\n",
      "Epoch: 130, Train Loss: 0.0269, Test Loss: 0.5246,  Train Acc: 0.9936, Test Acc: 0.8576, Test f1: 0.8781, AUC: class-0>>0.982638888888889|class-1>>0.9334245326037393|class-2>>0.9870766023970818\n",
      "Epoch: 131, Train Loss: 0.0236, Test Loss: 0.5304,  Train Acc: 0.9957, Test Acc: 0.8363, Test f1: 0.8658, AUC: class-0>>0.9818376068376069|class-1>>0.9306885544915641|class-2>>0.9878061490359562\n",
      "Epoch: 132, Train Loss: 0.0233, Test Loss: 0.5575,  Train Acc: 0.9933, Test Acc: 0.8441, Test f1: 0.8713, AUC: class-0>>0.9813034188034189|class-1>>0.9308405532755738|class-2>>0.9863470557582075\n",
      "Epoch: 133, Train Loss: 0.0223, Test Loss: 0.5375,  Train Acc: 0.9957, Test Acc: 0.8454, Test f1: 0.8720, AUC: class-0>>0.9823717948717949|class-1>>0.9299285605715154|class-2>>0.9870766023970818\n",
      "Epoch: 134, Train Loss: 0.0244, Test Loss: 0.5375,  Train Acc: 0.9936, Test Acc: 0.8565, Test f1: 0.8823, AUC: class-0>>0.983440170940171|class-1>>0.931296549627603|class-2>>0.9866597186034393\n",
      "Epoch: 135, Train Loss: 0.0238, Test Loss: 0.5656,  Train Acc: 0.9933, Test Acc: 0.8396, Test f1: 0.8706, AUC: class-0>>0.9831730769230769|class-1>>0.9299285605715154|class-2>>0.9882230328295987\n",
      "Epoch: 136, Train Loss: 0.0210, Test Loss: 0.5677,  Train Acc: 0.9933, Test Acc: 0.8441, Test f1: 0.8713, AUC: class-0>>0.9811698717948718|class-1>>0.9306885544915641|class-2>>0.987597707139135\n",
      "Epoch: 137, Train Loss: 0.0206, Test Loss: 0.5631,  Train Acc: 0.9957, Test Acc: 0.8441, Test f1: 0.8713, AUC: class-0>>0.9809027777777779|class-1>>0.9305365557075543|class-2>>0.9870766023970818\n",
      "Epoch: 138, Train Loss: 0.0198, Test Loss: 0.5528,  Train Acc: 0.9957, Test Acc: 0.8363, Test f1: 0.8658, AUC: class-0>>0.9829059829059829|class-1>>0.9306885544915642|class-2>>0.987597707139135\n",
      "Epoch: 139, Train Loss: 0.0195, Test Loss: 0.5702,  Train Acc: 0.9955, Test Acc: 0.8363, Test f1: 0.8658, AUC: class-0>>0.9819711538461539|class-1>>0.9305365557075543|class-2>>0.9874934861907243\n",
      "Epoch: 140, Train Loss: 0.0195, Test Loss: 0.5680,  Train Acc: 0.9957, Test Acc: 0.8363, Test f1: 0.8658, AUC: class-0>>0.9823717948717948|class-1>>0.9309925520595835|class-2>>0.9879103699843668\n",
      "Epoch: 141, Train Loss: 0.0197, Test Loss: 0.5734,  Train Acc: 0.9955, Test Acc: 0.8363, Test f1: 0.8658, AUC: class-0>>0.982638888888889|class-1>>0.931296549627603|class-2>>0.9874934861907243\n",
      "Epoch: 142, Train Loss: 0.0188, Test Loss: 0.5688,  Train Acc: 0.9957, Test Acc: 0.8363, Test f1: 0.8658, AUC: class-0>>0.9806356837606838|class-1>>0.9305365557075543|class-2>>0.9872850442939032\n",
      "Epoch: 143, Train Loss: 0.0188, Test Loss: 0.5731,  Train Acc: 0.9955, Test Acc: 0.8363, Test f1: 0.8658, AUC: class-0>>0.9823717948717948|class-1>>0.9306885544915642|class-2>>0.9877019280875456\n",
      "Epoch: 144, Train Loss: 0.0185, Test Loss: 0.5712,  Train Acc: 0.9957, Test Acc: 0.8441, Test f1: 0.8713, AUC: class-0>>0.9809027777777777|class-1>>0.930688554491564|class-2>>0.9872850442939032\n",
      "Epoch: 145, Train Loss: 0.0182, Test Loss: 0.5831,  Train Acc: 0.9955, Test Acc: 0.8363, Test f1: 0.8658, AUC: class-0>>0.9821047008547008|class-1>>0.9308405532755738|class-2>>0.9871808233454924\n",
      "Epoch: 146, Train Loss: 0.0180, Test Loss: 0.5752,  Train Acc: 0.9957, Test Acc: 0.8363, Test f1: 0.8658, AUC: class-0>>0.9819711538461539|class-1>>0.9308405532755738|class-2>>0.9877019280875456\n",
      "Epoch: 147, Train Loss: 0.0189, Test Loss: 0.5748,  Train Acc: 0.9957, Test Acc: 0.8454, Test f1: 0.8720, AUC: class-0>>0.9802350427350427|class-1>>0.9306885544915642|class-2>>0.9874934861907243\n",
      "Epoch: 148, Train Loss: 0.0176, Test Loss: 0.5827,  Train Acc: 0.9955, Test Acc: 0.8363, Test f1: 0.8658, AUC: class-0>>0.9826388888888888|class-1>>0.9309925520595835|class-2>>0.9874934861907243\n",
      "Epoch: 149, Train Loss: 0.0174, Test Loss: 0.5811,  Train Acc: 0.9955, Test Acc: 0.8363, Test f1: 0.8658, AUC: class-0>>0.9829059829059829|class-1>>0.9314485484116126|class-2>>0.9878061490359562\n",
      "Running cross-validation fold: 02\n",
      "Epoch: 001, Train Loss: 0.6736, Test Loss: 0.7279,  Train Acc: 0.7866, Test Acc: 0.7004, Test f1: 0.7471, AUC: class-0>>0.9211001444390949|class-1>>0.7288427033492823|class-2>>0.9605870020964361\n",
      "Saved model!\n",
      "Epoch: 002, Train Loss: 0.4411, Test Loss: 0.5443,  Train Acc: 0.7683, Test Acc: 0.6686, Test f1: 0.7191, AUC: class-0>>0.9261555127587867|class-1>>0.7985197368421053|class-2>>0.9631027253668764\n",
      "Epoch: 003, Train Loss: 0.4214, Test Loss: 0.5458,  Train Acc: 0.7641, Test Acc: 0.7039, Test f1: 0.7529, AUC: class-0>>0.9272388059701493|class-1>>0.8131728468899521|class-2>>0.9628930817610063\n",
      "Saved model!\n",
      "Epoch: 004, Train Loss: 0.5076, Test Loss: 0.6154,  Train Acc: 0.8163, Test Acc: 0.6873, Test f1: 0.7378, AUC: class-0>>0.9250722195474242|class-1>>0.7738486842105264|class-2>>0.9655136268343816\n",
      "Epoch: 005, Train Loss: 0.3986, Test Loss: 0.5892,  Train Acc: 0.8198, Test Acc: 0.7281, Test f1: 0.7704, AUC: class-0>>0.9297664901299951|class-1>>0.8018092105263157|class-2>>0.9725366876310273\n",
      "Saved model!\n",
      "Epoch: 006, Train Loss: 0.5292, Test Loss: 0.7149,  Train Acc: 0.7332, Test Acc: 0.6543, Test f1: 0.6947, AUC: class-0>>0.9254333172845449|class-1>>0.7917912679425837|class-2>>0.9691823899371068\n",
      "Epoch: 007, Train Loss: 0.4440, Test Loss: 0.5730,  Train Acc: 0.7582, Test Acc: 0.6813, Test f1: 0.7278, AUC: class-0>>0.9290442946557536|class-1>>0.83440490430622|class-2>>0.9730607966457023\n",
      "Epoch: 008, Train Loss: 0.6641, Test Loss: 0.7517,  Train Acc: 0.7058, Test Acc: 0.6360, Test f1: 0.6874, AUC: class-0>>0.9260351468464131|class-1>>0.7750448564593302|class-2>>0.9721174004192873\n",
      "Epoch: 009, Train Loss: 0.5081, Test Loss: 0.6579,  Train Acc: 0.8125, Test Acc: 0.7411, Test f1: 0.7731, AUC: class-0>>0.9263360616273472|class-1>>0.8410586124401913|class-2>>0.9767295597484277\n",
      "Saved model!\n",
      "Epoch: 010, Train Loss: 0.3663, Test Loss: 0.5577,  Train Acc: 0.7953, Test Acc: 0.6816, Test f1: 0.7318, AUC: class-0>>0.9309701492537313|class-1>>0.8333582535885167|class-2>>0.9755765199161426\n",
      "Epoch: 011, Train Loss: 0.3837, Test Loss: 0.5070,  Train Acc: 0.7908, Test Acc: 0.6716, Test f1: 0.7176, AUC: class-0>>0.9296461242176216|class-1>>0.8448714114832536|class-2>>0.9756813417190776\n",
      "Epoch: 012, Train Loss: 0.3401, Test Loss: 0.5183,  Train Acc: 0.8217, Test Acc: 0.7386, Test f1: 0.7805, AUC: class-0>>0.9319330765527204|class-1>>0.852497009569378|class-2>>0.9756813417190776\n",
      "Saved model!\n",
      "Epoch: 013, Train Loss: 0.4420, Test Loss: 0.6714,  Train Acc: 0.7973, Test Acc: 0.6598, Test f1: 0.7061, AUC: class-0>>0.9279610014443909|class-1>>0.7451405502392344|class-2>>0.9748427672955975\n",
      "Epoch: 014, Train Loss: 0.3562, Test Loss: 0.5568,  Train Acc: 0.8280, Test Acc: 0.7257, Test f1: 0.7538, AUC: class-0>>0.9357847857486759|class-1>>0.8587769138755981|class-2>>0.9774633123689728\n",
      "Epoch: 015, Train Loss: 0.4167, Test Loss: 0.5455,  Train Acc: 0.7663, Test Acc: 0.6954, Test f1: 0.7446, AUC: class-0>>0.9318127106403467|class-1>>0.8393391148325359|class-2>>0.9751572327044025\n",
      "Epoch: 016, Train Loss: 0.3426, Test Loss: 0.6253,  Train Acc: 0.8467, Test Acc: 0.7744, Test f1: 0.8008, AUC: class-0>>0.9242296581608089|class-1>>0.8364982057416268|class-2>>0.9790356394129979\n",
      "Saved model!\n",
      "Epoch: 017, Train Loss: 0.3008, Test Loss: 0.5170,  Train Acc: 0.8587, Test Acc: 0.7221, Test f1: 0.7649, AUC: class-0>>0.9325349061145883|class-1>>0.8613187799043063|class-2>>0.9751572327044025\n",
      "Epoch: 018, Train Loss: 0.4391, Test Loss: 0.6589,  Train Acc: 0.7715, Test Acc: 0.7072, Test f1: 0.7505, AUC: class-0>>0.926396244583534|class-1>>0.8248355263157895|class-2>>0.9748427672955975\n",
      "Epoch: 019, Train Loss: 0.3041, Test Loss: 0.5013,  Train Acc: 0.8351, Test Acc: 0.7393, Test f1: 0.7780, AUC: class-0>>0.9321738083774675|class-1>>0.8684958133971292|class-2>>0.9781970649895179\n",
      "Epoch: 020, Train Loss: 0.3507, Test Loss: 0.5702,  Train Acc: 0.8567, Test Acc: 0.7624, Test f1: 0.7941, AUC: class-0>>0.9244703899855561|class-1>>0.8607954545454546|class-2>>0.980188679245283\n",
      "Epoch: 021, Train Loss: 0.3114, Test Loss: 0.5790,  Train Acc: 0.8561, Test Acc: 0.7530, Test f1: 0.7901, AUC: class-0>>0.9289239287433798|class-1>>0.8454694976076556|class-2>>0.9759958071278827\n",
      "Epoch: 022, Train Loss: 0.2988, Test Loss: 0.5041,  Train Acc: 0.8315, Test Acc: 0.7372, Test f1: 0.7835, AUC: class-0>>0.9336783822821377|class-1>>0.8705143540669855|class-2>>0.9785115303983228\n",
      "Epoch: 023, Train Loss: 0.2713, Test Loss: 0.4993,  Train Acc: 0.8574, Test Acc: 0.7353, Test f1: 0.7776, AUC: class-0>>0.9353033220991815|class-1>>0.8698415071770335|class-2>>0.9788259958071279\n",
      "Epoch: 024, Train Loss: 0.2953, Test Loss: 0.5799,  Train Acc: 0.8394, Test Acc: 0.7426, Test f1: 0.7885, AUC: class-0>>0.9343403948001926|class-1>>0.8548893540669856|class-2>>0.9755765199161426\n",
      "Epoch: 025, Train Loss: 0.2588, Test Loss: 0.5028,  Train Acc: 0.8597, Test Acc: 0.7606, Test f1: 0.8002, AUC: class-0>>0.9233870967741936|class-1>>0.8771680622009569|class-2>>0.979454926624738\n",
      "Epoch: 026, Train Loss: 0.2869, Test Loss: 0.6035,  Train Acc: 0.8814, Test Acc: 0.7954, Test f1: 0.8148, AUC: class-0>>0.9239889263360617|class-1>>0.8776166267942583|class-2>>0.9808176100628931\n",
      "Saved model!\n",
      "Epoch: 027, Train Loss: 0.2462, Test Loss: 0.5027,  Train Acc: 0.8798, Test Acc: 0.7528, Test f1: 0.7919, AUC: class-0>>0.9297664901299952|class-1>>0.8699910287081339|class-2>>0.9807127882599581\n",
      "Epoch: 028, Train Loss: 0.2536, Test Loss: 0.5036,  Train Acc: 0.8711, Test Acc: 0.7561, Test f1: 0.7907, AUC: class-0>>0.9309701492537313|class-1>>0.8689443779904306|class-2>>0.9793501048218031\n",
      "Epoch: 029, Train Loss: 0.3260, Test Loss: 0.6142,  Train Acc: 0.8200, Test Acc: 0.6964, Test f1: 0.7455, AUC: class-0>>0.9271184400577756|class-1>>0.8586273923444976|class-2>>0.978511530398323\n",
      "Epoch: 030, Train Loss: 0.2730, Test Loss: 0.5735,  Train Acc: 0.8627, Test Acc: 0.7184, Test f1: 0.7653, AUC: class-0>>0.9318127106403467|class-1>>0.8587769138755981|class-2>>0.979140461215933\n",
      "Epoch: 031, Train Loss: 0.2875, Test Loss: 0.6033,  Train Acc: 0.8570, Test Acc: 0.7123, Test f1: 0.7589, AUC: class-0>>0.9333774675012035|class-1>>0.8511513157894737|class-2>>0.9773584905660377\n",
      "Epoch: 032, Train Loss: 0.2655, Test Loss: 0.5305,  Train Acc: 0.8494, Test Acc: 0.7123, Test f1: 0.7621, AUC: class-0>>0.9312108810784786|class-1>>0.8716357655502391|class-2>>0.979664570230608\n",
      "Epoch: 033, Train Loss: 0.2978, Test Loss: 0.5498,  Train Acc: 0.8853, Test Acc: 0.7745, Test f1: 0.7963, AUC: class-0>>0.9255536831969187|class-1>>0.8722338516746411|class-2>>0.9816561844863733\n",
      "Epoch: 034, Train Loss: 0.2897, Test Loss: 0.7079,  Train Acc: 0.8795, Test Acc: 0.7917, Test f1: 0.8105, AUC: class-0>>0.9245907558979297|class-1>>0.8634120813397128|class-2>>0.979979035639413\n",
      "Epoch: 035, Train Loss: 0.2297, Test Loss: 0.5599,  Train Acc: 0.8823, Test Acc: 0.7675, Test f1: 0.8012, AUC: class-0>>0.9265767934520944|class-1>>0.8720095693779905|class-2>>0.9815513626834381\n",
      "Epoch: 036, Train Loss: 0.2420, Test Loss: 0.5656,  Train Acc: 0.8869, Test Acc: 0.7512, Test f1: 0.7870, AUC: class-0>>0.9265166104959075|class-1>>0.8599730861244019|class-2>>0.9781970649895179\n",
      "Epoch: 037, Train Loss: 0.2966, Test Loss: 0.6771,  Train Acc: 0.8849, Test Acc: 0.7796, Test f1: 0.8093, AUC: class-0>>0.9237481945113144|class-1>>0.8507027511961722|class-2>>0.979874213836478\n",
      "Epoch: 038, Train Loss: 0.2857, Test Loss: 0.6052,  Train Acc: 0.8404, Test Acc: 0.7182, Test f1: 0.7675, AUC: class-0>>0.9349422243620606|class-1>>0.8613187799043063|class-2>>0.9763102725366876\n",
      "Epoch: 039, Train Loss: 0.2350, Test Loss: 0.5659,  Train Acc: 0.8852, Test Acc: 0.7167, Test f1: 0.7632, AUC: class-0>>0.9259147809340396|class-1>>0.8607206937799042|class-2>>0.9791404612159329\n",
      "Epoch: 040, Train Loss: 0.2194, Test Loss: 0.5374,  Train Acc: 0.8994, Test Acc: 0.7869, Test f1: 0.8135, AUC: class-0>>0.9268777082330284|class-1>>0.8680472488038278|class-2>>0.9806079664570231\n",
      "Epoch: 041, Train Loss: 0.2712, Test Loss: 0.6324,  Train Acc: 0.8539, Test Acc: 0.7318, Test f1: 0.7792, AUC: class-0>>0.9284424650938854|class-1>>0.8553379186602872|class-2>>0.9752620545073376\n",
      "Epoch: 042, Train Loss: 0.2116, Test Loss: 0.6130,  Train Acc: 0.9054, Test Acc: 0.7560, Test f1: 0.7957, AUC: class-0>>0.924831487722677|class-1>>0.8643092105263157|class-2>>0.9817610062893082\n",
      "Epoch: 043, Train Loss: 0.2065, Test Loss: 0.5779,  Train Acc: 0.9070, Test Acc: 0.7704, Test f1: 0.7962, AUC: class-0>>0.9155633124699084|class-1>>0.8737290669856459|class-2>>0.9809224318658282\n",
      "Epoch: 044, Train Loss: 0.2069, Test Loss: 0.6420,  Train Acc: 0.9070, Test Acc: 0.7371, Test f1: 0.7710, AUC: class-0>>0.9200168512277322|class-1>>0.8629635167464114|class-2>>0.979559748427673\n",
      "Epoch: 045, Train Loss: 0.1974, Test Loss: 0.5699,  Train Acc: 0.9082, Test Acc: 0.7771, Test f1: 0.8075, AUC: class-0>>0.9178502648050072|class-1>>0.8783642344497608|class-2>>0.9828092243186582\n",
      "Epoch: 046, Train Loss: 0.3005, Test Loss: 0.7641,  Train Acc: 0.8591, Test Acc: 0.7191, Test f1: 0.7669, AUC: class-0>>0.9230259990370727|class-1>>0.8376943779904306|class-2>>0.9783018867924528\n",
      "Epoch: 047, Train Loss: 0.2297, Test Loss: 0.6342,  Train Acc: 0.8863, Test Acc: 0.7521, Test f1: 0.7909, AUC: class-0>>0.9283220991815119|class-1>>0.8641596889952153|class-2>>0.9763102725366877\n",
      "Epoch: 048, Train Loss: 0.2323, Test Loss: 0.6473,  Train Acc: 0.9082, Test Acc: 0.7635, Test f1: 0.7892, AUC: class-0>>0.9155633124699086|class-1>>0.8797099282296651|class-2>>0.9821802935010483\n",
      "Epoch: 049, Train Loss: 0.1873, Test Loss: 0.5732,  Train Acc: 0.9081, Test Acc: 0.7707, Test f1: 0.8032, AUC: class-0>>0.9213408762638421|class-1>>0.8756728468899522|class-2>>0.9821802935010482\n",
      "Epoch: 050, Train Loss: 0.2195, Test Loss: 0.5285,  Train Acc: 0.8857, Test Acc: 0.7239, Test f1: 0.7690, AUC: class-0>>0.9164058738565238|class-1>>0.859075956937799|class-2>>0.9818658280922432\n",
      "Epoch: 051, Train Loss: 0.1808, Test Loss: 0.6045,  Train Acc: 0.9134, Test Acc: 0.7353, Test f1: 0.7801, AUC: class-0>>0.9255536831969187|class-1>>0.8680472488038278|class-2>>0.9810272536687631\n",
      "Epoch: 052, Train Loss: 0.1715, Test Loss: 0.5804,  Train Acc: 0.9126, Test Acc: 0.7576, Test f1: 0.7936, AUC: class-0>>0.9239889263360617|class-1>>0.8789623205741626|class-2>>0.9819706498951782\n",
      "Epoch: 053, Train Loss: 0.1855, Test Loss: 0.5940,  Train Acc: 0.9296, Test Acc: 0.7895, Test f1: 0.8094, AUC: class-0>>0.9197761194029852|class-1>>0.8813546650717703|class-2>>0.9819706498951782\n",
      "Epoch: 054, Train Loss: 0.2211, Test Loss: 0.6706,  Train Acc: 0.9018, Test Acc: 0.7474, Test f1: 0.7735, AUC: class-0>>0.9028045257583053|class-1>>0.8671501196172249|class-2>>0.9812368972746331\n",
      "Epoch: 055, Train Loss: 0.3269, Test Loss: 0.7637,  Train Acc: 0.8323, Test Acc: 0.7007, Test f1: 0.7457, AUC: class-0>>0.9314516129032259|class-1>>0.8683462918660287|class-2>>0.9796645702306079\n",
      "Epoch: 056, Train Loss: 0.3556, Test Loss: 0.9824,  Train Acc: 0.8575, Test Acc: 0.7154, Test f1: 0.7540, AUC: class-0>>0.9220630717380838|class-1>>0.8200508373205742|class-2>>0.9765199161425576\n",
      "Epoch: 057, Train Loss: 0.1954, Test Loss: 0.5863,  Train Acc: 0.8924, Test Acc: 0.7542, Test f1: 0.7929, AUC: class-0>>0.9339792970630718|class-1>>0.8701405502392344|class-2>>0.979979035639413\n",
      "Epoch: 058, Train Loss: 0.1664, Test Loss: 0.6540,  Train Acc: 0.9237, Test Acc: 0.7655, Test f1: 0.7982, AUC: class-0>>0.9198964853153586|class-1>>0.866552033492823|class-2>>0.9829140461215934\n",
      "Epoch: 059, Train Loss: 0.2208, Test Loss: 0.7734,  Train Acc: 0.9098, Test Acc: 0.7790, Test f1: 0.7985, AUC: class-0>>0.9156836783822822|class-1>>0.8484599282296651|class-2>>0.9810272536687632\n",
      "Epoch: 060, Train Loss: 0.1885, Test Loss: 0.6496,  Train Acc: 0.9128, Test Acc: 0.7523, Test f1: 0.7844, AUC: class-0>>0.9180909966297545|class-1>>0.8583283492822966|class-2>>0.979454926624738\n",
      "Epoch: 061, Train Loss: 0.2180, Test Loss: 0.6896,  Train Acc: 0.8818, Test Acc: 0.7238, Test f1: 0.7687, AUC: class-0>>0.931090515166105|class-1>>0.8513008373205742|class-2>>0.9784067085953879\n",
      "Epoch: 062, Train Loss: 0.1524, Test Loss: 0.5874,  Train Acc: 0.9337, Test Acc: 0.7473, Test f1: 0.7899, AUC: class-0>>0.9189335580163697|class-1>>0.8759718899521531|class-2>>0.9811320754716981\n",
      "Epoch: 063, Train Loss: 0.1442, Test Loss: 0.5945,  Train Acc: 0.9351, Test Acc: 0.7803, Test f1: 0.8113, AUC: class-0>>0.9156836783822822|class-1>>0.8764204545454546|class-2>>0.9823899371069182\n",
      "Epoch: 064, Train Loss: 0.3828, Test Loss: 1.0815,  Train Acc: 0.8250, Test Acc: 0.6949, Test f1: 0.7076, AUC: class-0>>0.8711482908040443|class-1>>0.8667015550239234|class-2>>0.9818658280922432\n",
      "Epoch: 065, Train Loss: 0.2275, Test Loss: 0.6594,  Train Acc: 0.8695, Test Acc: 0.7316, Test f1: 0.7782, AUC: class-0>>0.9274795377948966|class-1>>0.8568331339712918|class-2>>0.9772536687631027\n",
      "Epoch: 066, Train Loss: 0.2112, Test Loss: 0.7058,  Train Acc: 0.8897, Test Acc: 0.7432, Test f1: 0.7900, AUC: class-0>>0.9319330765527203|class-1>>0.8560855263157895|class-2>>0.9827044025157233\n",
      "Epoch: 067, Train Loss: 0.1396, Test Loss: 0.6397,  Train Acc: 0.9374, Test Acc: 0.7756, Test f1: 0.8056, AUC: class-0>>0.9156836783822822|class-1>>0.8794108851674641|class-2>>0.9824947589098533\n",
      "Epoch: 068, Train Loss: 0.3442, Test Loss: 0.7943,  Train Acc: 0.8065, Test Acc: 0.6806, Test f1: 0.7239, AUC: class-0>>0.9308497833413578|class-1>>0.8629635167464116|class-2>>0.9767295597484277\n",
      "Epoch: 069, Train Loss: 0.1335, Test Loss: 0.6394,  Train Acc: 0.9371, Test Acc: 0.7623, Test f1: 0.8012, AUC: class-0>>0.9209797785267212|class-1>>0.8819527511961722|class-2>>0.9838574423480084\n",
      "Epoch: 070, Train Loss: 0.1246, Test Loss: 0.7468,  Train Acc: 0.9384, Test Acc: 0.7566, Test f1: 0.7932, AUC: class-0>>0.9164058738565238|class-1>>0.8783642344497606|class-2>>0.9827044025157233\n",
      "Epoch: 071, Train Loss: 0.1368, Test Loss: 0.7338,  Train Acc: 0.9485, Test Acc: 0.7704, Test f1: 0.7985, AUC: class-0>>0.9028045257583053|class-1>>0.8761214114832536|class-2>>0.9825995807127883\n",
      "Epoch: 072, Train Loss: 0.1389, Test Loss: 0.6330,  Train Acc: 0.9497, Test Acc: 0.7751, Test f1: 0.8057, AUC: class-0>>0.9147207510832932|class-1>>0.8749252392344498|class-2>>0.9815513626834382\n",
      "Epoch: 073, Train Loss: 0.1501, Test Loss: 0.7714,  Train Acc: 0.9196, Test Acc: 0.7647, Test f1: 0.7970, AUC: class-0>>0.9121930669234473|class-1>>0.866552033492823|class-2>>0.9820754716981133\n",
      "Epoch: 074, Train Loss: 0.1413, Test Loss: 0.6671,  Train Acc: 0.9438, Test Acc: 0.7688, Test f1: 0.8074, AUC: class-0>>0.9251925854597979|class-1>>0.8698415071770336|class-2>>0.9820754716981132\n",
      "Epoch: 075, Train Loss: 0.1142, Test Loss: 0.6469,  Train Acc: 0.9664, Test Acc: 0.7576, Test f1: 0.7958, AUC: class-0>>0.9138781896966779|class-1>>0.8741776315789473|class-2>>0.9834381551362684\n",
      "Epoch: 076, Train Loss: 0.1377, Test Loss: 0.7331,  Train Acc: 0.9483, Test Acc: 0.7528, Test f1: 0.7929, AUC: class-0>>0.9249518536350506|class-1>>0.8577302631578947|class-2>>0.981236897274633\n",
      "Epoch: 077, Train Loss: 0.1806, Test Loss: 0.8309,  Train Acc: 0.9273, Test Acc: 0.7390, Test f1: 0.7599, AUC: class-0>>0.8849903707270101|class-1>>0.8675986842105263|class-2>>0.9803983228511531\n",
      "Epoch: 078, Train Loss: 0.1778, Test Loss: 0.9325,  Train Acc: 0.9176, Test Acc: 0.7676, Test f1: 0.7930, AUC: class-0>>0.8999157438613385|class-1>>0.8704395933014354|class-2>>0.980503144654088\n",
      "Epoch: 079, Train Loss: 0.1155, Test Loss: 0.6425,  Train Acc: 0.9473, Test Acc: 0.7386, Test f1: 0.7806, AUC: class-0>>0.9148411169956668|class-1>>0.8707386363636364|class-2>>0.9810272536687631\n",
      "Epoch: 080, Train Loss: 0.1035, Test Loss: 0.7593,  Train Acc: 0.9642, Test Acc: 0.7797, Test f1: 0.8043, AUC: class-0>>0.9136374578719306|class-1>>0.8771680622009569|class-2>>0.9824947589098533\n",
      "Epoch: 081, Train Loss: 0.2105, Test Loss: 0.9569,  Train Acc: 0.8920, Test Acc: 0.7573, Test f1: 0.7970, AUC: class-0>>0.9315719788155994|class-1>>0.8777661483253588|class-2>>0.979874213836478\n",
      "Epoch: 082, Train Loss: 0.0980, Test Loss: 0.7044,  Train Acc: 0.9598, Test Acc: 0.7620, Test f1: 0.7964, AUC: class-0>>0.9097857486759748|class-1>>0.873430023923445|class-2>>0.9819706498951782\n",
      "Epoch: 083, Train Loss: 0.1034, Test Loss: 0.7717,  Train Acc: 0.9662, Test Acc: 0.7416, Test f1: 0.7818, AUC: class-0>>0.9113505055368321|class-1>>0.873729066985646|class-2>>0.9824947589098533\n",
      "Epoch: 084, Train Loss: 0.1228, Test Loss: 0.7673,  Train Acc: 0.9348, Test Acc: 0.7277, Test f1: 0.7738, AUC: class-0>>0.9209797785267212|class-1>>0.8634120813397129|class-2>>0.9811320754716981\n",
      "Epoch: 085, Train Loss: 0.0889, Test Loss: 0.7036,  Train Acc: 0.9742, Test Acc: 0.7808, Test f1: 0.8152, AUC: class-0>>0.9132763601348098|class-1>>0.8720843301435407|class-2>>0.9819706498951782\n",
      "Epoch: 086, Train Loss: 0.0950, Test Loss: 0.8057,  Train Acc: 0.9666, Test Acc: 0.7688, Test f1: 0.7953, AUC: class-0>>0.9008786711603275|class-1>>0.8731309808612441|class-2>>0.9816561844863733\n",
      "Epoch: 087, Train Loss: 0.1570, Test Loss: 1.1239,  Train Acc: 0.9273, Test Acc: 0.7671, Test f1: 0.7800, AUC: class-0>>0.8710279248916706|class-1>>0.8683462918660287|class-2>>0.9823899371069184\n",
      "Epoch: 088, Train Loss: 0.0743, Test Loss: 0.7994,  Train Acc: 0.9832, Test Acc: 0.7616, Test f1: 0.7951, AUC: class-0>>0.9067766008666347|class-1>>0.8734300239234449|class-2>>0.9823899371069182\n",
      "Epoch: 089, Train Loss: 0.0859, Test Loss: 0.7863,  Train Acc: 0.9686, Test Acc: 0.7342, Test f1: 0.7791, AUC: class-0>>0.9120727010110736|class-1>>0.8723833732057417|class-2>>0.9810272536687632\n",
      "Epoch: 090, Train Loss: 0.0868, Test Loss: 0.8480,  Train Acc: 0.9642, Test Acc: 0.7452, Test f1: 0.7827, AUC: class-0>>0.9124337987481945|class-1>>0.8711872009569378|class-2>>0.9813417190775682\n",
      "Epoch: 091, Train Loss: 0.0724, Test Loss: 0.7537,  Train Acc: 0.9787, Test Acc: 0.7433, Test f1: 0.7794, AUC: class-0>>0.9031656234954262|class-1>>0.8768690191387559|class-2>>0.9819706498951782\n",
      "Epoch: 092, Train Loss: 0.0675, Test Loss: 0.8877,  Train Acc: 0.9776, Test Acc: 0.7630, Test f1: 0.7998, AUC: class-0>>0.9111097737120847|class-1>>0.8770185406698565|class-2>>0.9825995807127883\n",
      "Epoch: 093, Train Loss: 0.0687, Test Loss: 0.8331,  Train Acc: 0.9765, Test Acc: 0.7546, Test f1: 0.7901, AUC: class-0>>0.9013601348098218|class-1>>0.8753738038277512|class-2>>0.9827044025157233\n",
      "Epoch: 094, Train Loss: 0.0695, Test Loss: 0.7614,  Train Acc: 0.9732, Test Acc: 0.7657, Test f1: 0.8003, AUC: class-0>>0.9107486759749639|class-1>>0.8737290669856459|class-2>>0.9815513626834382\n",
      "Epoch: 095, Train Loss: 0.0635, Test Loss: 0.8349,  Train Acc: 0.9753, Test Acc: 0.7678, Test f1: 0.8038, AUC: class-0>>0.915081848820414|class-1>>0.8777661483253588|class-2>>0.9813417190775682\n",
      "Epoch: 096, Train Loss: 0.1086, Test Loss: 0.8878,  Train Acc: 0.9460, Test Acc: 0.7439, Test f1: 0.7885, AUC: class-0>>0.9183317284545017|class-1>>0.8771680622009569|class-2>>0.9817610062893082\n",
      "Epoch: 097, Train Loss: 0.0586, Test Loss: 0.8372,  Train Acc: 0.9798, Test Acc: 0.7717, Test f1: 0.8069, AUC: class-0>>0.9084617236398652|class-1>>0.8705891148325359|class-2>>0.9822851153039832\n",
      "Epoch: 098, Train Loss: 0.0928, Test Loss: 0.9295,  Train Acc: 0.9460, Test Acc: 0.7254, Test f1: 0.7727, AUC: class-0>>0.914118921521425|class-1>>0.8744766746411483|class-2>>0.9793501048218028\n",
      "Epoch: 099, Train Loss: 0.1654, Test Loss: 1.0164,  Train Acc: 0.9032, Test Acc: 0.7263, Test f1: 0.7739, AUC: class-0>>0.9039480019258546|class-1>>0.86872009569378|class-2>>0.9730607966457023\n",
      "Epoch: 100, Train Loss: 0.0580, Test Loss: 0.8430,  Train Acc: 0.9798, Test Acc: 0.7512, Test f1: 0.7908, AUC: class-0>>0.9071376986037555|class-1>>0.8756728468899522|class-2>>0.979769392033543\n",
      "Epoch: 101, Train Loss: 0.0533, Test Loss: 0.9041,  Train Acc: 0.9831, Test Acc: 0.7763, Test f1: 0.8059, AUC: class-0>>0.907258064516129|class-1>>0.8746261961722488|class-2>>0.9806079664570231\n",
      "Epoch: 102, Train Loss: 0.0454, Test Loss: 0.8554,  Train Acc: 0.9888, Test Acc: 0.7561, Test f1: 0.7935, AUC: class-0>>0.9052118440057776|class-1>>0.8740281100478469|class-2>>0.9822851153039832\n",
      "Epoch: 103, Train Loss: 0.0595, Test Loss: 1.1277,  Train Acc: 0.9776, Test Acc: 0.7704, Test f1: 0.7978, AUC: class-0>>0.8855320173326914|class-1>>0.8708133971291866|class-2>>0.9821802935010483\n",
      "Epoch: 104, Train Loss: 0.0432, Test Loss: 0.9247,  Train Acc: 0.9888, Test Acc: 0.7800, Test f1: 0.8080, AUC: class-0>>0.9083413577274916|class-1>>0.8813546650717703|class-2>>0.9812368972746331\n",
      "Epoch: 105, Train Loss: 0.0464, Test Loss: 0.9105,  Train Acc: 0.9888, Test Acc: 0.7587, Test f1: 0.7967, AUC: class-0>>0.9026841598459316|class-1>>0.8749252392344498|class-2>>0.9821802935010482\n",
      "Epoch: 106, Train Loss: 0.0439, Test Loss: 1.0610,  Train Acc: 0.9866, Test Acc: 0.7608, Test f1: 0.7882, AUC: class-0>>0.8860736639383726|class-1>>0.8741776315789473|class-2>>0.9814465408805031\n",
      "Epoch: 107, Train Loss: 0.0492, Test Loss: 1.0701,  Train Acc: 0.9777, Test Acc: 0.7482, Test f1: 0.7794, AUC: class-0>>0.8899253731343284|class-1>>0.8736543062200957|class-2>>0.9809224318658281\n",
      "Epoch: 108, Train Loss: 0.0350, Test Loss: 1.0153,  Train Acc: 0.9910, Test Acc: 0.7637, Test f1: 0.7983, AUC: class-0>>0.8944992778045258|class-1>>0.8762709330143541|class-2>>0.9827044025157232\n",
      "Epoch: 109, Train Loss: 0.0418, Test Loss: 1.0397,  Train Acc: 0.9888, Test Acc: 0.7778, Test f1: 0.8096, AUC: class-0>>0.9031656234954263|class-1>>0.8710376794258374|class-2>>0.9827044025157232\n",
      "Epoch: 110, Train Loss: 0.0357, Test Loss: 1.0708,  Train Acc: 0.9888, Test Acc: 0.7712, Test f1: 0.8032, AUC: class-0>>0.9034063553201734|class-1>>0.8738785885167464|class-2>>0.9823899371069182\n",
      "Epoch: 111, Train Loss: 0.0355, Test Loss: 1.1306,  Train Acc: 0.9933, Test Acc: 0.7660, Test f1: 0.7974, AUC: class-0>>0.8899253731343284|class-1>>0.8735795454545455|class-2>>0.9824947589098533\n",
      "Epoch: 112, Train Loss: 0.0303, Test Loss: 1.0700,  Train Acc: 0.9933, Test Acc: 0.7659, Test f1: 0.7983, AUC: class-0>>0.8952214732787676|class-1>>0.8807565789473685|class-2>>0.9831236897274633\n",
      "Epoch: 113, Train Loss: 0.0298, Test Loss: 1.0113,  Train Acc: 0.9933, Test Acc: 0.7874, Test f1: 0.8180, AUC: class-0>>0.9014805007221954|class-1>>0.8774671052631579|class-2>>0.9817610062893082\n",
      "Epoch: 114, Train Loss: 0.0375, Test Loss: 1.0184,  Train Acc: 0.9865, Test Acc: 0.7639, Test f1: 0.8029, AUC: class-0>>0.9026841598459315|class-1>>0.877168062200957|class-2>>0.9811320754716982\n",
      "Epoch: 115, Train Loss: 0.0266, Test Loss: 1.0581,  Train Acc: 0.9933, Test Acc: 0.7637, Test f1: 0.7974, AUC: class-0>>0.8988324506499759|class-1>>0.879709928229665|class-2>>0.9824947589098533\n",
      "Epoch: 116, Train Loss: 0.0271, Test Loss: 1.0721,  Train Acc: 0.9911, Test Acc: 0.7561, Test f1: 0.7917, AUC: class-0>>0.8977491574386134|class-1>>0.880607057416268|class-2>>0.9820754716981132\n",
      "Epoch: 117, Train Loss: 0.0244, Test Loss: 1.0906,  Train Acc: 0.9933, Test Acc: 0.7637, Test f1: 0.7979, AUC: class-0>>0.8882402503610978|class-1>>0.879410885167464|class-2>>0.9827044025157233\n",
      "Epoch: 118, Train Loss: 0.0281, Test Loss: 1.0535,  Train Acc: 0.9933, Test Acc: 0.7616, Test f1: 0.7945, AUC: class-0>>0.8866754935002407|class-1>>0.8818032296650717|class-2>>0.9818658280922432\n",
      "Epoch: 119, Train Loss: 0.0236, Test Loss: 1.0898,  Train Acc: 0.9933, Test Acc: 0.7598, Test f1: 0.7959, AUC: class-0>>0.8917308618199324|class-1>>0.8806070574162679|class-2>>0.9813417190775682\n",
      "Epoch: 120, Train Loss: 0.0237, Test Loss: 1.1211,  Train Acc: 0.9933, Test Acc: 0.7681, Test f1: 0.7977, AUC: class-0>>0.8857125662012517|class-1>>0.8771680622009569|class-2>>0.9822851153039833\n",
      "Epoch: 121, Train Loss: 0.0233, Test Loss: 1.1386,  Train Acc: 0.9933, Test Acc: 0.7674, Test f1: 0.8013, AUC: class-0>>0.8966658642272508|class-1>>0.8800089712918661|class-2>>0.9820754716981133\n",
      "Epoch: 122, Train Loss: 0.0248, Test Loss: 1.1516,  Train Acc: 0.9978, Test Acc: 0.7664, Test f1: 0.7969, AUC: class-0>>0.8851107366393838|class-1>>0.8785137559808612|class-2>>0.9827044025157233\n",
      "Epoch: 123, Train Loss: 0.0200, Test Loss: 1.1736,  Train Acc: 0.9933, Test Acc: 0.7566, Test f1: 0.7914, AUC: class-0>>0.8883606162734713|class-1>>0.8805322966507177|class-2>>0.9819706498951782\n",
      "Epoch: 124, Train Loss: 0.0212, Test Loss: 1.1104,  Train Acc: 0.9933, Test Acc: 0.7615, Test f1: 0.7965, AUC: class-0>>0.895642753972075|class-1>>0.8817284688995215|class-2>>0.9820754716981132\n",
      "Epoch: 125, Train Loss: 0.0188, Test Loss: 1.1915,  Train Acc: 0.9933, Test Acc: 0.7551, Test f1: 0.7875, AUC: class-0>>0.8892031776600867|class-1>>0.8816537081339713|class-2>>0.9828092243186584\n",
      "Epoch: 126, Train Loss: 0.0170, Test Loss: 1.2233,  Train Acc: 0.9933, Test Acc: 0.7529, Test f1: 0.7873, AUC: class-0>>0.8879995185363505|class-1>>0.8793361244019139|class-2>>0.9823899371069182\n",
      "Epoch: 127, Train Loss: 0.0182, Test Loss: 1.2172,  Train Acc: 0.9955, Test Acc: 0.7551, Test f1: 0.7884, AUC: class-0>>0.885953298025999|class-1>>0.8800837320574163|class-2>>0.9832285115303984\n",
      "Epoch: 128, Train Loss: 0.0178, Test Loss: 1.1989,  Train Acc: 0.9933, Test Acc: 0.7653, Test f1: 0.7993, AUC: class-0>>0.8872773230621088|class-1>>0.8797099282296651|class-2>>0.9821802935010483\n",
      "Epoch: 129, Train Loss: 0.0179, Test Loss: 1.2089,  Train Acc: 0.9933, Test Acc: 0.7668, Test f1: 0.8025, AUC: class-0>>0.8930548868560424|class-1>>0.8798594497607656|class-2>>0.9829140461215934\n",
      "Epoch: 130, Train Loss: 0.0212, Test Loss: 1.1742,  Train Acc: 0.9933, Test Acc: 0.7700, Test f1: 0.8054, AUC: class-0>>0.8922725084256139|class-1>>0.8806818181818182|class-2>>0.9807127882599581\n",
      "Epoch: 131, Train Loss: 0.0207, Test Loss: 1.3077,  Train Acc: 0.9978, Test Acc: 0.7627, Test f1: 0.7927, AUC: class-0>>0.8760832932113626|class-1>>0.8804575358851674|class-2>>0.9833333333333334\n",
      "Epoch: 132, Train Loss: 0.0149, Test Loss: 1.2482,  Train Acc: 0.9933, Test Acc: 0.7615, Test f1: 0.7969, AUC: class-0>>0.8878189696677902|class-1>>0.8814294258373206|class-2>>0.9823899371069182\n",
      "Epoch: 133, Train Loss: 0.0140, Test Loss: 1.2686,  Train Acc: 0.9978, Test Acc: 0.7561, Test f1: 0.7917, AUC: class-0>>0.8876986037554164|class-1>>0.8785885167464115|class-2>>0.9822851153039832\n",
      "Epoch: 134, Train Loss: 0.0152, Test Loss: 1.2815,  Train Acc: 0.9978, Test Acc: 0.7568, Test f1: 0.7890, AUC: class-0>>0.8789720751083294|class-1>>0.878738038277512|class-2>>0.9822851153039832\n",
      "Epoch: 135, Train Loss: 0.0165, Test Loss: 1.2987,  Train Acc: 0.9978, Test Acc: 0.7642, Test f1: 0.7967, AUC: class-0>>0.8801757342320655|class-1>>0.8788875598086124|class-2>>0.9825995807127883\n",
      "Epoch: 136, Train Loss: 0.0128, Test Loss: 1.3080,  Train Acc: 0.9955, Test Acc: 0.7561, Test f1: 0.7917, AUC: class-0>>0.8883004333172846|class-1>>0.8803827751196173|class-2>>0.9825995807127882\n",
      "Epoch: 137, Train Loss: 0.0140, Test Loss: 1.3654,  Train Acc: 0.9978, Test Acc: 0.7551, Test f1: 0.7875, AUC: class-0>>0.8794535387578238|class-1>>0.8800089712918661|class-2>>0.9833333333333334\n",
      "Epoch: 138, Train Loss: 0.0122, Test Loss: 1.3649,  Train Acc: 0.9955, Test Acc: 0.7529, Test f1: 0.7873, AUC: class-0>>0.8839672604718343|class-1>>0.8792613636363636|class-2>>0.9831236897274632\n",
      "Epoch: 139, Train Loss: 0.0131, Test Loss: 1.2921,  Train Acc: 0.9933, Test Acc: 0.7598, Test f1: 0.7963, AUC: class-0>>0.8899253731343284|class-1>>0.8812051435406699|class-2>>0.9818658280922432\n",
      "Epoch: 140, Train Loss: 0.0117, Test Loss: 1.3309,  Train Acc: 0.9955, Test Acc: 0.7561, Test f1: 0.7917, AUC: class-0>>0.8883004333172846|class-1>>0.880308014354067|class-2>>0.9824947589098533\n",
      "Epoch: 141, Train Loss: 0.0115, Test Loss: 1.3428,  Train Acc: 0.9955, Test Acc: 0.7561, Test f1: 0.7921, AUC: class-0>>0.8876986037554164|class-1>>0.8813546650717704|class-2>>0.9825995807127884\n",
      "Epoch: 142, Train Loss: 0.0117, Test Loss: 1.3700,  Train Acc: 0.9978, Test Acc: 0.7620, Test f1: 0.7964, AUC: class-0>>0.8845690900337024|class-1>>0.8801584928229665|class-2>>0.9824947589098533\n",
      "Epoch: 143, Train Loss: 0.0109, Test Loss: 1.3439,  Train Acc: 0.9978, Test Acc: 0.7561, Test f1: 0.7921, AUC: class-0>>0.8839672604718343|class-1>>0.8791118421052632|class-2>>0.9827044025157233\n",
      "Epoch: 144, Train Loss: 0.0108, Test Loss: 1.3819,  Train Acc: 0.9978, Test Acc: 0.7583, Test f1: 0.7925, AUC: class-0>>0.8828839672604718|class-1>>0.8801584928229664|class-2>>0.9828092243186582\n",
      "Epoch: 145, Train Loss: 0.0114, Test Loss: 1.3395,  Train Acc: 0.9955, Test Acc: 0.7668, Test f1: 0.8017, AUC: class-0>>0.8882402503610978|class-1>>0.8791866028708134|class-2>>0.9817610062893082\n",
      "Epoch: 146, Train Loss: 0.0105, Test Loss: 1.3702,  Train Acc: 0.9955, Test Acc: 0.7561, Test f1: 0.7921, AUC: class-0>>0.8877587867116032|class-1>>0.8792613636363635|class-2>>0.9824947589098533\n",
      "Epoch: 147, Train Loss: 0.0102, Test Loss: 1.3886,  Train Acc: 0.9978, Test Acc: 0.7507, Test f1: 0.7868, AUC: class-0>>0.8831246990852191|class-1>>0.8791118421052632|class-2>>0.9827044025157233\n",
      "Epoch: 148, Train Loss: 0.0105, Test Loss: 1.3629,  Train Acc: 0.9955, Test Acc: 0.7615, Test f1: 0.7969, AUC: class-0>>0.8860736639383726|class-1>>0.8790370813397129|class-2>>0.9819706498951782\n",
      "Epoch: 149, Train Loss: 0.0099, Test Loss: 1.3757,  Train Acc: 0.9978, Test Acc: 0.7561, Test f1: 0.7921, AUC: class-0>>0.8851709195955706|class-1>>0.8792613636363636|class-2>>0.9822851153039832\n",
      "Running cross-validation fold: 03\n",
      "Epoch: 001, Train Loss: 0.6466, Test Loss: 0.6531,  Train Acc: 0.7088, Test Acc: 0.6863, Test f1: 0.7003, AUC: class-0>>0.9272300469483568|class-1>>0.8350694444444444|class-2>>0.9661425576519916\n",
      "Saved model!\n",
      "Epoch: 002, Train Loss: 0.5519, Test Loss: 0.5617,  Train Acc: 0.6989, Test Acc: 0.7122, Test f1: 0.7299, AUC: class-0>>0.9280125195618153|class-1>>0.8389423076923077|class-2>>0.9730607966457023\n",
      "Saved model!\n",
      "Epoch: 003, Train Loss: 0.5081, Test Loss: 0.5120,  Train Acc: 0.7100, Test Acc: 0.7537, Test f1: 0.7709, AUC: class-0>>0.9323161189358372|class-1>>0.8647168803418803|class-2>>0.9743186582809225\n",
      "Saved model!\n",
      "Epoch: 004, Train Loss: 0.4712, Test Loss: 0.5024,  Train Acc: 0.7573, Test Acc: 0.7753, Test f1: 0.7933, AUC: class-0>>0.9328377673448096|class-1>>0.8655181623931623|class-2>>0.9771488469601678\n",
      "Saved model!\n",
      "Epoch: 005, Train Loss: 0.5451, Test Loss: 0.5634,  Train Acc: 0.7838, Test Acc: 0.7913, Test f1: 0.7998, AUC: class-0>>0.9314032342201356|class-1>>0.8163728632478633|class-2>>0.9751572327044026\n",
      "Saved model!\n",
      "Epoch: 006, Train Loss: 0.6440, Test Loss: 0.6461,  Train Acc: 0.7079, Test Acc: 0.7156, Test f1: 0.7545, AUC: class-0>>0.9333594157537819|class-1>>0.8512286324786325|class-2>>0.9778825995807128\n",
      "Epoch: 007, Train Loss: 0.4278, Test Loss: 0.4661,  Train Acc: 0.8074, Test Acc: 0.7903, Test f1: 0.8135, AUC: class-0>>0.9325769431403234|class-1>>0.8838141025641025|class-2>>0.979979035639413\n",
      "Saved model!\n",
      "Epoch: 008, Train Loss: 0.4035, Test Loss: 0.4428,  Train Acc: 0.7741, Test Acc: 0.7861, Test f1: 0.8146, AUC: class-0>>0.9341418883672404|class-1>>0.8965010683760685|class-2>>0.9809224318658281\n",
      "Saved model!\n",
      "Epoch: 009, Train Loss: 0.4813, Test Loss: 0.5410,  Train Acc: 0.7551, Test Acc: 0.7926, Test f1: 0.8115, AUC: class-0>>0.9364893062076161|class-1>>0.8297275641025641|class-2>>0.980503144654088\n",
      "Epoch: 010, Train Loss: 0.3909, Test Loss: 0.4745,  Train Acc: 0.8062, Test Acc: 0.8115, Test f1: 0.8298, AUC: class-0>>0.9349243609806991|class-1>>0.8798076923076923|class-2>>0.9817610062893082\n",
      "Saved model!\n",
      "Epoch: 011, Train Loss: 0.3726, Test Loss: 0.4520,  Train Acc: 0.7955, Test Acc: 0.8125, Test f1: 0.8333, AUC: class-0>>0.9346635367762129|class-1>>0.8866185897435898|class-2>>0.9809224318658281\n",
      "Saved model!\n",
      "Epoch: 012, Train Loss: 0.4281, Test Loss: 0.4588,  Train Acc: 0.7638, Test Acc: 0.7528, Test f1: 0.7857, AUC: class-0>>0.9341418883672404|class-1>>0.8943643162393162|class-2>>0.9824947589098533\n",
      "Epoch: 013, Train Loss: 0.3527, Test Loss: 0.4219,  Train Acc: 0.8260, Test Acc: 0.7923, Test f1: 0.8200, AUC: class-0>>0.9368805425143454|class-1>>0.9009081196581196|class-2>>0.9825995807127883\n",
      "Epoch: 014, Train Loss: 0.5563, Test Loss: 0.7043,  Train Acc: 0.7915, Test Acc: 0.7087, Test f1: 0.7147, AUC: class-0>>0.9342723004694835|class-1>>0.8800747863247862|class-2>>0.9825995807127883\n",
      "Epoch: 015, Train Loss: 0.4634, Test Loss: 0.5674,  Train Acc: 0.7914, Test Acc: 0.8291, Test f1: 0.8416, AUC: class-0>>0.9389671361502347|class-1>>0.8162393162393162|class-2>>0.9840670859538784\n",
      "Saved model!\n",
      "Epoch: 016, Train Loss: 0.3898, Test Loss: 0.4362,  Train Acc: 0.7843, Test Acc: 0.7874, Test f1: 0.8175, AUC: class-0>>0.9389671361502349|class-1>>0.9053151709401709|class-2>>0.9849056603773585\n",
      "Epoch: 017, Train Loss: 0.3601, Test Loss: 0.4572,  Train Acc: 0.8509, Test Acc: 0.7932, Test f1: 0.8078, AUC: class-0>>0.9353155972874284|class-1>>0.8991720085470085|class-2>>0.9844863731656185\n",
      "Epoch: 018, Train Loss: 0.3368, Test Loss: 0.4007,  Train Acc: 0.8314, Test Acc: 0.8090, Test f1: 0.8352, AUC: class-0>>0.9387063119457485|class-1>>0.9189369658119658|class-2>>0.9844863731656185\n",
      "Saved model!\n",
      "Epoch: 019, Train Loss: 0.3239, Test Loss: 0.4061,  Train Acc: 0.7998, Test Acc: 0.7962, Test f1: 0.8252, AUC: class-0>>0.9413145539906103|class-1>>0.9075854700854701|class-2>>0.9856394129979036\n",
      "Epoch: 020, Train Loss: 0.3203, Test Loss: 0.4279,  Train Acc: 0.7923, Test Acc: 0.7943, Test f1: 0.8195, AUC: class-0>>0.9427490871152844|class-1>>0.8998397435897436|class-2>>0.9833333333333333\n",
      "Epoch: 021, Train Loss: 0.3139, Test Loss: 0.3969,  Train Acc: 0.8113, Test Acc: 0.8111, Test f1: 0.8359, AUC: class-0>>0.9380542514345331|class-1>>0.9256143162393162|class-2>>0.9854297693920335\n",
      "Saved model!\n",
      "Epoch: 022, Train Loss: 0.6227, Test Loss: 0.7318,  Train Acc: 0.7442, Test Acc: 0.7677, Test f1: 0.7869, AUC: class-0>>0.942227438706312|class-1>>0.8333333333333334|class-2>>0.9830188679245283\n",
      "Epoch: 023, Train Loss: 0.3360, Test Loss: 0.4019,  Train Acc: 0.8151, Test Acc: 0.7841, Test f1: 0.8154, AUC: class-0>>0.9383150756390194|class-1>>0.9192040598290598|class-2>>0.9851153039832286\n",
      "Epoch: 024, Train Loss: 0.3095, Test Loss: 0.3956,  Train Acc: 0.8180, Test Acc: 0.7839, Test f1: 0.8150, AUC: class-0>>0.9413145539906104|class-1>>0.9165331196581196|class-2>>0.9848008385744235\n",
      "Epoch: 025, Train Loss: 0.3282, Test Loss: 0.4537,  Train Acc: 0.8181, Test Acc: 0.8173, Test f1: 0.8396, AUC: class-0>>0.9300991131977047|class-1>>0.9198717948717948|class-2>>0.9855345911949686\n",
      "Saved model!\n",
      "Epoch: 026, Train Loss: 0.3098, Test Loss: 0.4515,  Train Acc: 0.8632, Test Acc: 0.8164, Test f1: 0.8304, AUC: class-0>>0.940792905581638|class-1>>0.8856837606837606|class-2>>0.9849056603773585\n",
      "Epoch: 027, Train Loss: 0.3452, Test Loss: 0.4328,  Train Acc: 0.8168, Test Acc: 0.8104, Test f1: 0.8296, AUC: class-0>>0.9390975482524778|class-1>>0.9341613247863247|class-2>>0.9863731656184487\n",
      "Epoch: 028, Train Loss: 0.4011, Test Loss: 0.4683,  Train Acc: 0.7458, Test Acc: 0.7798, Test f1: 0.8044, AUC: class-0>>0.939227960354721|class-1>>0.9250801282051282|class-2>>0.9835429769392035\n",
      "Epoch: 029, Train Loss: 0.2969, Test Loss: 0.4382,  Train Acc: 0.8699, Test Acc: 0.8204, Test f1: 0.8346, AUC: class-0>>0.9432707355242566|class-1>>0.8931623931623932|class-2>>0.9846960167714884\n",
      "Epoch: 030, Train Loss: 0.2751, Test Loss: 0.3762,  Train Acc: 0.8411, Test Acc: 0.7861, Test f1: 0.8154, AUC: class-0>>0.942227438706312|class-1>>0.9293536324786325|class-2>>0.9853249475890986\n",
      "Epoch: 031, Train Loss: 0.3566, Test Loss: 0.4748,  Train Acc: 0.8501, Test Acc: 0.7849, Test f1: 0.7962, AUC: class-0>>0.9359676577986437|class-1>>0.8771367521367521|class-2>>0.9850104821802935\n",
      "Epoch: 032, Train Loss: 0.2751, Test Loss: 0.4560,  Train Acc: 0.8848, Test Acc: 0.8127, Test f1: 0.8308, AUC: class-0>>0.9426186750130412|class-1>>0.922676282051282|class-2>>0.9853249475890986\n",
      "Epoch: 033, Train Loss: 0.4006, Test Loss: 0.5984,  Train Acc: 0.8386, Test Acc: 0.7994, Test f1: 0.8043, AUC: class-0>>0.9401408450704226|class-1>>0.8448183760683761|class-2>>0.9861635220125786\n",
      "Epoch: 034, Train Loss: 0.2455, Test Loss: 0.4311,  Train Acc: 0.8804, Test Acc: 0.8144, Test f1: 0.8387, AUC: class-0>>0.9409233176838812|class-1>>0.9260149572649572|class-2>>0.9846960167714885\n",
      "Saved model!\n",
      "Epoch: 035, Train Loss: 0.3401, Test Loss: 0.4241,  Train Acc: 0.7965, Test Acc: 0.7889, Test f1: 0.8142, AUC: class-0>>0.936358894105373|class-1>>0.9256143162393163|class-2>>0.9862683438155136\n",
      "Epoch: 036, Train Loss: 0.2544, Test Loss: 0.3849,  Train Acc: 0.8465, Test Acc: 0.8036, Test f1: 0.8340, AUC: class-0>>0.9430099113197704|class-1>>0.9322916666666666|class-2>>0.9859538784067086\n",
      "Epoch: 037, Train Loss: 0.2472, Test Loss: 0.4090,  Train Acc: 0.8510, Test Acc: 0.8392, Test f1: 0.8598, AUC: class-0>>0.9435315597287429|class-1>>0.9264155982905983|class-2>>0.9849056603773585\n",
      "Saved model!\n",
      "Epoch: 038, Train Loss: 0.4686, Test Loss: 0.6270,  Train Acc: 0.7882, Test Acc: 0.7644, Test f1: 0.7649, AUC: class-0>>0.9397496087636933|class-1>>0.8540331196581197|class-2>>0.9868972746331237\n",
      "Epoch: 039, Train Loss: 0.2332, Test Loss: 0.4515,  Train Acc: 0.8942, Test Acc: 0.8159, Test f1: 0.8352, AUC: class-0>>0.9443140323422013|class-1>>0.9256143162393162|class-2>>0.9846960167714884\n",
      "Epoch: 040, Train Loss: 0.2722, Test Loss: 0.4048,  Train Acc: 0.8261, Test Acc: 0.8143, Test f1: 0.8376, AUC: class-0>>0.9413145539906104|class-1>>0.9321581196581197|class-2>>0.9860587002096436\n",
      "Epoch: 041, Train Loss: 0.3329, Test Loss: 0.4578,  Train Acc: 0.7825, Test Acc: 0.7924, Test f1: 0.8162, AUC: class-0>>0.942227438706312|class-1>>0.9248130341880341|class-2>>0.9857442348008386\n",
      "Epoch: 042, Train Loss: 0.2954, Test Loss: 0.5112,  Train Acc: 0.8956, Test Acc: 0.7739, Test f1: 0.7894, AUC: class-0>>0.940923317683881|class-1>>0.9098557692307692|class-2>>0.9842767295597485\n",
      "Epoch: 043, Train Loss: 0.3001, Test Loss: 0.4380,  Train Acc: 0.8279, Test Acc: 0.8265, Test f1: 0.8500, AUC: class-0>>0.9410537297861241|class-1>>0.921073717948718|class-2>>0.9836477987421384\n",
      "Epoch: 044, Train Loss: 0.2564, Test Loss: 0.4263,  Train Acc: 0.8696, Test Acc: 0.8285, Test f1: 0.8516, AUC: class-0>>0.9383150756390193|class-1>>0.9306891025641025|class-2>>0.9864779874213837\n",
      "Epoch: 045, Train Loss: 0.5208, Test Loss: 0.6409,  Train Acc: 0.6861, Test Acc: 0.6717, Test f1: 0.7134, AUC: class-0>>0.934924360980699|class-1>>0.8836805555555556|class-2>>0.9807127882599581\n",
      "Epoch: 046, Train Loss: 0.2603, Test Loss: 0.4082,  Train Acc: 0.8371, Test Acc: 0.8051, Test f1: 0.8319, AUC: class-0>>0.9419666145018257|class-1>>0.9337606837606838|class-2>>0.9857442348008385\n",
      "Epoch: 047, Train Loss: 0.2499, Test Loss: 0.4422,  Train Acc: 0.8633, Test Acc: 0.8402, Test f1: 0.8548, AUC: class-0>>0.9431403234220136|class-1>>0.905315170940171|class-2>>0.9861635220125786\n",
      "Epoch: 048, Train Loss: 0.2828, Test Loss: 0.4390,  Train Acc: 0.8192, Test Acc: 0.8019, Test f1: 0.8280, AUC: class-0>>0.9428794992175273|class-1>>0.9290865384615384|class-2>>0.9859538784067087\n",
      "Epoch: 049, Train Loss: 0.1983, Test Loss: 0.4120,  Train Acc: 0.8974, Test Acc: 0.8211, Test f1: 0.8462, AUC: class-0>>0.9410537297861242|class-1>>0.9332264957264957|class-2>>0.9858490566037736\n",
      "Epoch: 050, Train Loss: 0.2463, Test Loss: 0.4331,  Train Acc: 0.8661, Test Acc: 0.8254, Test f1: 0.8455, AUC: class-0>>0.9307511737089202|class-1>>0.9186698717948718|class-2>>0.9851153039832286\n",
      "Epoch: 051, Train Loss: 0.2268, Test Loss: 0.4330,  Train Acc: 0.8757, Test Acc: 0.8320, Test f1: 0.8515, AUC: class-0>>0.943792383933229|class-1>>0.9153311965811965|class-2>>0.9863731656184486\n",
      "Epoch: 052, Train Loss: 0.2371, Test Loss: 0.4213,  Train Acc: 0.8577, Test Acc: 0.8138, Test f1: 0.8379, AUC: class-0>>0.9427490871152843|class-1>>0.9260149572649573|class-2>>0.9855345911949687\n",
      "Epoch: 053, Train Loss: 0.1860, Test Loss: 0.3978,  Train Acc: 0.9169, Test Acc: 0.8359, Test f1: 0.8572, AUC: class-0>>0.9443140323422013|class-1>>0.9330929487179487|class-2>>0.9862683438155135\n",
      "Epoch: 054, Train Loss: 0.2825, Test Loss: 0.6024,  Train Acc: 0.8619, Test Acc: 0.7869, Test f1: 0.8016, AUC: class-0>>0.9443140323422015|class-1>>0.9341613247863247|class-2>>0.9837526205450735\n",
      "Epoch: 055, Train Loss: 0.2915, Test Loss: 0.6079,  Train Acc: 0.8786, Test Acc: 0.7936, Test f1: 0.7967, AUC: class-0>>0.9389671361502349|class-1>>0.9042467948717948|class-2>>0.9837526205450733\n",
      "Epoch: 056, Train Loss: 0.1750, Test Loss: 0.4109,  Train Acc: 0.9203, Test Acc: 0.8147, Test f1: 0.8414, AUC: class-0>>0.9413145539906104|class-1>>0.9345619658119657|class-2>>0.9868448637316563\n",
      "Epoch: 057, Train Loss: 0.1757, Test Loss: 0.4609,  Train Acc: 0.9093, Test Acc: 0.8374, Test f1: 0.8583, AUC: class-0>>0.9428794992175273|class-1>>0.9356303418803419|class-2>>0.9871069182389938\n",
      "Saved model!\n",
      "Epoch: 058, Train Loss: 0.1867, Test Loss: 0.4773,  Train Acc: 0.9198, Test Acc: 0.8122, Test f1: 0.8307, AUC: class-0>>0.9374021909233177|class-1>>0.921073717948718|class-2>>0.9859538784067086\n",
      "Epoch: 059, Train Loss: 0.1658, Test Loss: 0.4252,  Train Acc: 0.9379, Test Acc: 0.8144, Test f1: 0.8387, AUC: class-0>>0.9401408450704225|class-1>>0.9312232905982906|class-2>>0.9859538784067086\n",
      "Epoch: 060, Train Loss: 0.2148, Test Loss: 0.4788,  Train Acc: 0.8896, Test Acc: 0.8347, Test f1: 0.8568, AUC: class-0>>0.9410537297861241|class-1>>0.9333600427350427|class-2>>0.9848008385744236\n",
      "Epoch: 061, Train Loss: 0.1623, Test Loss: 0.4426,  Train Acc: 0.9290, Test Acc: 0.8226, Test f1: 0.8407, AUC: class-0>>0.9402712571726657|class-1>>0.9294871794871795|class-2>>0.9860587002096437\n",
      "Epoch: 062, Train Loss: 0.1952, Test Loss: 0.5234,  Train Acc: 0.9023, Test Acc: 0.8294, Test f1: 0.8451, AUC: class-0>>0.9402712571726656|class-1>>0.9349626068376068|class-2>>0.9853249475890986\n",
      "Epoch: 063, Train Loss: 0.3395, Test Loss: 0.5719,  Train Acc: 0.7829, Test Acc: 0.7539, Test f1: 0.7748, AUC: class-0>>0.9345331246739698|class-1>>0.9312232905982906|class-2>>0.9873165618448638\n",
      "Epoch: 064, Train Loss: 0.1502, Test Loss: 0.4374,  Train Acc: 0.9465, Test Acc: 0.8361, Test f1: 0.8547, AUC: class-0>>0.9379238393322901|class-1>>0.9305555555555556|class-2>>0.9868972746331237\n",
      "Epoch: 065, Train Loss: 0.1540, Test Loss: 0.4344,  Train Acc: 0.9145, Test Acc: 0.8399, Test f1: 0.8618, AUC: class-0>>0.9443140323422015|class-1>>0.9306891025641026|class-2>>0.9860587002096436\n",
      "Saved model!\n",
      "Epoch: 066, Train Loss: 0.1668, Test Loss: 0.4614,  Train Acc: 0.9171, Test Acc: 0.8280, Test f1: 0.8514, AUC: class-0>>0.9441836202399583|class-1>>0.9285523504273504|class-2>>0.9849056603773586\n",
      "Epoch: 067, Train Loss: 0.1771, Test Loss: 0.5389,  Train Acc: 0.9133, Test Acc: 0.8260, Test f1: 0.8404, AUC: class-0>>0.942227438706312|class-1>>0.9344284188034188|class-2>>0.9848008385744236\n",
      "Epoch: 068, Train Loss: 0.1511, Test Loss: 0.4831,  Train Acc: 0.9369, Test Acc: 0.8152, Test f1: 0.8344, AUC: class-0>>0.9401408450704226|class-1>>0.921607905982906|class-2>>0.9858490566037736\n",
      "Epoch: 069, Train Loss: 0.3549, Test Loss: 0.6317,  Train Acc: 0.8067, Test Acc: 0.7802, Test f1: 0.8079, AUC: class-0>>0.9405320813771518|class-1>>0.922275641025641|class-2>>0.9845387840670861\n",
      "Epoch: 070, Train Loss: 0.2139, Test Loss: 0.6296,  Train Acc: 0.9051, Test Acc: 0.8149, Test f1: 0.8201, AUC: class-0>>0.9411841418883672|class-1>>0.906784188034188|class-2>>0.9845911949685534\n",
      "Epoch: 071, Train Loss: 0.2524, Test Loss: 0.5264,  Train Acc: 0.8543, Test Acc: 0.7894, Test f1: 0.8162, AUC: class-0>>0.9434011476264997|class-1>>0.9250801282051283|class-2>>0.9853249475890986\n",
      "Epoch: 072, Train Loss: 0.1498, Test Loss: 0.5335,  Train Acc: 0.9419, Test Acc: 0.8206, Test f1: 0.8359, AUC: class-0>>0.9419666145018258|class-1>>0.9147970085470085|class-2>>0.9853773584905661\n",
      "Epoch: 073, Train Loss: 0.1252, Test Loss: 0.5149,  Train Acc: 0.9511, Test Acc: 0.8376, Test f1: 0.8509, AUC: class-0>>0.9430099113197704|class-1>>0.9209401709401709|class-2>>0.984748427672956\n",
      "Epoch: 074, Train Loss: 0.1337, Test Loss: 0.4898,  Train Acc: 0.9379, Test Acc: 0.8177, Test f1: 0.8413, AUC: class-0>>0.9351851851851852|class-1>>0.9290865384615385|class-2>>0.9840670859538784\n",
      "Epoch: 075, Train Loss: 0.1275, Test Loss: 0.5375,  Train Acc: 0.9445, Test Acc: 0.8354, Test f1: 0.8506, AUC: class-0>>0.942357850808555|class-1>>0.9306891025641025|class-2>>0.9850104821802936\n",
      "Epoch: 076, Train Loss: 0.1251, Test Loss: 0.4930,  Train Acc: 0.9463, Test Acc: 0.8335, Test f1: 0.8565, AUC: class-0>>0.9398800208659364|class-1>>0.9325587606837606|class-2>>0.9840670859538785\n",
      "Epoch: 077, Train Loss: 0.1070, Test Loss: 0.5680,  Train Acc: 0.9541, Test Acc: 0.8364, Test f1: 0.8541, AUC: class-0>>0.9371413667188315|class-1>>0.9253472222222222|class-2>>0.984329140461216\n",
      "Epoch: 078, Train Loss: 0.1016, Test Loss: 0.5100,  Train Acc: 0.9572, Test Acc: 0.8428, Test f1: 0.8631, AUC: class-0>>0.9424882629107981|class-1>>0.9337606837606838|class-2>>0.984329140461216\n",
      "Saved model!\n",
      "Epoch: 079, Train Loss: 0.0978, Test Loss: 0.5456,  Train Acc: 0.9567, Test Acc: 0.8038, Test f1: 0.8277, AUC: class-0>>0.9396191966614501|class-1>>0.9240117521367521|class-2>>0.9839098532494759\n",
      "Epoch: 080, Train Loss: 0.1038, Test Loss: 0.6136,  Train Acc: 0.9495, Test Acc: 0.8090, Test f1: 0.8296, AUC: class-0>>0.940923317683881|class-1>>0.922676282051282|class-2>>0.9837002096436059\n",
      "Epoch: 081, Train Loss: 0.1446, Test Loss: 0.6722,  Train Acc: 0.9164, Test Acc: 0.8267, Test f1: 0.8442, AUC: class-0>>0.9400104329681794|class-1>>0.9290865384615384|class-2>>0.9836477987421384\n",
      "Epoch: 082, Train Loss: 0.1277, Test Loss: 0.5537,  Train Acc: 0.9337, Test Acc: 0.8187, Test f1: 0.8445, AUC: class-0>>0.940792905581638|class-1>>0.936965811965812|class-2>>0.9822851153039833\n",
      "Epoch: 083, Train Loss: 0.1004, Test Loss: 0.6042,  Train Acc: 0.9505, Test Acc: 0.8251, Test f1: 0.8437, AUC: class-0>>0.9430099113197705|class-1>>0.922275641025641|class-2>>0.984224318658281\n",
      "Epoch: 084, Train Loss: 0.1058, Test Loss: 0.6094,  Train Acc: 0.9514, Test Acc: 0.8118, Test f1: 0.8383, AUC: class-0>>0.939358372456964|class-1>>0.9341613247863247|class-2>>0.9865828092243186\n",
      "Epoch: 085, Train Loss: 0.0964, Test Loss: 0.6351,  Train Acc: 0.9576, Test Acc: 0.8322, Test f1: 0.8497, AUC: class-0>>0.9370109546165883|class-1>>0.9274839743589743|class-2>>0.9835429769392035\n",
      "Epoch: 086, Train Loss: 0.1905, Test Loss: 0.8165,  Train Acc: 0.9199, Test Acc: 0.7880, Test f1: 0.7962, AUC: class-0>>0.9357068335941575|class-1>>0.9011752136752137|class-2>>0.9854297693920335\n",
      "Epoch: 087, Train Loss: 0.0724, Test Loss: 0.6069,  Train Acc: 0.9701, Test Acc: 0.8317, Test f1: 0.8497, AUC: class-0>>0.9392279603547209|class-1>>0.923744658119658|class-2>>0.984329140461216\n",
      "Epoch: 088, Train Loss: 0.0923, Test Loss: 0.6666,  Train Acc: 0.9583, Test Acc: 0.8258, Test f1: 0.8446, AUC: class-0>>0.9354460093896714|class-1>>0.921340811965812|class-2>>0.9837526205450734\n",
      "Epoch: 089, Train Loss: 0.0800, Test Loss: 0.6039,  Train Acc: 0.9683, Test Acc: 0.8071, Test f1: 0.8305, AUC: class-0>>0.9439227960354721|class-1>>0.9261485042735043|class-2>>0.9837526205450733\n",
      "Epoch: 090, Train Loss: 0.0767, Test Loss: 0.5906,  Train Acc: 0.9654, Test Acc: 0.8132, Test f1: 0.8353, AUC: class-0>>0.9431403234220136|class-1>>0.9284188034188035|class-2>>0.9835429769392033\n",
      "Epoch: 091, Train Loss: 0.1103, Test Loss: 0.6830,  Train Acc: 0.9381, Test Acc: 0.8131, Test f1: 0.8338, AUC: class-0>>0.9414449660928533|class-1>>0.923611111111111|class-2>>0.9854821802935011\n",
      "Epoch: 092, Train Loss: 0.0778, Test Loss: 0.6439,  Train Acc: 0.9640, Test Acc: 0.8248, Test f1: 0.8433, AUC: class-0>>0.9420970266040689|class-1>>0.9168002136752137|class-2>>0.9835953878406708\n",
      "Epoch: 093, Train Loss: 0.1403, Test Loss: 0.7273,  Train Acc: 0.9241, Test Acc: 0.7903, Test f1: 0.8202, AUC: class-0>>0.9422274387063119|class-1>>0.9184027777777778|class-2>>0.984853249475891\n",
      "Epoch: 094, Train Loss: 0.0963, Test Loss: 0.6662,  Train Acc: 0.9487, Test Acc: 0.7888, Test f1: 0.8162, AUC: class-0>>0.9410537297861241|class-1>>0.9249465811965811|class-2>>0.9853249475890986\n",
      "Epoch: 095, Train Loss: 0.0644, Test Loss: 0.6437,  Train Acc: 0.9778, Test Acc: 0.8295, Test f1: 0.8496, AUC: class-0>>0.9379238393322901|class-1>>0.9234775641025641|class-2>>0.9841194968553458\n",
      "Epoch: 096, Train Loss: 0.0586, Test Loss: 0.6504,  Train Acc: 0.9773, Test Acc: 0.8394, Test f1: 0.8587, AUC: class-0>>0.9401408450704225|class-1>>0.9290865384615384|class-2>>0.9860062893081762\n",
      "Epoch: 097, Train Loss: 0.0612, Test Loss: 0.7554,  Train Acc: 0.9743, Test Acc: 0.8384, Test f1: 0.8550, AUC: class-0>>0.9355764214919144|class-1>>0.9165331196581197|class-2>>0.9841719077568134\n",
      "Epoch: 098, Train Loss: 0.0902, Test Loss: 0.6934,  Train Acc: 0.9479, Test Acc: 0.7933, Test f1: 0.8169, AUC: class-0>>0.9344027125717266|class-1>>0.9163995726495726|class-2>>0.9846960167714884\n",
      "Epoch: 099, Train Loss: 0.0564, Test Loss: 0.7102,  Train Acc: 0.9794, Test Acc: 0.8357, Test f1: 0.8545, AUC: class-0>>0.9362284820031299|class-1>>0.9155982905982906|class-2>>0.9846960167714884\n",
      "Epoch: 100, Train Loss: 0.0518, Test Loss: 0.6853,  Train Acc: 0.9773, Test Acc: 0.8391, Test f1: 0.8585, AUC: class-0>>0.9384454877412624|class-1>>0.9282852564102565|class-2>>0.9857966457023061\n",
      "Epoch: 101, Train Loss: 0.1092, Test Loss: 0.7574,  Train Acc: 0.9537, Test Acc: 0.8123, Test f1: 0.8396, AUC: class-0>>0.9371413667188316|class-1>>0.9290865384615384|class-2>>0.986006289308176\n",
      "Epoch: 102, Train Loss: 0.0623, Test Loss: 0.7489,  Train Acc: 0.9756, Test Acc: 0.8450, Test f1: 0.8601, AUC: class-0>>0.936358894105373|class-1>>0.9173344017094017|class-2>>0.9843815513626835\n",
      "Epoch: 103, Train Loss: 0.0466, Test Loss: 0.7231,  Train Acc: 0.9820, Test Acc: 0.8421, Test f1: 0.8596, AUC: class-0>>0.9390975482524778|class-1>>0.922275641025641|class-2>>0.9848532494758909\n",
      "Epoch: 104, Train Loss: 0.0550, Test Loss: 0.7312,  Train Acc: 0.9799, Test Acc: 0.8164, Test f1: 0.8387, AUC: class-0>>0.9329681794470527|class-1>>0.9202724358974359|class-2>>0.985587002096436\n",
      "Epoch: 105, Train Loss: 0.0456, Test Loss: 0.7474,  Train Acc: 0.9818, Test Acc: 0.8290, Test f1: 0.8483, AUC: class-0>>0.9400104329681794|class-1>>0.9237446581196581|class-2>>0.9855345911949684\n",
      "Epoch: 106, Train Loss: 0.0431, Test Loss: 0.7348,  Train Acc: 0.9823, Test Acc: 0.8394, Test f1: 0.8584, AUC: class-0>>0.9396191966614501|class-1>>0.9269497863247863|class-2>>0.984958071278826\n",
      "Epoch: 107, Train Loss: 0.0470, Test Loss: 0.8171,  Train Acc: 0.9802, Test Acc: 0.8295, Test f1: 0.8492, AUC: class-0>>0.9353155972874282|class-1>>0.921073717948718|class-2>>0.9853249475890985\n",
      "Epoch: 108, Train Loss: 0.0432, Test Loss: 0.8021,  Train Acc: 0.9788, Test Acc: 0.8288, Test f1: 0.8484, AUC: class-0>>0.9372717788210746|class-1>>0.9146634615384616|class-2>>0.9839622641509432\n",
      "Epoch: 109, Train Loss: 0.0496, Test Loss: 0.8018,  Train Acc: 0.9788, Test Acc: 0.8007, Test f1: 0.8255, AUC: class-0>>0.9385106937923838|class-1>>0.9159989316239316|class-2>>0.9849056603773585\n",
      "Epoch: 110, Train Loss: 0.0755, Test Loss: 0.8605,  Train Acc: 0.9669, Test Acc: 0.7795, Test f1: 0.8080, AUC: class-0>>0.927490871152843|class-1>>0.90625|class-2>>0.9856394129979036\n",
      "Epoch: 111, Train Loss: 0.0394, Test Loss: 0.7963,  Train Acc: 0.9841, Test Acc: 0.8107, Test f1: 0.8342, AUC: class-0>>0.9329681794470527|class-1>>0.9188034188034188|class-2>>0.9859538784067086\n",
      "Epoch: 112, Train Loss: 0.0386, Test Loss: 0.8273,  Train Acc: 0.9833, Test Acc: 0.8349, Test f1: 0.8532, AUC: class-0>>0.9376630151278039|class-1>>0.9193376068376069|class-2>>0.9859538784067086\n",
      "Epoch: 113, Train Loss: 0.0381, Test Loss: 0.9465,  Train Acc: 0.9836, Test Acc: 0.8349, Test f1: 0.8537, AUC: class-0>>0.9372717788210746|class-1>>0.9159989316239316|class-2>>0.9865303983228512\n",
      "Epoch: 114, Train Loss: 0.0318, Test Loss: 0.8360,  Train Acc: 0.9844, Test Acc: 0.8421, Test f1: 0.8592, AUC: class-0>>0.9371413667188314|class-1>>0.921607905982906|class-2>>0.9856394129979035\n",
      "Epoch: 115, Train Loss: 0.0416, Test Loss: 0.7947,  Train Acc: 0.9786, Test Acc: 0.8258, Test f1: 0.8467, AUC: class-0>>0.939488784559207|class-1>>0.921474358974359|class-2>>0.986530398322851\n",
      "Epoch: 116, Train Loss: 0.0440, Test Loss: 0.9068,  Train Acc: 0.9784, Test Acc: 0.8425, Test f1: 0.8597, AUC: class-0>>0.9328377673448096|class-1>>0.9209401709401709|class-2>>0.9852201257861636\n",
      "Epoch: 117, Train Loss: 0.0341, Test Loss: 0.8618,  Train Acc: 0.9833, Test Acc: 0.8285, Test f1: 0.8479, AUC: class-0>>0.9391627543035993|class-1>>0.9188034188034188|class-2>>0.9856394129979036\n",
      "Epoch: 118, Train Loss: 0.0327, Test Loss: 0.9221,  Train Acc: 0.9863, Test Acc: 0.8482, Test f1: 0.8645, AUC: class-0>>0.935641627543036|class-1>>0.9123931623931625|class-2>>0.9850628930817611\n",
      "Epoch: 119, Train Loss: 0.0390, Test Loss: 0.8691,  Train Acc: 0.9786, Test Acc: 0.8322, Test f1: 0.8526, AUC: class-0>>0.9382498695878977|class-1>>0.9180021367521367|class-2>>0.9856394129979036\n",
      "Epoch: 120, Train Loss: 0.0302, Test Loss: 0.9183,  Train Acc: 0.9868, Test Acc: 0.8487, Test f1: 0.8647, AUC: class-0>>0.9389019300991132|class-1>>0.9240117521367521|class-2>>0.985167714884696\n",
      "Saved model!\n",
      "Epoch: 121, Train Loss: 0.0264, Test Loss: 0.9154,  Train Acc: 0.9910, Test Acc: 0.8423, Test f1: 0.8597, AUC: class-0>>0.9383150756390193|class-1>>0.9242788461538461|class-2>>0.985482180293501\n",
      "Epoch: 122, Train Loss: 0.0255, Test Loss: 0.9217,  Train Acc: 0.9910, Test Acc: 0.8482, Test f1: 0.8641, AUC: class-0>>0.9364241001564945|class-1>>0.9193376068376068|class-2>>0.9848008385744236\n",
      "Epoch: 123, Train Loss: 0.0278, Test Loss: 0.8872,  Train Acc: 0.9857, Test Acc: 0.8414, Test f1: 0.8584, AUC: class-0>>0.9376630151278039|class-1>>0.9159989316239315|class-2>>0.9855345911949684\n",
      "Epoch: 124, Train Loss: 0.0226, Test Loss: 0.9158,  Train Acc: 0.9907, Test Acc: 0.8416, Test f1: 0.8587, AUC: class-0>>0.9385106937923839|class-1>>0.921340811965812|class-2>>0.9863731656184486\n",
      "Epoch: 125, Train Loss: 0.0234, Test Loss: 0.9134,  Train Acc: 0.9905, Test Acc: 0.8480, Test f1: 0.8639, AUC: class-0>>0.9372717788210746|class-1>>0.9198717948717949|class-2>>0.986006289308176\n",
      "Epoch: 126, Train Loss: 0.0240, Test Loss: 0.9408,  Train Acc: 0.9929, Test Acc: 0.8611, Test f1: 0.8746, AUC: class-0>>0.9401408450704226|class-1>>0.922542735042735|class-2>>0.9853249475890986\n",
      "Saved model!\n",
      "Epoch: 127, Train Loss: 0.0201, Test Loss: 0.9493,  Train Acc: 0.9910, Test Acc: 0.8423, Test f1: 0.8597, AUC: class-0>>0.9383802816901409|class-1>>0.9229433760683761|class-2>>0.9854297693920336\n",
      "Epoch: 128, Train Loss: 0.0204, Test Loss: 0.9253,  Train Acc: 0.9929, Test Acc: 0.8418, Test f1: 0.8590, AUC: class-0>>0.938641105894627|class-1>>0.9206730769230769|class-2>>0.985377358490566\n",
      "Epoch: 129, Train Loss: 0.0207, Test Loss: 0.9823,  Train Acc: 0.9910, Test Acc: 0.8485, Test f1: 0.8646, AUC: class-0>>0.9367501304121022|class-1>>0.9170673076923077|class-2>>0.9856918238993712\n",
      "Epoch: 130, Train Loss: 0.0203, Test Loss: 0.9561,  Train Acc: 0.9931, Test Acc: 0.8485, Test f1: 0.8646, AUC: class-0>>0.938641105894627|class-1>>0.9222756410256411|class-2>>0.9860587002096437\n",
      "Epoch: 131, Train Loss: 0.0207, Test Loss: 0.9977,  Train Acc: 0.9910, Test Acc: 0.8423, Test f1: 0.8597, AUC: class-0>>0.9345983307250912|class-1>>0.9200053418803419|class-2>>0.9857966457023062\n",
      "Epoch: 132, Train Loss: 0.0193, Test Loss: 0.9707,  Train Acc: 0.9910, Test Acc: 0.8359, Test f1: 0.8547, AUC: class-0>>0.9389019300991132|class-1>>0.9231436965811965|class-2>>0.9856918238993712\n",
      "Epoch: 133, Train Loss: 0.0179, Test Loss: 0.9877,  Train Acc: 0.9929, Test Acc: 0.8482, Test f1: 0.8645, AUC: class-0>>0.9370109546165885|class-1>>0.9200721153846154|class-2>>0.9857966457023062\n",
      "Epoch: 134, Train Loss: 0.0183, Test Loss: 0.9606,  Train Acc: 0.9929, Test Acc: 0.8547, Test f1: 0.8696, AUC: class-0>>0.9396191966614502|class-1>>0.922676282051282|class-2>>0.9857442348008387\n",
      "Epoch: 135, Train Loss: 0.0175, Test Loss: 0.9894,  Train Acc: 0.9907, Test Acc: 0.8423, Test f1: 0.8597, AUC: class-0>>0.9389671361502346|class-1>>0.9220085470085471|class-2>>0.985691823899371\n",
      "Epoch: 136, Train Loss: 0.0171, Test Loss: 0.9847,  Train Acc: 0.9907, Test Acc: 0.8485, Test f1: 0.8646, AUC: class-0>>0.9394235785080856|class-1>>0.922676282051282|class-2>>0.9858490566037736\n",
      "Epoch: 137, Train Loss: 0.0168, Test Loss: 0.9780,  Train Acc: 0.9929, Test Acc: 0.8482, Test f1: 0.8645, AUC: class-0>>0.9389671361502349|class-1>>0.921474358974359|class-2>>0.9858490566037736\n",
      "Epoch: 138, Train Loss: 0.0171, Test Loss: 1.0009,  Train Acc: 0.9931, Test Acc: 0.8423, Test f1: 0.8597, AUC: class-0>>0.9393583724569641|class-1>>0.9217414529914529|class-2>>0.985167714884696\n",
      "Epoch: 139, Train Loss: 0.0160, Test Loss: 0.9982,  Train Acc: 0.9910, Test Acc: 0.8485, Test f1: 0.8646, AUC: class-0>>0.9385758998435055|class-1>>0.9206063034188035|class-2>>0.9852725366876309\n",
      "Epoch: 140, Train Loss: 0.0172, Test Loss: 1.0222,  Train Acc: 0.9910, Test Acc: 0.8423, Test f1: 0.8597, AUC: class-0>>0.9388367240479918|class-1>>0.922542735042735|class-2>>0.985272536687631\n",
      "Epoch: 141, Train Loss: 0.0156, Test Loss: 1.0173,  Train Acc: 0.9931, Test Acc: 0.8485, Test f1: 0.8646, AUC: class-0>>0.9380542514345331|class-1>>0.9208066239316239|class-2>>0.9855870020964361\n",
      "Epoch: 142, Train Loss: 0.0164, Test Loss: 1.0014,  Train Acc: 0.9929, Test Acc: 0.8416, Test f1: 0.8592, AUC: class-0>>0.9387063119457485|class-1>>0.9198717948717948|class-2>>0.9858490566037735\n",
      "Epoch: 143, Train Loss: 0.0157, Test Loss: 1.0372,  Train Acc: 0.9931, Test Acc: 0.8485, Test f1: 0.8646, AUC: class-0>>0.9383150756390193|class-1>>0.921340811965812|class-2>>0.9852725366876309\n",
      "Epoch: 144, Train Loss: 0.0152, Test Loss: 1.0129,  Train Acc: 0.9931, Test Acc: 0.8418, Test f1: 0.8594, AUC: class-0>>0.9375326030255607|class-1>>0.9194711538461539|class-2>>0.9854821802935011\n",
      "Epoch: 145, Train Loss: 0.0151, Test Loss: 1.0427,  Train Acc: 0.9931, Test Acc: 0.8485, Test f1: 0.8646, AUC: class-0>>0.9377934272300469|class-1>>0.9207398504273505|class-2>>0.9853249475890985\n",
      "Epoch: 146, Train Loss: 0.0147, Test Loss: 1.0296,  Train Acc: 0.9929, Test Acc: 0.8421, Test f1: 0.8596, AUC: class-0>>0.9384454877412624|class-1>>0.9206730769230769|class-2>>0.985796645702306\n",
      "Epoch: 147, Train Loss: 0.0147, Test Loss: 1.0299,  Train Acc: 0.9929, Test Acc: 0.8357, Test f1: 0.8545, AUC: class-0>>0.9384454877412624|class-1>>0.9209401709401709|class-2>>0.9857442348008385\n",
      "Epoch: 148, Train Loss: 0.0148, Test Loss: 1.0318,  Train Acc: 0.9929, Test Acc: 0.8357, Test f1: 0.8545, AUC: class-0>>0.9387063119457486|class-1>>0.9204059829059829|class-2>>0.9855870020964361\n",
      "Epoch: 149, Train Loss: 0.0150, Test Loss: 1.0665,  Train Acc: 0.9910, Test Acc: 0.8485, Test f1: 0.8646, AUC: class-0>>0.9376630151278038|class-1>>0.9196714743589743|class-2>>0.9853249475890986\n"
     ]
    }
   ],
   "source": [
    "torch.save(model.state_dict(), os.path.join(weights_path, 'init_3way.pth'))\n",
    "for i in range(num_fold):\n",
    "    model.load_state_dict(torch.load(os.path.join(weights_path, 'init_3way.pth')))\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, num_epochs, 0.00001)\n",
    "    optimal_score = 0\n",
    "    print(f'Running cross-validation fold: {i:02d}')\n",
    "    graph_train  = []\n",
    "    graph_test = []\n",
    "    for k in range(num_fold):\n",
    "        if k != i: graph_train += chunks_data[k]\n",
    "        else: graph_test = chunks_data[k]\n",
    "    n_sample = []\n",
    "    counts = sample_weights(graph_train)\n",
    "    # criterion = torch.nn.CrossEntropyLoss(weight=torch.from_numpy(counts).float()).cuda()  \n",
    "    criterion = torch.nn.CrossEntropyLoss().cuda()  \n",
    "    # criterion = nn.MSELoss()\n",
    "    for epoch in range(1, num_epochs):\n",
    "        shuffle(graph_train)\n",
    "        train(graph_train)\n",
    "        scheduler.step()    \n",
    "        train_loss, train_acc, _, aucs = test(graph_train)\n",
    "        test_loss, test_acc, avg_f1, aucs = test(graph_test)\n",
    "        log_msg = f'Epoch: {epoch:03d}, Train Loss: {train_loss:.4f}, Test Loss: {test_loss:.4f},  Train Acc: {train_acc:.4f}, Test Acc: {test_acc:.4f}, Test f1: {avg_f1:.4f}' \\\n",
    "        + ', AUC: ' + '|'.join('class-{}>>{}'.format(*k) for k in enumerate(aucs))\n",
    "        print(log_msg)\n",
    "        sum_metrics = (avg_f1+test_acc+sum(aucs)/3)/3\n",
    "        if sum_metrics > optimal_score:\n",
    "            optimal_score = sum_metrics\n",
    "            os.makedirs(weights_path, exist_ok=True)\n",
    "            torch.save(model.state_dict(), os.path.join(weights_path, f'{weights_name}_fold{i:02d}.pth'))\n",
    "            print('Saved model!')\n",
    "            with open(os.path.join(weights_path, f'{weights_name}_fold{i:02d}.txt'), \"w\") as text_file:\n",
    "                print(log_msg, file=text_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f31d9168",
   "metadata": {},
   "source": [
    "### Read result summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "10904f65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation results of GCN_TMA_3way: Acc 0.842775, f1 0.863925, AUC Normal 0.9534665360024511, AUC CP 0.9216169340228997, AUC PDAC 0.9849745731491217\n"
     ]
    }
   ],
   "source": [
    "log_files = glob(os.path.join(weights_path, f'{weights_name}_fold*.txt'))\n",
    "results = []\n",
    "for log_file in log_files:\n",
    "    with open(log_file) as text_file:\n",
    "        lines = text_file.readlines()[0]\n",
    "        results.append([float(i) for i in re.findall(\"\\d+\\.\\d+\", lines)[3:]])\n",
    "results = np.vstack(results).mean(0)\n",
    "print(f'Cross-validation results of {weights_name}: Acc {results[0]}, f1 {results[1]}, AUC Normal {results[2]}, AUC CP {results[3]}, AUC PDAC {results[4]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "f1613ad5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation results of MLP_TMA_3way: Acc 0.844725, f1 0.865525, AUC Normal 0.951784127205279, AUC CP 0.9160509399202613, AUC PDAC 0.9825778472691606\n"
     ]
    }
   ],
   "source": [
    "log_files = glob(os.path.join(weights_path, f'{weights_name}_fold*.txt'))\n",
    "results = []\n",
    "for log_file in log_files:\n",
    "    with open(log_file) as text_file:\n",
    "        lines = text_file.readlines()[0]\n",
    "        results.append([float(i) for i in re.findall(\"\\d+\\.\\d+\", lines)[3:]])\n",
    "results = np.vstack(results).mean(0)\n",
    "print(f'Cross-validation results of {weights_name}: Acc {results[0]}, f1 {results[1]}, AUC Normal {results[2]}, AUC CP {results[3]}, AUC PDAC {results[4]}')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "de88db708fd7569f2666ff37f921dc0f88a8459546879a55f00d65d0006c4c3e"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('pt')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
