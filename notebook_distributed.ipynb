{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0df9abff-b363-48fd-8cd4-873ce5a7145c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "from os.path import join, basename, splitext\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.transforms import RadiusGraph\n",
    "from skimage import io\n",
    "from tqdm import tqdm\n",
    "from skimage import img_as_float\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from torchvision import models\n",
    "import torch.nn as nn\n",
    "from random import shuffle\n",
    "from sklearn.metrics import roc_auc_score, f1_score, balanced_accuracy_score, roc_curve\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "from torch_geometric.data import Data\n",
    "from models import GraphNet, Preprocess, AttentionPool\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fdb892be-fc61-43b4-befb-80b56c5f9d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions for visualization.\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "import networkx as nx\n",
    "\n",
    "def visualize_points(pos, edge_index=None, index=None, edge_weights=None, node_weights=None, show=True, img=None, patch_size=1, fig_text=None, box_color='palegreen'):\n",
    "    fig = plt.figure(figsize=(torch.max(pos, 0)[0].numpy()[0], torch.max(pos, 0)[0].numpy()[1]))\n",
    "        # fig_w, fig_h = plt.gcf().get_size_inches()\n",
    "    if edge_index is not None:\n",
    "        if edge_weights is None:\n",
    "            for (src, dst) in edge_index.t().tolist():\n",
    "                src = pos[src].tolist()\n",
    "                dst = pos[dst].tolist()\n",
    "                plt.plot([src[0]*patch_size+int(patch_size/2), dst[0]*patch_size+int(patch_size/2)], [src[1]*patch_size+int(patch_size/2), dst[1]*patch_size+int(patch_size/2)], linewidth=3, color='royalblue')\n",
    "        else:\n",
    "            i = 0\n",
    "            for (s, d) in edge_index.t().tolist():\n",
    "                src = pos[s].tolist()\n",
    "                dst = pos[d].tolist()\n",
    "                plt.plot([src[0]*patch_size+int(patch_size/2), dst[0]*patch_size+int(patch_size/2)], [src[1]*patch_size+int(patch_size/2), dst[1]*patch_size+int(patch_size/2)], linewidth=widths[i]*3, color='royalblue')\n",
    "                i+=1\n",
    "    if index is None:\n",
    "        if node_weights is not None:\n",
    "            for p, w in zip(pos, node_weights):\n",
    "                plt.scatter(p[0]*patch_size+int(patch_size/2), p[1]*patch_size+int(patch_size/2), s=w*500, zorder=1000, color='red')\n",
    "        else:\n",
    "            plt.scatter(pos[:, 0]*patch_size+int(patch_size/2), pos[:, 1]*patch_size+int(patch_size/2), s=500, zorder=1000, color='red')\n",
    "    else:\n",
    "        mask = torch.zeros(pos.size(0), dtype=torch.bool)\n",
    "        mask[index] = True\n",
    "        plt.scatter(pos[~mask, 0], pos[~mask, 1], s=50, color='lightgray', zorder=1000)\n",
    "        plt.scatter(pos[mask, 0], pos[mask, 1], s=50, zorder=1000)\n",
    "    plt.axis('off')\n",
    "    plt.gca().invert_yaxis()\n",
    "    if img is not None:\n",
    "        im = plt.imread(img)\n",
    "        plt.imshow(im, alpha=0.5)\n",
    "    if fig_text is not None:\n",
    "        plt.figtext(0.5, 0.1, fig_text, ha=\"center\", fontsize=18, bbox={\"facecolor\":box_color, \"alpha\":0.5, \"pad\":5} )\n",
    "    if show:\n",
    "        plt.show()\n",
    "    plt.close()\n",
    "    return fig\n",
    "    \n",
    "def visualize_projection(h, color):\n",
    "    z = TSNE(n_components=2).fit_transform(h.detach().cpu().numpy())\n",
    "\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "\n",
    "    plt.scatter(z[:, 0], z[:, 1], s=70, c=color, cmap=\"Set2\")\n",
    "    plt.show()\n",
    "    \n",
    "def visualize_grid(pos, color):\n",
    "    color = color.detach().cpu().numpy()\n",
    "    pos = pos.detach().cpu().numpy()\n",
    "    # plt.figure(figsize=(10,10))\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "\n",
    "    plt.scatter(pos[:, 0], pos[:, 1], s=70, c=color, cmap=\"Set2\")\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.show()\n",
    "    \n",
    "def visualize_graph(h, color, epoch=None, loss=None):\n",
    "    plt.figure(figsize=(7,7))\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "\n",
    "    if torch.is_tensor(h):\n",
    "        h = h.detach().cpu().numpy()\n",
    "        plt.scatter(h[:, 0], h[:, 1], s=140, c=color, cmap=\"Set2\")\n",
    "        if epoch is not None and loss is not None:\n",
    "            plt.xlabel(f'Epoch: {epoch}, Loss: {loss.item():.4f}', fontsize=16)\n",
    "    else:\n",
    "        nx.draw_networkx(h, pos=nx.spring_layout(h, seed=42), with_labels=False,\n",
    "                         node_color=color, cmap=\"Set2\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "38d25aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_dataset_to_graph(csv, class_codes, num_feat=(1, 513), compute_edges=False, weight_func=cosine_similarity, radius=1.5, remove_empty=False):\n",
    "    df = pd.read_csv(csv)\n",
    "    feat_arr = np.array(df.iloc[:, num_feat[0]:num_feat[1]])\n",
    "    pos_arr = np.array(df.iloc[:, -3:-1])\n",
    "    label_arr = np.array(class_codes.get(df.iloc[0, -1]))\n",
    "\n",
    "    # if remove_empty and os.path.splitext(os.path.basename(csv))[0]:\n",
    "    #     empty_idx = np.where( np.sum(feat_arr, 1) == 0)[0]\n",
    "    #     feat_arr[empty_idx, :] = 0\n",
    "    #     if len(empty_idx)>0: pos_arr[empty_idx, :] = pos_arr[empty_idx[0], :]\n",
    "\n",
    "    data = Data(x=torch.tensor(feat_arr, dtype=torch.float), pos=torch.tensor(pos_arr, dtype=torch.long), y=torch.tensor(label_arr, dtype=torch.long))\n",
    "    radius_graph = RadiusGraph(radius, loop=True) # 1.5 or 2\n",
    "    data = radius_graph(data) \n",
    "    if compute_edges:\n",
    "        weights = []\n",
    "        for i in range(data.edge_index.shape[1]):\n",
    "            edge = data.edge_index[:, i]\n",
    "            weight = weight_func( data.x[edge[0]].view(1, -1), data.x[edge[1]].view(1, -1) )\n",
    "            weights.append(weight)\n",
    "        # weights = exposure.rescale_intensity( np.vstack(weights).squeeze(), in_range=(0.5, 1), out_range=(0, 1) )\n",
    "        weights = np.vstack(weights).squeeze()\n",
    "        data.edge_weight = torch.tensor(weights, dtype=torch.float)\n",
    "    data.name = os.path.splitext(os.path.basename(csv))[0]\n",
    "    data.label = df.iloc[0, -1]\n",
    "    return data\n",
    "\n",
    "def compute_dataset(embedder, path, class_codes, ext='png', opt_folder=False):\n",
    "    classes = glob(join(path, '*'))\n",
    "    # print(classes) \n",
    "    data_list = [] # pos, feats, node labels, node_mask, graph label\n",
    "    # return None\n",
    "    for idx, c in enumerate(classes):\n",
    "        class_name = basename(c)\n",
    "        if opt_folder:\n",
    "            regions = glob(join(path, class_name, '*', '*'))\n",
    "        else:\n",
    "            regions = glob(join(path, class_name, '*'))\n",
    "        regions = [x for x in regions if os.path.isdir(x)]\n",
    "        for region in tqdm(regions):\n",
    "            try:\n",
    "                pos_list = []\n",
    "                feat_list = []\n",
    "                node_list = []\n",
    "                mask_list = []\n",
    "                patches = glob(join(region, '*.'+ext))\n",
    "                for patch in patches:\n",
    "                    ### patch pos -> x, y\n",
    "                    patch_data = img_as_float(io.imread(patch))\n",
    "\n",
    "                    # feat = np.mean(patch_data)\n",
    "                    with torch.no_grad():\n",
    "                        patch_tensor = torch.FloatTensor(patch_data.transpose(2, 0, 1))[None, :].cuda()\n",
    "                        feat_tensor = embedder(patch_tensor)\n",
    "                        feat = feat_tensor.detach().cpu().numpy().squeeze()\n",
    "                    masked = False\n",
    "                    node_label = np.nan\n",
    "\n",
    "                    x = int(splitext(basename(patch))[0].split('_')[0])\n",
    "                    y = int(splitext(basename(patch))[0].split('_')[1])\n",
    "                    pos_list.append((x, y))\n",
    "                    feat_list.append(feat)  \n",
    "                    node_list.append(node_label)\n",
    "                    mask_list.append(masked)\n",
    "                pos_arr = np.vstack(pos_list)\n",
    "                feat_arr = np.vstack(feat_list)\n",
    "                node_arr = np.vstack(node_list)\n",
    "                mask_arr = np.vstack(mask_list)\n",
    "                graph_label = np.array(class_codes.get(class_name))\n",
    "                graph_name = region\n",
    "                data_list.append((pos_arr, feat_arr, node_arr, mask_arr, graph_label, graph_name))\n",
    "            except:\n",
    "                print(region)\n",
    "    return data_list\n",
    "\n",
    "def sample_weights(graph_train):\n",
    "    n_sample = []\n",
    "    for graph in graph_train:\n",
    "        n_sample.append(graph.y.numpy())\n",
    "    n_sample = np.asarray(n_sample)\n",
    "    _, counts = np.unique(n_sample, return_counts=True)\n",
    "    counts = counts.max() / (10e-3+counts)\n",
    "    return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd710312-a6b0-4d71-aff4-bd597820fe2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "distributed_codes = {\n",
    "    'normal': (0, 0),\n",
    "    'pancreatitis': (0, 1),\n",
    "    'pdac': (1, 1)\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a669b6",
   "metadata": {},
   "source": [
    "### Compute representations of patches of every ROI, save them as csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5851a6b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_path = 'data_bags/TMA'\n",
    "patch_ext = 'png'\n",
    "out_path = 'dataframes/TMA'\n",
    "resnet = models.resnet18(pretrained=True)\n",
    "resnet.fc = nn.Identity()\n",
    "resnet = resnet.cuda()\n",
    "resnet.eval()\n",
    "data_list = compute_dataset(resnet, bag_path, distributed_codes, ext='png')\n",
    "for data in tqdm(data_list):\n",
    "    pos = data[0]\n",
    "    feat = data[1]\n",
    "    label = data[5].split(os.sep)[-2]\n",
    "    name = os.sep.join(data[5].split(os.sep)[-2:])\n",
    "    save_name = os.path.join(out_path, name)\n",
    "    os.makedirs(os.sep.join(save_name.split(os.sep)[:-1]), exist_ok=True)\n",
    "    df = pd.DataFrame(data=feat)\n",
    "    df = df.assign(pos_x=pd.Series(pos[:, 0]).values)\n",
    "    df = df.assign(pos_y=pd.Series(pos[:, 1]).values)\n",
    "    df = df.assign(label=pd.Series([label]*feat.shape[0]).values)\n",
    "    df.to_csv(save_name+'.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e2144da",
   "metadata": {},
   "source": [
    "### Read representations of ROIs from csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3e5e2c6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 786/786 [00:27<00:00, 28.17it/s]\n"
     ]
    }
   ],
   "source": [
    "csv_path = 'dataframes/TMA'  \n",
    "num_feat = (1, 513) #512x1 vector\n",
    "csvs = glob(os.path.join(csv_path, '*', '*.csv'))\n",
    "train_graph_list = []\n",
    "fov_normal = []\n",
    "fov_pancreatitis = []\n",
    "fov_pdac = []\n",
    "for csv in tqdm(csvs):\n",
    "    data = read_dataset_to_graph(csv, distributed_codes, num_feat, compute_edges=False, weight_func=cosine_similarity, radius=1.5)\n",
    "    train_graph_list.append(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27182867",
   "metadata": {},
   "source": [
    "### Core level separation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d6a1355d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 864x864 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_fold = 4\n",
    "while(True):\n",
    "    weight_mean = max(sample_weights(train_graph_list))\n",
    "    shuffle(train_graph_list)\n",
    "    cores_per_chunk = int(len(train_graph_list)/num_fold)\n",
    "    chunks_data = [train_graph_list[i*cores_per_chunk:(i+1)*cores_per_chunk] for i in range(num_fold)]\n",
    "    weights = []\n",
    "    for chunk_data in chunks_data:\n",
    "        weights.append(sample_weights(chunk_data))\n",
    "    max_weight = max([max(i) for i in weights])\n",
    "    if max_weight<weight_mean*1.2: break\n",
    "fig = visualize_points(train_graph_list[0].pos, edge_index=train_graph_list[0].edge_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d00191a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_label_roc(labels, predictions, pos_label=1):\n",
    "    num_class = labels.shape[1]\n",
    "    fprs = []\n",
    "    tprs = []\n",
    "    thresholds = []\n",
    "    thresholds_optimal = []\n",
    "    aucs = []\n",
    "    if len(predictions.shape)==1:\n",
    "        predictions = predictions[:, None]\n",
    "    for c in range(0, num_class):\n",
    "        label = labels[:, c]\n",
    "        prediction = predictions[:, c]\n",
    "        fpr, tpr, threshold = roc_curve(label, prediction, pos_label=pos_label)\n",
    "        fpr_optimal, tpr_optimal, threshold_optimal = optimal_thresh(fpr, tpr, threshold)\n",
    "        c_auc = roc_auc_score(label, prediction)\n",
    "        aucs.append(c_auc)\n",
    "        thresholds.append(threshold)\n",
    "        thresholds_optimal.append(threshold_optimal)\n",
    "    return aucs, thresholds, thresholds_optimal\n",
    "\n",
    "def optimal_thresh(fpr, tpr, thresholds, p=0):\n",
    "    loss = (fpr - tpr) - p * tpr / (fpr + tpr + 1)\n",
    "    idx = np.argmin(loss, axis=0)\n",
    "    return fpr[idx], tpr[idx], thresholds[idx]\n",
    "\n",
    "def accuracy_from_thresh(labels, predictions, thresh, priority_func=None): # numpy array, 2D, 2D, 1D\n",
    "    labels = copy.deepcopy(labels)\n",
    "    predictions = copy.deepcopy(predictions)\n",
    "    num_class = labels.shape[1]\n",
    "    if num_class==1:\n",
    "        class_prediction_bag = copy.deepcopy(predictions)\n",
    "        class_prediction_bag[predictions>=thresh[0]] = 1\n",
    "        class_prediction_bag[predictions<thresh[0]] = 0\n",
    "        predictions = class_prediction_bag\n",
    "    else:        \n",
    "        for i in range(num_class):\n",
    "            class_prediction_bag = copy.deepcopy(predictions[:, i])\n",
    "            class_prediction_bag[predictions[:, i]>=thresh[i]] = 1\n",
    "            class_prediction_bag[predictions[:, i]<thresh[i]] = 0\n",
    "            predictions[:, i] = class_prediction_bag\n",
    "    if priority_func is not None:\n",
    "        predictions = priority_func(predictions)\n",
    "        labels = priority_func(labels)\n",
    "        # print(predictions)\n",
    "        # print(labels)\n",
    "    # zeros = np.zeros((labels.shape[0], 1))\n",
    "    # labels = np.argmax(np.concatenate((zeros, labels), 1), 1)\n",
    "    # predictions = np.argmax(np.concatenate((zeros, predictions), 1), 1)\n",
    "    acc = balanced_accuracy_score(labels, predictions)\n",
    "    f1 = f1_score(labels, predictions, average='weighted')\n",
    "    return acc, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d2db9d53-7fce-43b2-adb2-48ca6a1295fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(loader):\n",
    "    model.train()\n",
    "    loss_avg = 0\n",
    "    count = 0\n",
    "    for data in loader:  \n",
    "        optimizer.zero_grad()  \n",
    "        out = model(\n",
    "            data.x.float().cuda(), \n",
    "            edge_index=data.edge_index.cuda(), \n",
    "            batch=torch.LongTensor(np.zeros(data.x.shape[0])).cuda(), \n",
    "            edge_weight=data.edge_weight.squeeze().cuda() if data.edge_weight is not None else None,\n",
    "            )\n",
    "        loss = criterion(out, data.y.float().cuda().view(1, -1)) \n",
    "        loss.backward()  \n",
    "        optimizer.step() \n",
    "        loss_avg += loss.item()\n",
    "        count += 1\n",
    "    return loss_avg / count\n",
    "\n",
    "def test(loader):\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        labels = []\n",
    "        preds = []\n",
    "        loss_avg = 0\n",
    "        for data in loader:   \n",
    "            out = model(\n",
    "                data.x.float().cuda(), \n",
    "                edge_index=data.edge_index.long().cuda(), \n",
    "                batch=torch.LongTensor(np.zeros(data.x.shape[0])).cuda(),\n",
    "                edge_weight=data.edge_weight.squeeze().cuda() if data.edge_weight is not None else None,\n",
    "                )  \n",
    "            loss = criterion(out, data.y.float().cuda().view(1, -1)) \n",
    "            preds.append(torch.sigmoid(out).cpu().numpy().squeeze())\n",
    "            labels.append(data.y.numpy().squeeze())\n",
    "            loss_avg += loss.item()\n",
    "        preds = np.array(preds)\n",
    "        labels = np.array(labels)\n",
    "        aucs, thresholds, thresholds_optimal = multi_label_roc(labels, preds)\n",
    "        def priority_func(pred):\n",
    "            codes = []\n",
    "            for x in pred:\n",
    "                if x[0]==1: code = 2\n",
    "                if x[0]==0 and x[1]==0: code = 0\n",
    "                if x[0]==0 and x[1]==1: code = 1\n",
    "                codes.append(code)\n",
    "            return codes\n",
    "        # priority_func = None\n",
    "        acc, f1 = accuracy_from_thresh(labels, preds, thresholds_optimal, priority_func)\n",
    "    return loss_avg / len(loader), acc, f1, aucs, thresholds_optimal "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1a2bfcff-147b-49dc-9a57-05963a3cca25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Preprocess(\n",
       "  (lin1): Sequential(\n",
       "    (0): Linear(in_features=512, out_features=1024, bias=True)\n",
       "    (1): PReLU(num_parameters=1)\n",
       "  )\n",
       "  (layers): Sequential(\n",
       "    (0): Sequential(\n",
       "      (0): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      (1): PReLU(num_parameters=1)\n",
       "    )\n",
       "  )\n",
       "  (lin2): Sequential(\n",
       "    (0): Linear(in_features=1024, out_features=512, bias=True)\n",
       "    (1): PReLU(num_parameters=1)\n",
       "  )\n",
       "  (classifier): Linear(in_features=512, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_epochs = 150\n",
    "weight_decay = 0.0005\n",
    "weights_name = 'MLP_TMA_Distributed'\n",
    "weights_path = 'weights_cv'\n",
    "os.makedirs('weights_cv', exist_ok=True)\n",
    "model = GraphNet(\n",
    "        in_channels=512, \n",
    "        out_channels=2, \n",
    "        hidden_channels=512, \n",
    "        gcn_layer='GCN', \n",
    "        graph_pool='mean', \n",
    "        drop_p0=0.0,\n",
    "        drop_p1=0.25, \n",
    "        preprocess=Preprocess(in_channels=512, hidden_channels=512, out_channels=512, n_layers=0),\n",
    "        )\n",
    "model = Preprocess(in_channels=512, hidden_channels=1024, out_channels=512, out_class=2, n_layers=1, head=True)\n",
    "lr = 0.0005\n",
    "model = model.cuda()\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "af793c0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running cross-validation fold: 00\n",
      "Epoch: 001, Train Loss: 0.2983, Test Loss: 0.3229, Train Acc: 0.7671, Test Acc: 0.7512, Test f1: 0.7630, AUC: class-0>>0.9571651902032309|class-1>>0.9342623873873873\n",
      "Saved model!\n",
      "Best thresholds ===>>> class-0>>0.8423381447792053|class-1>>0.7631375789642334\n",
      "Epoch: 002, Train Loss: 0.3977, Test Loss: 0.4339, Train Acc: 0.7675, Test Acc: 0.7749, Test f1: 0.7850, AUC: class-0>>0.9551849921834289|class-1>>0.9408783783783784\n",
      "Saved model!\n",
      "Best thresholds ===>>> class-0>>0.9807170033454895|class-1>>0.7713815569877625\n",
      "Epoch: 003, Train Loss: 0.2918, Test Loss: 0.3149, Train Acc: 0.7917, Test Acc: 0.7415, Test f1: 0.7606, AUC: class-0>>0.9515372589890568|class-1>>0.9436936936936937\n",
      "Epoch: 004, Train Loss: 0.2817, Test Loss: 0.3076, Train Acc: 0.7862, Test Acc: 0.7842, Test f1: 0.8012, AUC: class-0>>0.9582073996873373|class-1>>0.9463682432432433\n",
      "Saved model!\n",
      "Best thresholds ===>>> class-0>>0.9089021682739258|class-1>>0.5206250548362732\n",
      "Epoch: 005, Train Loss: 0.2742, Test Loss: 0.2986, Train Acc: 0.7681, Test Acc: 0.7680, Test f1: 0.7826, AUC: class-0>>0.9641479937467432|class-1>>0.9477759009009009\n",
      "Epoch: 006, Train Loss: 0.2787, Test Loss: 0.3162, Train Acc: 0.7991, Test Acc: 0.7763, Test f1: 0.7886, AUC: class-0>>0.9621677957269412|class-1>>0.9522804054054054\n",
      "Epoch: 007, Train Loss: 0.3893, Test Loss: 0.4152, Train Acc: 0.7976, Test Acc: 0.7701, Test f1: 0.7856, AUC: class-0>>0.9643564356435643|class-1>>0.9534065315315315\n",
      "Epoch: 008, Train Loss: 0.2587, Test Loss: 0.2826, Train Acc: 0.8241, Test Acc: 0.7975, Test f1: 0.8158, AUC: class-0>>0.9656070870244919|class-1>>0.9532657657657657\n",
      "Saved model!\n",
      "Best thresholds ===>>> class-0>>0.7436205744743347|class-1>>0.5070015788078308\n",
      "Epoch: 009, Train Loss: 0.2636, Test Loss: 0.2965, Train Acc: 0.8010, Test Acc: 0.7443, Test f1: 0.7580, AUC: class-0>>0.9649817613340281|class-1>>0.9542511261261261\n",
      "Epoch: 010, Train Loss: 0.3714, Test Loss: 0.3926, Train Acc: 0.7974, Test Acc: 0.7827, Test f1: 0.7997, AUC: class-0>>0.9767587285044294|class-1>>0.946509009009009\n",
      "Epoch: 011, Train Loss: 0.3632, Test Loss: 0.4125, Train Acc: 0.7923, Test Acc: 0.8078, Test f1: 0.8237, AUC: class-0>>0.9662324127149557|class-1>>0.9539695945945945\n",
      "Saved model!\n",
      "Best thresholds ===>>> class-0>>0.9745033979415894|class-1>>0.9541670083999634\n",
      "Epoch: 012, Train Loss: 0.2435, Test Loss: 0.2910, Train Acc: 0.8246, Test Acc: 0.7750, Test f1: 0.7909, AUC: class-0>>0.9666492965085982|class-1>>0.9557995495495495\n",
      "Epoch: 013, Train Loss: 0.2244, Test Loss: 0.2706, Train Acc: 0.8057, Test Acc: 0.7686, Test f1: 0.7774, AUC: class-0>>0.9718603439291298|class-1>>0.9528434684684686\n",
      "Epoch: 014, Train Loss: 0.2588, Test Loss: 0.2959, Train Acc: 0.8084, Test Acc: 0.7575, Test f1: 0.7739, AUC: class-0>>0.9722772277227723|class-1>>0.945242117117117\n",
      "Epoch: 015, Train Loss: 0.2367, Test Loss: 0.2775, Train Acc: 0.8211, Test Acc: 0.7588, Test f1: 0.7675, AUC: class-0>>0.9704012506513808|class-1>>0.9545326576576576\n",
      "Epoch: 016, Train Loss: 0.2252, Test Loss: 0.2672, Train Acc: 0.8259, Test Acc: 0.7400, Test f1: 0.7471, AUC: class-0>>0.969775924960917|class-1>>0.953125\n",
      "Epoch: 017, Train Loss: 0.2346, Test Loss: 0.2709, Train Acc: 0.8282, Test Acc: 0.7785, Test f1: 0.7955, AUC: class-0>>0.9705054715997915|class-1>>0.9569256756756757\n",
      "Epoch: 018, Train Loss: 0.2208, Test Loss: 0.2683, Train Acc: 0.8176, Test Acc: 0.7560, Test f1: 0.7640, AUC: class-0>>0.9743616466909848|class-1>>0.9538288288288288\n",
      "Epoch: 019, Train Loss: 0.2613, Test Loss: 0.3123, Train Acc: 0.8117, Test Acc: 0.7821, Test f1: 0.8023, AUC: class-0>>0.9659197498697238|class-1>>0.9579110360360361\n",
      "Epoch: 020, Train Loss: 0.2844, Test Loss: 0.3348, Train Acc: 0.8343, Test Acc: 0.7281, Test f1: 0.7284, AUC: class-0>>0.9661281917665451|class-1>>0.9546734234234234\n",
      "Epoch: 021, Train Loss: 0.2071, Test Loss: 0.2539, Train Acc: 0.8331, Test Acc: 0.7645, Test f1: 0.7794, AUC: class-0>>0.9698801459093277|class-1>>0.9553772522522523\n",
      "Epoch: 022, Train Loss: 0.2008, Test Loss: 0.2529, Train Acc: 0.8286, Test Acc: 0.7765, Test f1: 0.7945, AUC: class-0>>0.9709223553934341|class-1>>0.9557995495495495\n",
      "Epoch: 023, Train Loss: 0.1909, Test Loss: 0.2481, Train Acc: 0.8258, Test Acc: 0.7582, Test f1: 0.7726, AUC: class-0>>0.9723814486711828|class-1>>0.9529842342342342\n",
      "Epoch: 024, Train Loss: 0.2024, Test Loss: 0.2527, Train Acc: 0.8246, Test Acc: 0.7953, Test f1: 0.8136, AUC: class-0>>0.9698801459093278|class-1>>0.9566441441441441\n",
      "Epoch: 025, Train Loss: 0.1857, Test Loss: 0.2383, Train Acc: 0.8363, Test Acc: 0.7595, Test f1: 0.7669, AUC: class-0>>0.9755080771235017|class-1>>0.953125\n",
      "Epoch: 026, Train Loss: 0.1943, Test Loss: 0.2447, Train Acc: 0.8361, Test Acc: 0.7737, Test f1: 0.7927, AUC: class-0>>0.9742574257425742|class-1>>0.9548141891891891\n",
      "Epoch: 027, Train Loss: 0.2001, Test Loss: 0.2605, Train Acc: 0.8378, Test Acc: 0.7883, Test f1: 0.8069, AUC: class-0>>0.9770713913496613|class-1>>0.954954954954955\n",
      "Epoch: 028, Train Loss: 0.1943, Test Loss: 0.2562, Train Acc: 0.8259, Test Acc: 0.7800, Test f1: 0.7987, AUC: class-0>>0.9719645648775403|class-1>>0.9563626126126126\n",
      "Epoch: 029, Train Loss: 0.1808, Test Loss: 0.2505, Train Acc: 0.8287, Test Acc: 0.7765, Test f1: 0.7953, AUC: class-0>>0.9759249609171443|class-1>>0.9562218468468469\n",
      "Epoch: 030, Train Loss: 0.1959, Test Loss: 0.2532, Train Acc: 0.8504, Test Acc: 0.7833, Test f1: 0.7974, AUC: class-0>>0.9784262636789995|class-1>>0.9497466216216216\n",
      "Epoch: 031, Train Loss: 0.1825, Test Loss: 0.2405, Train Acc: 0.8462, Test Acc: 0.7667, Test f1: 0.7837, AUC: class-0>>0.9742574257425742|class-1>>0.9560810810810811\n",
      "Epoch: 032, Train Loss: 0.1906, Test Loss: 0.2541, Train Acc: 0.8692, Test Acc: 0.7498, Test f1: 0.7551, AUC: class-0>>0.9741532047941637|class-1>>0.9505912162162162\n",
      "Epoch: 033, Train Loss: 0.1774, Test Loss: 0.2454, Train Acc: 0.8454, Test Acc: 0.7435, Test f1: 0.7484, AUC: class-0>>0.9738405419489317|class-1>>0.9562218468468469\n",
      "Epoch: 034, Train Loss: 0.1886, Test Loss: 0.2640, Train Acc: 0.8654, Test Acc: 0.7763, Test f1: 0.7882, AUC: class-0>>0.9767587285044295|class-1>>0.9522804054054054\n",
      "Epoch: 035, Train Loss: 0.1954, Test Loss: 0.2689, Train Acc: 0.8430, Test Acc: 0.7667, Test f1: 0.7837, AUC: class-0>>0.9689421573736321|class-1>>0.9556587837837838\n",
      "Epoch: 036, Train Loss: 0.1731, Test Loss: 0.2697, Train Acc: 0.8579, Test Acc: 0.7400, Test f1: 0.7473, AUC: class-0>>0.9749869723814486|class-1>>0.9528434684684685\n",
      "Epoch: 037, Train Loss: 0.1699, Test Loss: 0.2547, Train Acc: 0.8870, Test Acc: 0.7372, Test f1: 0.7400, AUC: class-0>>0.972068785825951|class-1>>0.9548141891891891\n",
      "Epoch: 038, Train Loss: 0.1798, Test Loss: 0.2494, Train Acc: 0.8497, Test Acc: 0.7709, Test f1: 0.7894, AUC: class-0>>0.9783220427305889|class-1>>0.9557995495495495\n",
      "Epoch: 039, Train Loss: 0.1684, Test Loss: 0.2506, Train Acc: 0.8457, Test Acc: 0.7729, Test f1: 0.7902, AUC: class-0>>0.9726941115164147|class-1>>0.9559403153153153\n",
      "Epoch: 040, Train Loss: 0.2447, Test Loss: 0.3154, Train Acc: 0.8363, Test Acc: 0.7610, Test f1: 0.7749, AUC: class-0>>0.9684210526315788|class-1>>0.9550957207207207\n",
      "Epoch: 041, Train Loss: 0.1662, Test Loss: 0.2618, Train Acc: 0.8858, Test Acc: 0.7547, Test f1: 0.7688, AUC: class-0>>0.9758207399687338|class-1>>0.9529842342342342\n",
      "Epoch: 042, Train Loss: 0.1668, Test Loss: 0.2643, Train Acc: 0.8692, Test Acc: 0.7639, Test f1: 0.7811, AUC: class-0>>0.9767587285044295|class-1>>0.9543918918918919\n",
      "Epoch: 043, Train Loss: 0.2030, Test Loss: 0.2794, Train Acc: 0.8818, Test Acc: 0.7604, Test f1: 0.7771, AUC: class-0>>0.9709223553934341|class-1>>0.9559403153153153\n",
      "Epoch: 044, Train Loss: 0.2146, Test Loss: 0.3014, Train Acc: 0.8600, Test Acc: 0.7638, Test f1: 0.7771, AUC: class-0>>0.9706096925482022|class-1>>0.9552364864864864\n",
      "Epoch: 045, Train Loss: 0.1729, Test Loss: 0.2481, Train Acc: 0.8466, Test Acc: 0.7667, Test f1: 0.7837, AUC: class-0>>0.9731109953100573|class-1>>0.9565033783783783\n",
      "Epoch: 046, Train Loss: 0.1972, Test Loss: 0.2868, Train Acc: 0.8705, Test Acc: 0.7407, Test f1: 0.7451, AUC: class-0>>0.9772798332464825|class-1>>0.9545326576576576\n",
      "Epoch: 047, Train Loss: 0.2332, Test Loss: 0.3151, Train Acc: 0.8651, Test Acc: 0.7610, Test f1: 0.7745, AUC: class-0>>0.9698801459093278|class-1>>0.9553772522522522\n",
      "Epoch: 048, Train Loss: 0.1602, Test Loss: 0.2499, Train Acc: 0.8477, Test Acc: 0.7764, Test f1: 0.7913, AUC: class-0>>0.9747785304846273|class-1>>0.957911036036036\n",
      "Epoch: 049, Train Loss: 0.2008, Test Loss: 0.2828, Train Acc: 0.8886, Test Acc: 0.7463, Test f1: 0.7539, AUC: class-0>>0.9736321000521104|class-1>>0.9517173423423423\n",
      "Epoch: 050, Train Loss: 0.2271, Test Loss: 0.3565, Train Acc: 0.8493, Test Acc: 0.7673, Test f1: 0.7821, AUC: class-0>>0.9725898905680042|class-1>>0.9579110360360361\n",
      "Epoch: 051, Train Loss: 0.1728, Test Loss: 0.2837, Train Acc: 0.8759, Test Acc: 0.7800, Test f1: 0.7990, AUC: class-0>>0.9776967170401252|class-1>>0.9573479729729729\n",
      "Epoch: 052, Train Loss: 0.2771, Test Loss: 0.3464, Train Acc: 0.8696, Test Acc: 0.7800, Test f1: 0.7982, AUC: class-0>>0.9699843668577384|class-1>>0.9572072072072073\n",
      "Epoch: 053, Train Loss: 0.2006, Test Loss: 0.2918, Train Acc: 0.8477, Test Acc: 0.7610, Test f1: 0.7743, AUC: class-0>>0.9689421573736321|class-1>>0.957911036036036\n",
      "Epoch: 054, Train Loss: 0.1573, Test Loss: 0.2465, Train Acc: 0.8835, Test Acc: 0.7302, Test f1: 0.7353, AUC: class-0>>0.9721730067743617|class-1>>0.9565033783783784\n",
      "Epoch: 055, Train Loss: 0.2397, Test Loss: 0.3798, Train Acc: 0.8878, Test Acc: 0.7757, Test f1: 0.7901, AUC: class-0>>0.9793642522146953|class-1>>0.9546734234234234\n",
      "Epoch: 056, Train Loss: 0.1815, Test Loss: 0.2780, Train Acc: 0.8549, Test Acc: 0.7884, Test f1: 0.8087, AUC: class-0>>0.9734236581552892|class-1>>0.9581925675675675\n",
      "Epoch: 057, Train Loss: 0.1698, Test Loss: 0.2680, Train Acc: 0.8985, Test Acc: 0.7547, Test f1: 0.7676, AUC: class-0>>0.9710265763418447|class-1>>0.9565033783783783\n",
      "Epoch: 058, Train Loss: 0.1631, Test Loss: 0.2827, Train Acc: 0.8782, Test Acc: 0.7576, Test f1: 0.7738, AUC: class-0>>0.9724856696195935|class-1>>0.9541103603603603\n",
      "Epoch: 059, Train Loss: 0.2363, Test Loss: 0.3483, Train Acc: 0.8824, Test Acc: 0.8010, Test f1: 0.8214, AUC: class-0>>0.9699843668577384|class-1>>0.9566441441441442\n",
      "Epoch: 060, Train Loss: 0.1934, Test Loss: 0.2908, Train Acc: 0.8918, Test Acc: 0.7849, Test f1: 0.8051, AUC: class-0>>0.973215216258468|class-1>>0.9572072072072072\n",
      "Epoch: 061, Train Loss: 0.1893, Test Loss: 0.2994, Train Acc: 0.8819, Test Acc: 0.7953, Test f1: 0.8140, AUC: class-0>>0.9716519020323084|class-1>>0.9572072072072072\n",
      "Epoch: 062, Train Loss: 0.1473, Test Loss: 0.3222, Train Acc: 0.8893, Test Acc: 0.7520, Test f1: 0.7636, AUC: class-0>>0.9710265763418447|class-1>>0.9556587837837838\n",
      "Epoch: 063, Train Loss: 0.1397, Test Loss: 0.2760, Train Acc: 0.9063, Test Acc: 0.7617, Test f1: 0.7727, AUC: class-0>>0.9738405419489318|class-1>>0.9524211711711712\n",
      "Epoch: 064, Train Loss: 0.1621, Test Loss: 0.3098, Train Acc: 0.8628, Test Acc: 0.7890, Test f1: 0.8072, AUC: class-0>>0.9749869723814486|class-1>>0.9566441441441441\n",
      "Epoch: 065, Train Loss: 0.1335, Test Loss: 0.2414, Train Acc: 0.9011, Test Acc: 0.7702, Test f1: 0.7878, AUC: class-0>>0.978843147472642|class-1>>0.9546734234234234\n",
      "Epoch: 066, Train Loss: 0.7279, Test Loss: 0.8773, Train Acc: 0.8418, Test Acc: 0.7765, Test f1: 0.7938, AUC: class-0>>0.9616466909848879|class-1>>0.9542511261261261\n",
      "Epoch: 067, Train Loss: 0.1954, Test Loss: 0.3018, Train Acc: 0.9070, Test Acc: 0.7765, Test f1: 0.7938, AUC: class-0>>0.9689421573736321|class-1>>0.9577702702702703\n",
      "Epoch: 068, Train Loss: 0.1389, Test Loss: 0.2658, Train Acc: 0.8918, Test Acc: 0.7645, Test f1: 0.7794, AUC: class-0>>0.9761334028139657|class-1>>0.9565033783783783\n",
      "Epoch: 069, Train Loss: 0.1466, Test Loss: 0.2651, Train Acc: 0.8829, Test Acc: 0.7365, Test f1: 0.7429, AUC: class-0>>0.9718603439291298|class-1>>0.954954954954955\n",
      "Epoch: 070, Train Loss: 0.1537, Test Loss: 0.2952, Train Acc: 0.8792, Test Acc: 0.7890, Test f1: 0.8077, AUC: class-0>>0.9763418447107869|class-1>>0.9565033783783784\n",
      "Epoch: 071, Train Loss: 0.1385, Test Loss: 0.2750, Train Acc: 0.8899, Test Acc: 0.7827, Test f1: 0.8008, AUC: class-0>>0.974882751433038|class-1>>0.9570664414414414\n",
      "Epoch: 072, Train Loss: 0.1480, Test Loss: 0.3037, Train Acc: 0.9101, Test Acc: 0.7547, Test f1: 0.7652, AUC: class-0>>0.9783220427305889|class-1>>0.9546734234234234\n",
      "Epoch: 073, Train Loss: 0.2597, Test Loss: 0.4708, Train Acc: 0.8994, Test Acc: 0.7680, Test f1: 0.7793, AUC: class-0>>0.9769671704012507|class-1>>0.9515765765765766\n",
      "Epoch: 074, Train Loss: 0.1280, Test Loss: 0.3001, Train Acc: 0.9030, Test Acc: 0.7786, Test f1: 0.7980, AUC: class-0>>0.9729025534132361|class-1>>0.9563626126126126\n",
      "Epoch: 075, Train Loss: 0.1261, Test Loss: 0.2689, Train Acc: 0.8947, Test Acc: 0.7617, Test f1: 0.7755, AUC: class-0>>0.9766545075560187|class-1>>0.9529842342342342\n",
      "Epoch: 076, Train Loss: 0.1217, Test Loss: 0.2854, Train Acc: 0.9046, Test Acc: 0.7939, Test f1: 0.8122, AUC: class-0>>0.9734236581552893|class-1>>0.9556587837837838\n",
      "Epoch: 077, Train Loss: 0.1222, Test Loss: 0.2835, Train Acc: 0.8987, Test Acc: 0.7835, Test f1: 0.8030, AUC: class-0>>0.9691505992704532|class-1>>0.9535472972972973\n",
      "Epoch: 078, Train Loss: 0.1156, Test Loss: 0.2873, Train Acc: 0.9047, Test Acc: 0.7694, Test f1: 0.7858, AUC: class-0>>0.9710265763418447|class-1>>0.9548141891891891\n",
      "Epoch: 079, Train Loss: 0.1192, Test Loss: 0.2749, Train Acc: 0.9027, Test Acc: 0.7610, Test f1: 0.7749, AUC: class-0>>0.9726941115164147|class-1>>0.9557995495495496\n",
      "Epoch: 080, Train Loss: 0.2035, Test Loss: 0.4679, Train Acc: 0.8952, Test Acc: 0.7512, Test f1: 0.7630, AUC: class-0>>0.971756122980719|class-1>>0.9550957207207207\n",
      "Epoch: 081, Train Loss: 0.1247, Test Loss: 0.2722, Train Acc: 0.8988, Test Acc: 0.7520, Test f1: 0.7636, AUC: class-0>>0.9738405419489318|class-1>>0.9531249999999999\n",
      "Epoch: 082, Train Loss: 0.1103, Test Loss: 0.2894, Train Acc: 0.9069, Test Acc: 0.7393, Test f1: 0.7459, AUC: class-0>>0.9736321000521104|class-1>>0.9542511261261262\n",
      "Epoch: 083, Train Loss: 0.1151, Test Loss: 0.2699, Train Acc: 0.9102, Test Acc: 0.7520, Test f1: 0.7655, AUC: class-0>>0.9763418447107869|class-1>>0.9538288288288288\n",
      "Epoch: 084, Train Loss: 0.1138, Test Loss: 0.2687, Train Acc: 0.8807, Test Acc: 0.7463, Test f1: 0.7546, AUC: class-0>>0.9781136008337676|class-1>>0.9519988738738738\n",
      "Epoch: 085, Train Loss: 0.2412, Test Loss: 0.4186, Train Acc: 0.8540, Test Acc: 0.7422, Test f1: 0.7509, AUC: class-0>>0.9585200625325689|class-1>>0.9545326576576577\n",
      "Epoch: 086, Train Loss: 0.1148, Test Loss: 0.2900, Train Acc: 0.9035, Test Acc: 0.7561, Test f1: 0.7654, AUC: class-0>>0.9772798332464826|class-1>>0.9500281531531531\n",
      "Epoch: 087, Train Loss: 0.1092, Test Loss: 0.2913, Train Acc: 0.9204, Test Acc: 0.7765, Test f1: 0.7944, AUC: class-0>>0.9766545075560188|class-1>>0.9527027027027027\n",
      "Epoch: 088, Train Loss: 0.1685, Test Loss: 0.3287, Train Acc: 0.9020, Test Acc: 0.7870, Test f1: 0.8075, AUC: class-0>>0.9702970297029703|class-1>>0.9556587837837838\n",
      "Epoch: 089, Train Loss: 0.1476, Test Loss: 0.3007, Train Acc: 0.9197, Test Acc: 0.7729, Test f1: 0.7898, AUC: class-0>>0.973215216258468|class-1>>0.9522804054054055\n",
      "Epoch: 090, Train Loss: 0.0988, Test Loss: 0.2769, Train Acc: 0.9142, Test Acc: 0.7729, Test f1: 0.7898, AUC: class-0>>0.9755080771235018|class-1>>0.9525619369369369\n",
      "Epoch: 091, Train Loss: 0.1074, Test Loss: 0.2650, Train Acc: 0.9013, Test Acc: 0.7785, Test f1: 0.7950, AUC: class-0>>0.9776967170401252|class-1>>0.9505912162162162\n",
      "Epoch: 092, Train Loss: 0.1787, Test Loss: 0.4486, Train Acc: 0.8996, Test Acc: 0.7693, Test f1: 0.7802, AUC: class-0>>0.9739447628973423|class-1>>0.9501689189189189\n",
      "Epoch: 093, Train Loss: 0.1232, Test Loss: 0.3021, Train Acc: 0.9181, Test Acc: 0.7631, Test f1: 0.7787, AUC: class-0>>0.9718603439291298|class-1>>0.954954954954955\n",
      "Epoch: 094, Train Loss: 0.1021, Test Loss: 0.2801, Train Acc: 0.9258, Test Acc: 0.7555, Test f1: 0.7697, AUC: class-0>>0.9801980198019803|class-1>>0.9518581081081081\n",
      "Epoch: 095, Train Loss: 0.0946, Test Loss: 0.2908, Train Acc: 0.9142, Test Acc: 0.7520, Test f1: 0.7648, AUC: class-0>>0.97686294945284|class-1>>0.9521396396396397\n",
      "Epoch: 096, Train Loss: 0.0926, Test Loss: 0.3158, Train Acc: 0.9154, Test Acc: 0.7653, Test f1: 0.7831, AUC: class-0>>0.972068785825951|class-1>>0.9543918918918919\n",
      "Epoch: 097, Train Loss: 0.1306, Test Loss: 0.3491, Train Acc: 0.9134, Test Acc: 0.7772, Test f1: 0.7961, AUC: class-0>>0.9667535174570089|class-1>>0.9562218468468467\n",
      "Epoch: 098, Train Loss: 0.1093, Test Loss: 0.3177, Train Acc: 0.8965, Test Acc: 0.7639, Test f1: 0.7818, AUC: class-0>>0.9775924960917144|class-1>>0.9515765765765766\n",
      "Epoch: 099, Train Loss: 0.0908, Test Loss: 0.3213, Train Acc: 0.9241, Test Acc: 0.7807, Test f1: 0.8016, AUC: class-0>>0.9733194372068786|class-1>>0.9535472972972974\n",
      "Epoch: 100, Train Loss: 0.0906, Test Loss: 0.3416, Train Acc: 0.9260, Test Acc: 0.7365, Test f1: 0.7435, AUC: class-0>>0.9752996352266806|class-1>>0.9515765765765766\n",
      "Epoch: 101, Train Loss: 0.1030, Test Loss: 0.3692, Train Acc: 0.9060, Test Acc: 0.7386, Test f1: 0.7497, AUC: class-0>>0.97686294945284|class-1>>0.9512950450450451\n",
      "Epoch: 102, Train Loss: 0.0847, Test Loss: 0.3269, Train Acc: 0.9276, Test Acc: 0.7449, Test f1: 0.7567, AUC: class-0>>0.9757165190203231|class-1>>0.9524211711711711\n",
      "Epoch: 103, Train Loss: 0.1027, Test Loss: 0.3865, Train Acc: 0.9049, Test Acc: 0.7590, Test f1: 0.7717, AUC: class-0>>0.9772798332464825|class-1>>0.948902027027027\n",
      "Epoch: 104, Train Loss: 0.1578, Test Loss: 0.5328, Train Acc: 0.9225, Test Acc: 0.7689, Test f1: 0.7904, AUC: class-0>>0.9756122980719124|class-1>>0.9517173423423423\n",
      "Epoch: 105, Train Loss: 0.0842, Test Loss: 0.3454, Train Acc: 0.9222, Test Acc: 0.7625, Test f1: 0.7808, AUC: class-0>>0.9747785304846275|class-1>>0.9490427927927928\n",
      "Epoch: 106, Train Loss: 0.1117, Test Loss: 0.3171, Train Acc: 0.9328, Test Acc: 0.7653, Test f1: 0.7877, AUC: class-0>>0.9756122980719124|class-1>>0.9470720720720721\n",
      "Epoch: 107, Train Loss: 0.0936, Test Loss: 0.3580, Train Acc: 0.9202, Test Acc: 0.8018, Test f1: 0.8262, AUC: class-0>>0.9766545075560188|class-1>>0.9504504504504504\n",
      "Saved model!\n",
      "Best thresholds ===>>> class-0>>0.06943394988775253|class-1>>0.3710940480232239\n",
      "Epoch: 108, Train Loss: 0.0805, Test Loss: 0.3087, Train Acc: 0.9249, Test Acc: 0.7582, Test f1: 0.7717, AUC: class-0>>0.9757165190203231|class-1>>0.9508727477477478\n",
      "Epoch: 109, Train Loss: 0.0796, Test Loss: 0.3333, Train Acc: 0.9179, Test Acc: 0.7527, Test f1: 0.7705, AUC: class-0>>0.9722772277227723|class-1>>0.9486204954954955\n",
      "Epoch: 110, Train Loss: 0.0782, Test Loss: 0.3330, Train Acc: 0.9104, Test Acc: 0.7520, Test f1: 0.7634, AUC: class-0>>0.9745700885878061|class-1>>0.948338963963964\n",
      "Epoch: 111, Train Loss: 0.0796, Test Loss: 0.3393, Train Acc: 0.9143, Test Acc: 0.7702, Test f1: 0.7911, AUC: class-0>>0.9755080771235018|class-1>>0.9470720720720721\n",
      "Epoch: 112, Train Loss: 0.0831, Test Loss: 0.3330, Train Acc: 0.9191, Test Acc: 0.7673, Test f1: 0.7843, AUC: class-0>>0.9771756122980719|class-1>>0.9453828828828829\n",
      "Epoch: 113, Train Loss: 0.0867, Test Loss: 0.3908, Train Acc: 0.9176, Test Acc: 0.7457, Test f1: 0.7595, AUC: class-0>>0.9752996352266805|class-1>>0.9507319819819819\n",
      "Epoch: 114, Train Loss: 0.0767, Test Loss: 0.3391, Train Acc: 0.9239, Test Acc: 0.7437, Test f1: 0.7638, AUC: class-0>>0.9726941115164147|class-1>>0.9448198198198198\n",
      "Epoch: 115, Train Loss: 0.0802, Test Loss: 0.3777, Train Acc: 0.9133, Test Acc: 0.7793, Test f1: 0.7997, AUC: class-0>>0.9690463783220427|class-1>>0.9498873873873874\n",
      "Epoch: 116, Train Loss: 0.0810, Test Loss: 0.3699, Train Acc: 0.9302, Test Acc: 0.7610, Test f1: 0.7776, AUC: class-0>>0.9750911933298593|class-1>>0.9443975225225224\n",
      "Epoch: 117, Train Loss: 0.0813, Test Loss: 0.3488, Train Acc: 0.9264, Test Acc: 0.7618, Test f1: 0.7792, AUC: class-0>>0.9710265763418446|class-1>>0.9470720720720721\n",
      "Epoch: 118, Train Loss: 0.0722, Test Loss: 0.3448, Train Acc: 0.9380, Test Acc: 0.7562, Test f1: 0.7749, AUC: class-0>>0.974882751433038|class-1>>0.9460867117117117\n",
      "Epoch: 119, Train Loss: 0.0709, Test Loss: 0.3510, Train Acc: 0.9268, Test Acc: 0.7625, Test f1: 0.7813, AUC: class-0>>0.9747785304846275|class-1>>0.9458051801801802\n",
      "Epoch: 120, Train Loss: 0.0826, Test Loss: 0.3792, Train Acc: 0.9344, Test Acc: 0.7955, Test f1: 0.8204, AUC: class-0>>0.9772798332464825|class-1>>0.9476351351351352\n",
      "Epoch: 121, Train Loss: 0.0913, Test Loss: 0.3845, Train Acc: 0.9339, Test Acc: 0.7499, Test f1: 0.7667, AUC: class-0>>0.969254820218864|class-1>>0.9452421171171173\n",
      "Epoch: 122, Train Loss: 0.0759, Test Loss: 0.3575, Train Acc: 0.9358, Test Acc: 0.7562, Test f1: 0.7749, AUC: class-0>>0.9739447628973423|class-1>>0.9448198198198199\n",
      "Epoch: 123, Train Loss: 0.0723, Test Loss: 0.3647, Train Acc: 0.9365, Test Acc: 0.7912, Test f1: 0.8129, AUC: class-0>>0.9733194372068785|class-1>>0.9477759009009009\n",
      "Epoch: 124, Train Loss: 0.0693, Test Loss: 0.3698, Train Acc: 0.9337, Test Acc: 0.7701, Test f1: 0.7855, AUC: class-0>>0.9741532047941636|class-1>>0.9445382882882883\n",
      "Epoch: 125, Train Loss: 0.0691, Test Loss: 0.3562, Train Acc: 0.9252, Test Acc: 0.7618, Test f1: 0.7792, AUC: class-0>>0.972068785825951|class-1>>0.9451013513513514\n",
      "Epoch: 126, Train Loss: 0.0680, Test Loss: 0.3436, Train Acc: 0.9279, Test Acc: 0.7857, Test f1: 0.8101, AUC: class-0>>0.9735278791036999|class-1>>0.9455236486486487\n",
      "Epoch: 127, Train Loss: 0.0718, Test Loss: 0.3859, Train Acc: 0.9436, Test Acc: 0.7701, Test f1: 0.7855, AUC: class-0>>0.973215216258468|class-1>>0.9429898648648649\n",
      "Epoch: 128, Train Loss: 0.0656, Test Loss: 0.3684, Train Acc: 0.9330, Test Acc: 0.7948, Test f1: 0.8174, AUC: class-0>>0.9747785304846274|class-1>>0.9455236486486487\n",
      "Epoch: 129, Train Loss: 0.0658, Test Loss: 0.3611, Train Acc: 0.9351, Test Acc: 0.7506, Test f1: 0.7661, AUC: class-0>>0.9734236581552892|class-1>>0.9456644144144145\n",
      "Epoch: 130, Train Loss: 0.0636, Test Loss: 0.3723, Train Acc: 0.9375, Test Acc: 0.7625, Test f1: 0.7813, AUC: class-0>>0.9738405419489318|class-1>>0.9448198198198198\n",
      "Epoch: 131, Train Loss: 0.0720, Test Loss: 0.3646, Train Acc: 0.9378, Test Acc: 0.7675, Test f1: 0.7909, AUC: class-0>>0.9724856696195936|class-1>>0.9448198198198198\n",
      "Epoch: 132, Train Loss: 0.0651, Test Loss: 0.3890, Train Acc: 0.9449, Test Acc: 0.7464, Test f1: 0.7640, AUC: class-0>>0.9721730067743617|class-1>>0.9436936936936936\n",
      "Epoch: 133, Train Loss: 0.0625, Test Loss: 0.3704, Train Acc: 0.9422, Test Acc: 0.7506, Test f1: 0.7661, AUC: class-0>>0.9737363210005211|class-1>>0.9453828828828829\n",
      "Epoch: 134, Train Loss: 0.0616, Test Loss: 0.3715, Train Acc: 0.9398, Test Acc: 0.7885, Test f1: 0.8117, AUC: class-0>>0.9736321000521104|class-1>>0.9438344594594594\n",
      "Epoch: 135, Train Loss: 0.0614, Test Loss: 0.3881, Train Acc: 0.9449, Test Acc: 0.7527, Test f1: 0.7711, AUC: class-0>>0.9734236581552892|class-1>>0.9425675675675675\n",
      "Epoch: 136, Train Loss: 0.0660, Test Loss: 0.4261, Train Acc: 0.9398, Test Acc: 0.7533, Test f1: 0.7690, AUC: class-0>>0.9730067743616467|class-1>>0.944115990990991\n",
      "Epoch: 137, Train Loss: 0.0602, Test Loss: 0.3787, Train Acc: 0.9449, Test Acc: 0.7822, Test f1: 0.8060, AUC: class-0>>0.9734236581552892|class-1>>0.9428490990990991\n",
      "Epoch: 138, Train Loss: 0.0626, Test Loss: 0.4064, Train Acc: 0.9473, Test Acc: 0.7590, Test f1: 0.7775, AUC: class-0>>0.9735278791036998|class-1>>0.9414414414414415\n",
      "Epoch: 139, Train Loss: 0.0597, Test Loss: 0.3904, Train Acc: 0.9449, Test Acc: 0.7527, Test f1: 0.7711, AUC: class-0>>0.9730067743616467|class-1>>0.9417229729729729\n",
      "Epoch: 140, Train Loss: 0.0601, Test Loss: 0.3873, Train Acc: 0.9449, Test Acc: 0.7499, Test f1: 0.7678, AUC: class-0>>0.972381448671183|class-1>>0.942286036036036\n",
      "Epoch: 141, Train Loss: 0.0594, Test Loss: 0.3964, Train Acc: 0.9473, Test Acc: 0.7492, Test f1: 0.7667, AUC: class-0>>0.9722772277227723|class-1>>0.9414414414414415\n",
      "Epoch: 142, Train Loss: 0.0593, Test Loss: 0.3993, Train Acc: 0.9449, Test Acc: 0.7527, Test f1: 0.7711, AUC: class-0>>0.9725898905680042|class-1>>0.9410191441441441\n",
      "Epoch: 143, Train Loss: 0.0584, Test Loss: 0.4022, Train Acc: 0.9473, Test Acc: 0.7562, Test f1: 0.7756, AUC: class-0>>0.9730067743616467|class-1>>0.9420045045045045\n",
      "Epoch: 144, Train Loss: 0.0580, Test Loss: 0.3997, Train Acc: 0.9446, Test Acc: 0.7527, Test f1: 0.7711, AUC: class-0>>0.9729025534132361|class-1>>0.9420045045045045\n",
      "Epoch: 145, Train Loss: 0.0581, Test Loss: 0.3910, Train Acc: 0.9497, Test Acc: 0.7499, Test f1: 0.7684, AUC: class-0>>0.9727983324648254|class-1>>0.9424268018018018\n",
      "Epoch: 146, Train Loss: 0.0583, Test Loss: 0.3911, Train Acc: 0.9446, Test Acc: 0.7499, Test f1: 0.7684, AUC: class-0>>0.9725898905680042|class-1>>0.9420045045045045\n",
      "Epoch: 147, Train Loss: 0.0574, Test Loss: 0.3992, Train Acc: 0.9521, Test Acc: 0.7527, Test f1: 0.7711, AUC: class-0>>0.9727983324648255|class-1>>0.9415822072072072\n",
      "Epoch: 148, Train Loss: 0.0577, Test Loss: 0.3910, Train Acc: 0.9470, Test Acc: 0.7380, Test f1: 0.7525, AUC: class-0>>0.9727983324648254|class-1>>0.9427083333333333\n",
      "Epoch: 149, Train Loss: 0.0572, Test Loss: 0.3937, Train Acc: 0.9521, Test Acc: 0.7499, Test f1: 0.7684, AUC: class-0>>0.9730067743616466|class-1>>0.9421452702702703\n",
      "Running cross-validation fold: 01\n",
      "Epoch: 001, Train Loss: 0.2971, Test Loss: 0.2998, Train Acc: 0.7561, Test Acc: 0.7999, Test f1: 0.8372, AUC: class-0>>0.9541093032957865|class-1>>0.9456164383561644\n",
      "Saved model!\n",
      "Best thresholds ===>>> class-0>>0.7888235449790955|class-1>>0.5915810465812683\n",
      "Epoch: 002, Train Loss: 0.3264, Test Loss: 0.3256, Train Acc: 0.7636, Test Acc: 0.8042, Test f1: 0.8383, AUC: class-0>>0.9597413433458489|class-1>>0.9445205479452053\n",
      "Saved model!\n",
      "Best thresholds ===>>> class-0>>0.8853447437286377|class-1>>0.7760725617408752\n",
      "Epoch: 003, Train Loss: 0.3029, Test Loss: 0.3018, Train Acc: 0.7739, Test Acc: 0.7763, Test f1: 0.8139, AUC: class-0>>0.9637046307884856|class-1>>0.946027397260274\n",
      "Epoch: 004, Train Loss: 0.3300, Test Loss: 0.3188, Train Acc: 0.7599, Test Acc: 0.7763, Test f1: 0.8139, AUC: class-0>>0.9653733833959115|class-1>>0.9483561643835615\n",
      "Epoch: 005, Train Loss: 0.2654, Test Loss: 0.2953, Train Acc: 0.8069, Test Acc: 0.8099, Test f1: 0.8423, AUC: class-0>>0.9651647893199833|class-1>>0.948904109589041\n",
      "Saved model!\n",
      "Best thresholds ===>>> class-0>>0.907498836517334|class-1>>0.6713619232177734\n",
      "Epoch: 006, Train Loss: 0.2310, Test Loss: 0.2983, Train Acc: 0.7712, Test Acc: 0.7991, Test f1: 0.8319, AUC: class-0>>0.9659991656236963|class-1>>0.9478082191780821\n",
      "Epoch: 007, Train Loss: 0.2559, Test Loss: 0.3116, Train Acc: 0.7610, Test Acc: 0.8121, Test f1: 0.8490, AUC: class-0>>0.9652690863579474|class-1>>0.9482191780821917\n",
      "Saved model!\n",
      "Best thresholds ===>>> class-0>>0.6482983231544495|class-1>>0.5452690124511719\n",
      "Epoch: 008, Train Loss: 0.5500, Test Loss: 0.5506, Train Acc: 0.8035, Test Acc: 0.7814, Test f1: 0.8200, AUC: class-0>>0.9585940759282436|class-1>>0.9212328767123288\n",
      "Epoch: 009, Train Loss: 0.2559, Test Loss: 0.2905, Train Acc: 0.7924, Test Acc: 0.8013, Test f1: 0.8387, AUC: class-0>>0.9696495619524406|class-1>>0.9505479452054795\n",
      "Epoch: 010, Train Loss: 0.3069, Test Loss: 0.3171, Train Acc: 0.7812, Test Acc: 0.8294, Test f1: 0.8553, AUC: class-0>>0.9687108886107635|class-1>>0.95\n",
      "Saved model!\n",
      "Best thresholds ===>>> class-0>>0.9646797180175781|class-1>>0.671861469745636\n",
      "Epoch: 011, Train Loss: 0.2132, Test Loss: 0.2820, Train Acc: 0.7926, Test Acc: 0.8132, Test f1: 0.8470, AUC: class-0>>0.9697538589904047|class-1>>0.9463013698630137\n",
      "Epoch: 012, Train Loss: 0.2258, Test Loss: 0.2987, Train Acc: 0.7941, Test Acc: 0.8026, Test f1: 0.8449, AUC: class-0>>0.9699624530663329|class-1>>0.9412328767123288\n",
      "Epoch: 013, Train Loss: 0.2396, Test Loss: 0.3053, Train Acc: 0.7895, Test Acc: 0.8251, Test f1: 0.8543, AUC: class-0>>0.96912807676262|class-1>>0.9476712328767124\n",
      "Epoch: 014, Train Loss: 0.2568, Test Loss: 0.3239, Train Acc: 0.8065, Test Acc: 0.8037, Test f1: 0.8420, AUC: class-0>>0.968293700458907|class-1>>0.9364383561643834\n",
      "Epoch: 015, Train Loss: 0.2097, Test Loss: 0.2683, Train Acc: 0.7967, Test Acc: 0.8208, Test f1: 0.8528, AUC: class-0>>0.9694409678765122|class-1>>0.9473972602739725\n",
      "Epoch: 016, Train Loss: 0.1994, Test Loss: 0.2816, Train Acc: 0.8006, Test Acc: 0.8154, Test f1: 0.8541, AUC: class-0>>0.9704839382561535|class-1>>0.9475342465753425\n",
      "Epoch: 017, Train Loss: 0.2104, Test Loss: 0.3045, Train Acc: 0.8198, Test Acc: 0.8078, Test f1: 0.8479, AUC: class-0>>0.9722569879015436|class-1>>0.9487671232876712\n",
      "Epoch: 018, Train Loss: 0.1967, Test Loss: 0.2668, Train Acc: 0.7985, Test Acc: 0.8175, Test f1: 0.8486, AUC: class-0>>0.9715269086357948|class-1>>0.9469863013698631\n",
      "Epoch: 019, Train Loss: 0.2356, Test Loss: 0.2734, Train Acc: 0.8150, Test Acc: 0.8089, Test f1: 0.8452, AUC: class-0>>0.9728827701293283|class-1>>0.9498630136986301\n",
      "Epoch: 020, Train Loss: 0.2323, Test Loss: 0.3106, Train Acc: 0.8084, Test Acc: 0.8069, Test f1: 0.8470, AUC: class-0>>0.9711097204839383|class-1>>0.9427397260273972\n",
      "Epoch: 021, Train Loss: 0.2072, Test Loss: 0.2626, Train Acc: 0.7968, Test Acc: 0.8004, Test f1: 0.8382, AUC: class-0>>0.9730913642052567|class-1>>0.9493150684931507\n",
      "Epoch: 022, Train Loss: 0.2258, Test Loss: 0.2705, Train Acc: 0.8020, Test Acc: 0.8026, Test f1: 0.8454, AUC: class-0>>0.9729870671672924|class-1>>0.9443835616438356\n",
      "Epoch: 023, Train Loss: 0.2071, Test Loss: 0.2973, Train Acc: 0.8170, Test Acc: 0.7809, Test f1: 0.8232, AUC: class-0>>0.9709011264080101|class-1>>0.9431506849315069\n",
      "Epoch: 024, Train Loss: 0.2653, Test Loss: 0.3080, Train Acc: 0.8263, Test Acc: 0.7743, Test f1: 0.8207, AUC: class-0>>0.972569879015436|class-1>>0.9317808219178082\n",
      "Epoch: 025, Train Loss: 0.2009, Test Loss: 0.2771, Train Acc: 0.8046, Test Acc: 0.8197, Test f1: 0.8569, AUC: class-0>>0.973404255319149|class-1>>0.9461643835616439\n",
      "Epoch: 026, Train Loss: 0.2163, Test Loss: 0.3106, Train Acc: 0.7987, Test Acc: 0.8165, Test f1: 0.8522, AUC: class-0>>0.9723612849395078|class-1>>0.9398630136986301\n",
      "Epoch: 027, Train Loss: 0.2508, Test Loss: 0.3228, Train Acc: 0.7966, Test Acc: 0.8231, Test f1: 0.8563, AUC: class-0>>0.9719440967876513|class-1>>0.9431506849315068\n",
      "Epoch: 028, Train Loss: 0.2078, Test Loss: 0.2825, Train Acc: 0.8154, Test Acc: 0.8080, Test f1: 0.8456, AUC: class-0>>0.9754901960784313|class-1>>0.9501369863013698\n",
      "Epoch: 029, Train Loss: 0.1772, Test Loss: 0.2827, Train Acc: 0.8211, Test Acc: 0.8188, Test f1: 0.8557, AUC: class-0>>0.9736128493950772|class-1>>0.947945205479452\n",
      "Epoch: 030, Train Loss: 0.1963, Test Loss: 0.2774, Train Acc: 0.8270, Test Acc: 0.8240, Test f1: 0.8580, AUC: class-0>>0.9756987901543597|class-1>>0.9517808219178082\n",
      "Saved model!\n",
      "Best thresholds ===>>> class-0>>0.8928844332695007|class-1>>0.7712374925613403\n",
      "Epoch: 031, Train Loss: 0.1989, Test Loss: 0.2863, Train Acc: 0.8401, Test Acc: 0.8145, Test f1: 0.8538, AUC: class-0>>0.973404255319149|class-1>>0.9489041095890411\n",
      "Epoch: 032, Train Loss: 0.1778, Test Loss: 0.3033, Train Acc: 0.8161, Test Acc: 0.8155, Test f1: 0.8509, AUC: class-0>>0.9732999582811848|class-1>>0.9490410958904109\n",
      "Epoch: 033, Train Loss: 0.2578, Test Loss: 0.3449, Train Acc: 0.8316, Test Acc: 0.8037, Test f1: 0.8429, AUC: class-0>>0.9744472256987902|class-1>>0.9515068493150686\n",
      "Epoch: 034, Train Loss: 0.1893, Test Loss: 0.2933, Train Acc: 0.8151, Test Acc: 0.8230, Test f1: 0.8611, AUC: class-0>>0.9746558197747184|class-1>>0.9498630136986301\n",
      "Epoch: 035, Train Loss: 0.1861, Test Loss: 0.2760, Train Acc: 0.8225, Test Acc: 0.8273, Test f1: 0.8628, AUC: class-0>>0.9738214434710054|class-1>>0.9463013698630136\n",
      "Saved model!\n",
      "Best thresholds ===>>> class-0>>0.6745116710662842|class-1>>0.7171297669410706\n",
      "Epoch: 036, Train Loss: 0.2484, Test Loss: 0.2983, Train Acc: 0.8530, Test Acc: 0.8240, Test f1: 0.8580, AUC: class-0>>0.9755944931163956|class-1>>0.9526027397260275\n",
      "Saved model!\n",
      "Best thresholds ===>>> class-0>>0.9657270908355713|class-1>>0.8345862030982971\n",
      "Epoch: 037, Train Loss: 0.2063, Test Loss: 0.3117, Train Acc: 0.8230, Test Acc: 0.8273, Test f1: 0.8628, AUC: class-0>>0.973404255319149|class-1>>0.9487671232876711\n",
      "Saved model!\n",
      "Best thresholds ===>>> class-0>>0.5584543943405151|class-1>>0.5518923401832581\n",
      "Epoch: 038, Train Loss: 0.1710, Test Loss: 0.2636, Train Acc: 0.8468, Test Acc: 0.8316, Test f1: 0.8643, AUC: class-0>>0.9758030871923237|class-1>>0.9500000000000001\n",
      "Saved model!\n",
      "Best thresholds ===>>> class-0>>0.8905661106109619|class-1>>0.769415020942688\n",
      "Epoch: 039, Train Loss: 0.2150, Test Loss: 0.2932, Train Acc: 0.8378, Test Acc: 0.8273, Test f1: 0.8628, AUC: class-0>>0.9764288694201084|class-1>>0.9521917808219178\n",
      "Epoch: 040, Train Loss: 0.1735, Test Loss: 0.2788, Train Acc: 0.8585, Test Acc: 0.8154, Test f1: 0.8557, AUC: class-0>>0.9746558197747184|class-1>>0.9519178082191782\n",
      "Epoch: 041, Train Loss: 0.1718, Test Loss: 0.2642, Train Acc: 0.8278, Test Acc: 0.7809, Test f1: 0.8239, AUC: class-0>>0.974342928660826|class-1>>0.9465753424657534\n",
      "Epoch: 042, Train Loss: 0.1839, Test Loss: 0.2880, Train Acc: 0.8573, Test Acc: 0.8102, Test f1: 0.8524, AUC: class-0>>0.9765331664580726|class-1>>0.9506849315068492\n",
      "Epoch: 043, Train Loss: 0.1718, Test Loss: 0.2679, Train Acc: 0.8454, Test Acc: 0.7809, Test f1: 0.8239, AUC: class-0>>0.975177304964539|class-1>>0.946027397260274\n",
      "Epoch: 044, Train Loss: 0.1608, Test Loss: 0.2671, Train Acc: 0.8595, Test Acc: 0.8197, Test f1: 0.8569, AUC: class-0>>0.9761159783062161|class-1>>0.9515068493150685\n",
      "Epoch: 045, Train Loss: 0.1846, Test Loss: 0.2860, Train Acc: 0.8305, Test Acc: 0.8165, Test f1: 0.8522, AUC: class-0>>0.9770546516478933|class-1>>0.9523287671232876\n",
      "Epoch: 046, Train Loss: 0.2067, Test Loss: 0.3299, Train Acc: 0.8647, Test Acc: 0.8037, Test f1: 0.8435, AUC: class-0>>0.975177304964539|class-1>>0.9501369863013698\n",
      "Epoch: 047, Train Loss: 0.1596, Test Loss: 0.2803, Train Acc: 0.8543, Test Acc: 0.8155, Test f1: 0.8516, AUC: class-0>>0.9764288694201084|class-1>>0.9495890410958905\n",
      "Epoch: 048, Train Loss: 0.1576, Test Loss: 0.2756, Train Acc: 0.8512, Test Acc: 0.8208, Test f1: 0.8539, AUC: class-0>>0.9768460575719651|class-1>>0.9489041095890411\n",
      "Epoch: 049, Train Loss: 0.1613, Test Loss: 0.3017, Train Acc: 0.8283, Test Acc: 0.8349, Test f1: 0.8691, AUC: class-0>>0.9771589486858573|class-1>>0.9489041095890409\n",
      "Saved model!\n",
      "Best thresholds ===>>> class-0>>0.9061811566352844|class-1>>0.8126007914543152\n",
      "Epoch: 050, Train Loss: 0.1689, Test Loss: 0.2992, Train Acc: 0.8628, Test Acc: 0.8197, Test f1: 0.8569, AUC: class-0>>0.9752816020025031|class-1>>0.9502739726027397\n",
      "Epoch: 051, Train Loss: 0.1616, Test Loss: 0.2762, Train Acc: 0.8603, Test Acc: 0.7885, Test f1: 0.8303, AUC: class-0>>0.9755944931163955|class-1>>0.9445205479452055\n",
      "Epoch: 052, Train Loss: 0.2342, Test Loss: 0.3514, Train Acc: 0.8339, Test Acc: 0.7994, Test f1: 0.8413, AUC: class-0>>0.9764288694201085|class-1>>0.9473972602739725\n",
      "Epoch: 053, Train Loss: 0.1614, Test Loss: 0.2942, Train Acc: 0.8429, Test Acc: 0.8069, Test f1: 0.8476, AUC: class-0>>0.9763245723821443|class-1>>0.9491780821917808\n",
      "Epoch: 054, Train Loss: 0.1616, Test Loss: 0.2993, Train Acc: 0.8616, Test Acc: 0.8089, Test f1: 0.8470, AUC: class-0>>0.9761159783062161|class-1>>0.9457534246575343\n",
      "Epoch: 055, Train Loss: 0.1636, Test Loss: 0.2834, Train Acc: 0.8419, Test Acc: 0.8121, Test f1: 0.8510, AUC: class-0>>0.9770546516478933|class-1>>0.951917808219178\n",
      "Epoch: 056, Train Loss: 0.1548, Test Loss: 0.2692, Train Acc: 0.8629, Test Acc: 0.8121, Test f1: 0.8510, AUC: class-0>>0.9766374634960368|class-1>>0.9506849315068493\n",
      "Epoch: 057, Train Loss: 0.1516, Test Loss: 0.2747, Train Acc: 0.8698, Test Acc: 0.8121, Test f1: 0.8510, AUC: class-0>>0.9764288694201084|class-1>>0.9498630136986301\n",
      "Epoch: 058, Train Loss: 0.1438, Test Loss: 0.2694, Train Acc: 0.8533, Test Acc: 0.8121, Test f1: 0.8510, AUC: class-0>>0.9770546516478932|class-1>>0.9504109589041095\n",
      "Epoch: 059, Train Loss: 0.2509, Test Loss: 0.3730, Train Acc: 0.8748, Test Acc: 0.8140, Test f1: 0.8528, AUC: class-0>>0.9755944931163956|class-1>>0.9472602739726027\n",
      "Epoch: 060, Train Loss: 0.1462, Test Loss: 0.2693, Train Acc: 0.8862, Test Acc: 0.8291, Test f1: 0.8647, AUC: class-0>>0.9767417605340007|class-1>>0.9495890410958903\n",
      "Epoch: 061, Train Loss: 0.1732, Test Loss: 0.3281, Train Acc: 0.8595, Test Acc: 0.8258, Test f1: 0.8600, AUC: class-0>>0.9763245723821443|class-1>>0.9443835616438356\n",
      "Epoch: 062, Train Loss: 0.1422, Test Loss: 0.2831, Train Acc: 0.8739, Test Acc: 0.8003, Test f1: 0.8430, AUC: class-0>>0.9767417605340009|class-1>>0.9471232876712328\n",
      "Epoch: 063, Train Loss: 0.1519, Test Loss: 0.2818, Train Acc: 0.8655, Test Acc: 0.7961, Test f1: 0.8367, AUC: class-0>>0.9763245723821443|class-1>>0.9517808219178082\n",
      "Epoch: 064, Train Loss: 0.1594, Test Loss: 0.3220, Train Acc: 0.8693, Test Acc: 0.8324, Test f1: 0.8689, AUC: class-0>>0.9758030871923237|class-1>>0.948082191780822\n",
      "Epoch: 065, Train Loss: 0.1392, Test Loss: 0.3056, Train Acc: 0.8648, Test Acc: 0.8037, Test f1: 0.8442, AUC: class-0>>0.9774718397997498|class-1>>0.9457534246575343\n",
      "Epoch: 066, Train Loss: 0.1341, Test Loss: 0.2918, Train Acc: 0.8746, Test Acc: 0.8037, Test f1: 0.8435, AUC: class-0>>0.9769503546099291|class-1>>0.9506849315068493\n",
      "Epoch: 067, Train Loss: 0.1414, Test Loss: 0.2818, Train Acc: 0.8643, Test Acc: 0.8037, Test f1: 0.8435, AUC: class-0>>0.9762202753441802|class-1>>0.9493150684931507\n",
      "Epoch: 068, Train Loss: 0.1447, Test Loss: 0.2974, Train Acc: 0.8896, Test Acc: 0.8268, Test f1: 0.8608, AUC: class-0>>0.9763245723821443|class-1>>0.9463013698630136\n",
      "Epoch: 069, Train Loss: 0.1364, Test Loss: 0.2827, Train Acc: 0.8894, Test Acc: 0.8183, Test f1: 0.8539, AUC: class-0>>0.9748644138506466|class-1>>0.9476712328767124\n",
      "Epoch: 070, Train Loss: 0.4577, Test Loss: 0.5941, Train Acc: 0.8967, Test Acc: 0.7965, Test f1: 0.8378, AUC: class-0>>0.975177304964539|class-1>>0.9384931506849314\n",
      "Epoch: 071, Train Loss: 0.1511, Test Loss: 0.3138, Train Acc: 0.8937, Test Acc: 0.8235, Test f1: 0.8562, AUC: class-0>>0.9760116812682521|class-1>>0.9491780821917808\n",
      "Epoch: 072, Train Loss: 0.1549, Test Loss: 0.3054, Train Acc: 0.8698, Test Acc: 0.7994, Test f1: 0.8413, AUC: class-0>>0.9769503546099291|class-1>>0.9519178082191782\n",
      "Epoch: 073, Train Loss: 0.1324, Test Loss: 0.2842, Train Acc: 0.8800, Test Acc: 0.8003, Test f1: 0.8430, AUC: class-0>>0.9764288694201084|class-1>>0.9512328767123288\n",
      "Epoch: 074, Train Loss: 0.1276, Test Loss: 0.2810, Train Acc: 0.9029, Test Acc: 0.8172, Test f1: 0.8575, AUC: class-0>>0.9759073842302879|class-1>>0.9490410958904109\n",
      "Epoch: 075, Train Loss: 0.1235, Test Loss: 0.3004, Train Acc: 0.8998, Test Acc: 0.8098, Test f1: 0.8476, AUC: class-0>>0.976846057571965|class-1>>0.9495890410958905\n",
      "Epoch: 076, Train Loss: 0.1196, Test Loss: 0.2951, Train Acc: 0.8731, Test Acc: 0.8022, Test f1: 0.8412, AUC: class-0>>0.9753858990404674|class-1>>0.9486301369863013\n",
      "Epoch: 077, Train Loss: 0.1269, Test Loss: 0.2970, Train Acc: 0.8999, Test Acc: 0.8224, Test f1: 0.8594, AUC: class-0>>0.9758030871923238|class-1>>0.9434246575342465\n",
      "Epoch: 078, Train Loss: 0.1273, Test Loss: 0.3050, Train Acc: 0.8894, Test Acc: 0.8055, Test f1: 0.8458, AUC: class-0>>0.9766374634960368|class-1>>0.9467123287671233\n",
      "Epoch: 079, Train Loss: 0.3990, Test Loss: 0.5186, Train Acc: 0.8886, Test Acc: 0.8026, Test f1: 0.8460, AUC: class-0>>0.9764288694201084|class-1>>0.953013698630137\n",
      "Epoch: 080, Train Loss: 0.1375, Test Loss: 0.3103, Train Acc: 0.9013, Test Acc: 0.7894, Test f1: 0.8321, AUC: class-0>>0.9754901960784313|class-1>>0.946986301369863\n",
      "Epoch: 081, Train Loss: 0.1313, Test Loss: 0.3265, Train Acc: 0.9098, Test Acc: 0.8098, Test f1: 0.8476, AUC: class-0>>0.9769503546099291|class-1>>0.948904109589041\n",
      "Epoch: 082, Train Loss: 0.1198, Test Loss: 0.3063, Train Acc: 0.9080, Test Acc: 0.8022, Test f1: 0.8418, AUC: class-0>>0.9764288694201084|class-1>>0.9452054794520548\n",
      "Epoch: 083, Train Loss: 0.1224, Test Loss: 0.3339, Train Acc: 0.9080, Test Acc: 0.8121, Test f1: 0.8504, AUC: class-0>>0.9761159783062161|class-1>>0.9478082191780822\n",
      "Epoch: 084, Train Loss: 0.1099, Test Loss: 0.3236, Train Acc: 0.9011, Test Acc: 0.8078, Test f1: 0.8491, AUC: class-0>>0.9767417605340009|class-1>>0.9494520547945206\n",
      "Epoch: 085, Train Loss: 0.1182, Test Loss: 0.3107, Train Acc: 0.9126, Test Acc: 0.8089, Test f1: 0.8463, AUC: class-0>>0.9752816020025031|class-1>>0.948904109589041\n",
      "Epoch: 086, Train Loss: 0.1117, Test Loss: 0.3201, Train Acc: 0.8998, Test Acc: 0.7918, Test f1: 0.8350, AUC: class-0>>0.9775761368377138|class-1>>0.9468493150684931\n",
      "Epoch: 087, Train Loss: 0.1059, Test Loss: 0.3132, Train Acc: 0.9108, Test Acc: 0.8140, Test f1: 0.8528, AUC: class-0>>0.9767417605340006|class-1>>0.9501369863013699\n",
      "Epoch: 088, Train Loss: 0.1177, Test Loss: 0.3413, Train Acc: 0.9013, Test Acc: 0.8121, Test f1: 0.8510, AUC: class-0>>0.9770546516478932|class-1>>0.9484931506849315\n",
      "Epoch: 089, Train Loss: 0.1419, Test Loss: 0.3417, Train Acc: 0.9138, Test Acc: 0.8172, Test f1: 0.8575, AUC: class-0>>0.9765331664580726|class-1>>0.948082191780822\n",
      "Epoch: 090, Train Loss: 0.1063, Test Loss: 0.3266, Train Acc: 0.9221, Test Acc: 0.8121, Test f1: 0.8504, AUC: class-0>>0.9758030871923238|class-1>>0.9484931506849316\n",
      "Epoch: 091, Train Loss: 0.1323, Test Loss: 0.3778, Train Acc: 0.8999, Test Acc: 0.8197, Test f1: 0.8569, AUC: class-0>>0.9770546516478932|class-1>>0.9478082191780821\n",
      "Epoch: 092, Train Loss: 0.1078, Test Loss: 0.3483, Train Acc: 0.9195, Test Acc: 0.8154, Test f1: 0.8552, AUC: class-0>>0.9767417605340009|class-1>>0.9499999999999998\n",
      "Epoch: 093, Train Loss: 0.0993, Test Loss: 0.3064, Train Acc: 0.9102, Test Acc: 0.8121, Test f1: 0.8504, AUC: class-0>>0.9759073842302879|class-1>>0.9483561643835616\n",
      "Epoch: 094, Train Loss: 0.0980, Test Loss: 0.3182, Train Acc: 0.9045, Test Acc: 0.8046, Test f1: 0.8444, AUC: class-0>>0.9766374634960368|class-1>>0.9493150684931507\n",
      "Epoch: 095, Train Loss: 0.1175, Test Loss: 0.3296, Train Acc: 0.9183, Test Acc: 0.8121, Test f1: 0.8504, AUC: class-0>>0.9753858990404674|class-1>>0.9478082191780821\n",
      "Epoch: 096, Train Loss: 0.0952, Test Loss: 0.3322, Train Acc: 0.9242, Test Acc: 0.8121, Test f1: 0.8504, AUC: class-0>>0.9759073842302878|class-1>>0.9480821917808219\n",
      "Epoch: 097, Train Loss: 0.1154, Test Loss: 0.3544, Train Acc: 0.9245, Test Acc: 0.8154, Test f1: 0.8552, AUC: class-0>>0.9759073842302879|class-1>>0.9509589041095889\n",
      "Epoch: 098, Train Loss: 0.0979, Test Loss: 0.3376, Train Acc: 0.9307, Test Acc: 0.8121, Test f1: 0.8504, AUC: class-0>>0.9758030871923238|class-1>>0.9487671232876712\n",
      "Epoch: 099, Train Loss: 0.0952, Test Loss: 0.3330, Train Acc: 0.9138, Test Acc: 0.8046, Test f1: 0.8444, AUC: class-0>>0.9756987901543596|class-1>>0.9493150684931507\n",
      "Epoch: 100, Train Loss: 0.1061, Test Loss: 0.3442, Train Acc: 0.9299, Test Acc: 0.8078, Test f1: 0.8485, AUC: class-0>>0.9749687108886107|class-1>>0.9493150684931506\n",
      "Epoch: 101, Train Loss: 0.0955, Test Loss: 0.3730, Train Acc: 0.9183, Test Acc: 0.8121, Test f1: 0.8504, AUC: class-0>>0.9762202753441802|class-1>>0.9479452054794522\n",
      "Epoch: 102, Train Loss: 0.0901, Test Loss: 0.3800, Train Acc: 0.9009, Test Acc: 0.8121, Test f1: 0.8504, AUC: class-0>>0.9763245723821443|class-1>>0.9482191780821917\n",
      "Epoch: 103, Train Loss: 0.0921, Test Loss: 0.3620, Train Acc: 0.9311, Test Acc: 0.8089, Test f1: 0.8458, AUC: class-0>>0.9758030871923237|class-1>>0.9501369863013698\n",
      "Epoch: 104, Train Loss: 0.0949, Test Loss: 0.3788, Train Acc: 0.9264, Test Acc: 0.8089, Test f1: 0.8458, AUC: class-0>>0.9762202753441802|class-1>>0.9479452054794522\n",
      "Epoch: 105, Train Loss: 0.0937, Test Loss: 0.3589, Train Acc: 0.9183, Test Acc: 0.8078, Test f1: 0.8485, AUC: class-0>>0.9762202753441802|class-1>>0.9484931506849315\n",
      "Epoch: 106, Train Loss: 0.0915, Test Loss: 0.3522, Train Acc: 0.9355, Test Acc: 0.8056, Test f1: 0.8411, AUC: class-0>>0.9746558197747184|class-1>>0.947123287671233\n",
      "Epoch: 107, Train Loss: 0.0901, Test Loss: 0.3585, Train Acc: 0.9252, Test Acc: 0.8121, Test f1: 0.8504, AUC: class-0>>0.975281602002503|class-1>>0.948904109589041\n",
      "Epoch: 108, Train Loss: 0.0900, Test Loss: 0.3602, Train Acc: 0.9317, Test Acc: 0.8089, Test f1: 0.8458, AUC: class-0>>0.9758552357113058|class-1>>0.948904109589041\n",
      "Epoch: 109, Train Loss: 0.0812, Test Loss: 0.3744, Train Acc: 0.9273, Test Acc: 0.8089, Test f1: 0.8458, AUC: class-0>>0.9754901960784313|class-1>>0.949041095890411\n",
      "Epoch: 110, Train Loss: 0.0927, Test Loss: 0.3745, Train Acc: 0.9321, Test Acc: 0.8003, Test f1: 0.8417, AUC: class-0>>0.9753858990404672|class-1>>0.946027397260274\n",
      "Epoch: 111, Train Loss: 0.0806, Test Loss: 0.3771, Train Acc: 0.9305, Test Acc: 0.8003, Test f1: 0.8418, AUC: class-0>>0.9759073842302878|class-1>>0.9475342465753425\n",
      "Epoch: 112, Train Loss: 0.0846, Test Loss: 0.3708, Train Acc: 0.9271, Test Acc: 0.8046, Test f1: 0.8439, AUC: class-0>>0.975177304964539|class-1>>0.9468493150684931\n",
      "Epoch: 113, Train Loss: 0.0808, Test Loss: 0.3825, Train Acc: 0.9295, Test Acc: 0.8089, Test f1: 0.8458, AUC: class-0>>0.9756466416353775|class-1>>0.9493150684931506\n",
      "Epoch: 114, Train Loss: 0.0856, Test Loss: 0.4014, Train Acc: 0.9226, Test Acc: 0.8218, Test f1: 0.8507, AUC: class-0>>0.9766896120150187|class-1>>0.946986301369863\n",
      "Epoch: 115, Train Loss: 0.0752, Test Loss: 0.3858, Train Acc: 0.9269, Test Acc: 0.8121, Test f1: 0.8504, AUC: class-0>>0.9754901960784315|class-1>>0.9480821917808219\n",
      "Epoch: 116, Train Loss: 0.0855, Test Loss: 0.4080, Train Acc: 0.9271, Test Acc: 0.8089, Test f1: 0.8458, AUC: class-0>>0.9757509386733416|class-1>>0.9482191780821917\n",
      "Epoch: 117, Train Loss: 0.0831, Test Loss: 0.4018, Train Acc: 0.9362, Test Acc: 0.8089, Test f1: 0.8458, AUC: class-0>>0.9751251564455569|class-1>>0.948904109589041\n",
      "Epoch: 118, Train Loss: 0.0766, Test Loss: 0.4088, Train Acc: 0.9269, Test Acc: 0.8046, Test f1: 0.8439, AUC: class-0>>0.9755944931163953|class-1>>0.9473972602739726\n",
      "Epoch: 119, Train Loss: 0.0725, Test Loss: 0.3941, Train Acc: 0.9360, Test Acc: 0.8089, Test f1: 0.8458, AUC: class-0>>0.9755944931163955|class-1>>0.9478082191780822\n",
      "Epoch: 120, Train Loss: 0.0720, Test Loss: 0.3946, Train Acc: 0.9314, Test Acc: 0.8003, Test f1: 0.8418, AUC: class-0>>0.9756466416353775|class-1>>0.9484931506849315\n",
      "Epoch: 121, Train Loss: 0.0863, Test Loss: 0.4231, Train Acc: 0.9384, Test Acc: 0.8055, Test f1: 0.8458, AUC: class-0>>0.9764288694201084|class-1>>0.9463013698630138\n",
      "Epoch: 122, Train Loss: 0.0787, Test Loss: 0.4111, Train Acc: 0.9314, Test Acc: 0.8078, Test f1: 0.8485, AUC: class-0>>0.9752816020025031|class-1>>0.9489041095890409\n",
      "Epoch: 123, Train Loss: 0.0694, Test Loss: 0.4123, Train Acc: 0.9314, Test Acc: 0.8056, Test f1: 0.8411, AUC: class-0>>0.9748644138506467|class-1>>0.9482191780821917\n",
      "Epoch: 124, Train Loss: 0.0750, Test Loss: 0.4104, Train Acc: 0.9336, Test Acc: 0.7960, Test f1: 0.8395, AUC: class-0>>0.9746036712557362|class-1>>0.9483561643835616\n",
      "Epoch: 125, Train Loss: 0.0695, Test Loss: 0.4171, Train Acc: 0.9338, Test Acc: 0.7960, Test f1: 0.8395, AUC: class-0>>0.9751251564455569|class-1>>0.9472602739726026\n",
      "Epoch: 126, Train Loss: 0.0684, Test Loss: 0.4216, Train Acc: 0.9348, Test Acc: 0.8089, Test f1: 0.8458, AUC: class-0>>0.9759595327492698|class-1>>0.9480821917808218\n",
      "Epoch: 127, Train Loss: 0.0672, Test Loss: 0.4280, Train Acc: 0.9383, Test Acc: 0.7960, Test f1: 0.8395, AUC: class-0>>0.9754380475594492|class-1>>0.948082191780822\n",
      "Epoch: 128, Train Loss: 0.0682, Test Loss: 0.4369, Train Acc: 0.9429, Test Acc: 0.7960, Test f1: 0.8395, AUC: class-0>>0.9748122653316645|class-1>>0.9473972602739726\n",
      "Epoch: 129, Train Loss: 0.0688, Test Loss: 0.4493, Train Acc: 0.9453, Test Acc: 0.8046, Test f1: 0.8439, AUC: class-0>>0.975229453483521|class-1>>0.9472602739726027\n",
      "Epoch: 130, Train Loss: 0.0659, Test Loss: 0.4448, Train Acc: 0.9451, Test Acc: 0.7960, Test f1: 0.8395, AUC: class-0>>0.9753337505214851|class-1>>0.9475342465753425\n",
      "Epoch: 131, Train Loss: 0.0678, Test Loss: 0.4488, Train Acc: 0.9450, Test Acc: 0.7960, Test f1: 0.8395, AUC: class-0>>0.9747601168126825|class-1>>0.9469863013698631\n",
      "Epoch: 132, Train Loss: 0.0644, Test Loss: 0.4497, Train Acc: 0.9451, Test Acc: 0.7960, Test f1: 0.8395, AUC: class-0>>0.9747601168126825|class-1>>0.946986301369863\n",
      "Epoch: 133, Train Loss: 0.0654, Test Loss: 0.4491, Train Acc: 0.9427, Test Acc: 0.7960, Test f1: 0.8395, AUC: class-0>>0.9744993742177721|class-1>>0.9471232876712329\n",
      "Epoch: 134, Train Loss: 0.0634, Test Loss: 0.4507, Train Acc: 0.9474, Test Acc: 0.7960, Test f1: 0.8395, AUC: class-0>>0.9744993742177722|class-1>>0.9475342465753425\n",
      "Epoch: 135, Train Loss: 0.0638, Test Loss: 0.4589, Train Acc: 0.9427, Test Acc: 0.7960, Test f1: 0.8395, AUC: class-0>>0.974342928660826|class-1>>0.9475342465753425\n",
      "Epoch: 136, Train Loss: 0.0633, Test Loss: 0.4499, Train Acc: 0.9403, Test Acc: 0.7960, Test f1: 0.8395, AUC: class-0>>0.9745515227367543|class-1>>0.9472602739726026\n",
      "Epoch: 137, Train Loss: 0.0624, Test Loss: 0.4609, Train Acc: 0.9450, Test Acc: 0.7960, Test f1: 0.8395, AUC: class-0>>0.9746036712557363|class-1>>0.9473972602739726\n",
      "Epoch: 138, Train Loss: 0.0628, Test Loss: 0.4649, Train Acc: 0.9427, Test Acc: 0.7960, Test f1: 0.8395, AUC: class-0>>0.9747079682937004|class-1>>0.9471232876712328\n",
      "Epoch: 139, Train Loss: 0.0624, Test Loss: 0.4658, Train Acc: 0.9450, Test Acc: 0.7960, Test f1: 0.8395, AUC: class-0>>0.9739778890279516|class-1>>0.9475342465753424\n",
      "Epoch: 140, Train Loss: 0.0612, Test Loss: 0.4707, Train Acc: 0.9472, Test Acc: 0.7960, Test f1: 0.8395, AUC: class-0>>0.9745515227367543|class-1>>0.9472602739726028\n",
      "Epoch: 141, Train Loss: 0.0605, Test Loss: 0.4686, Train Acc: 0.9450, Test Acc: 0.7960, Test f1: 0.8395, AUC: class-0>>0.9742386316228618|class-1>>0.9475342465753425\n",
      "Epoch: 142, Train Loss: 0.0609, Test Loss: 0.4680, Train Acc: 0.9381, Test Acc: 0.7960, Test f1: 0.8395, AUC: class-0>>0.9749687108886107|class-1>>0.9483561643835616\n",
      "Epoch: 143, Train Loss: 0.0612, Test Loss: 0.4810, Train Acc: 0.9450, Test Acc: 0.7960, Test f1: 0.8395, AUC: class-0>>0.9744472256987902|class-1>>0.9472602739726026\n",
      "Epoch: 144, Train Loss: 0.0611, Test Loss: 0.4824, Train Acc: 0.9403, Test Acc: 0.7960, Test f1: 0.8395, AUC: class-0>>0.974395077179808|class-1>>0.9475342465753425\n",
      "Epoch: 145, Train Loss: 0.0635, Test Loss: 0.4762, Train Acc: 0.9472, Test Acc: 0.7960, Test f1: 0.8395, AUC: class-0>>0.9744993742177721|class-1>>0.9472602739726027\n",
      "Epoch: 146, Train Loss: 0.0609, Test Loss: 0.4787, Train Acc: 0.9472, Test Acc: 0.7960, Test f1: 0.8395, AUC: class-0>>0.9744993742177722|class-1>>0.9473972602739725\n",
      "Epoch: 147, Train Loss: 0.0596, Test Loss: 0.4860, Train Acc: 0.9450, Test Acc: 0.7960, Test f1: 0.8395, AUC: class-0>>0.9742907801418439|class-1>>0.9476712328767123\n",
      "Epoch: 148, Train Loss: 0.0593, Test Loss: 0.4853, Train Acc: 0.9448, Test Acc: 0.7960, Test f1: 0.8395, AUC: class-0>>0.9746036712557362|class-1>>0.9473972602739726\n",
      "Epoch: 149, Train Loss: 0.0603, Test Loss: 0.4781, Train Acc: 0.9448, Test Acc: 0.7960, Test f1: 0.8395, AUC: class-0>>0.9731435127242386|class-1>>0.9483561643835616\n",
      "Running cross-validation fold: 02\n",
      "Epoch: 001, Train Loss: 0.4015, Test Loss: 0.4057, Train Acc: 0.7312, Test Acc: 0.8003, Test f1: 0.8216, AUC: class-0>>0.9412255976615512|class-1>>0.947660696931834\n",
      "Saved model!\n",
      "Best thresholds ===>>> class-0>>0.8756857514381409|class-1>>0.7124780416488647\n",
      "Epoch: 002, Train Loss: 0.3389, Test Loss: 0.3522, Train Acc: 0.7485, Test Acc: 0.7976, Test f1: 0.8074, AUC: class-0>>0.9494728050944774|class-1>>0.9479383590170762\n",
      "Epoch: 003, Train Loss: 0.2881, Test Loss: 0.2973, Train Acc: 0.7863, Test Acc: 0.8217, Test f1: 0.8366, AUC: class-0>>0.9583463827121829|class-1>>0.956129390531723\n",
      "Saved model!\n",
      "Best thresholds ===>>> class-0>>0.7567451000213623|class-1>>0.6848294138908386\n",
      "Epoch: 004, Train Loss: 0.2670, Test Loss: 0.2831, Train Acc: 0.7746, Test Acc: 0.8119, Test f1: 0.8258, AUC: class-0>>0.9592859379893517|class-1>>0.9552964042759962\n",
      "Epoch: 005, Train Loss: 0.3977, Test Loss: 0.4376, Train Acc: 0.7764, Test Acc: 0.8181, Test f1: 0.8310, AUC: class-0>>0.9436266833698715|class-1>>0.9550187421907539\n",
      "Epoch: 006, Train Loss: 0.4398, Test Loss: 0.4880, Train Acc: 0.7576, Test Acc: 0.8401, Test f1: 0.8473, AUC: class-0>>0.9544837665727112|class-1>>0.9580730251284186\n",
      "Saved model!\n",
      "Best thresholds ===>>> class-0>>0.27772438526153564|class-1>>0.08201296627521515\n",
      "Epoch: 007, Train Loss: 0.2586, Test Loss: 0.2796, Train Acc: 0.8060, Test Acc: 0.8125, Test f1: 0.8254, AUC: class-0>>0.9639837143751957|class-1>>0.9561293905317229\n",
      "Epoch: 008, Train Loss: 0.2396, Test Loss: 0.2651, Train Acc: 0.7826, Test Acc: 0.8360, Test f1: 0.8498, AUC: class-0>>0.9637749243136027|class-1>>0.9572400388726919\n",
      "Saved model!\n",
      "Best thresholds ===>>> class-0>>0.5619968175888062|class-1>>0.438921183347702\n",
      "Epoch: 009, Train Loss: 0.2435, Test Loss: 0.2755, Train Acc: 0.7628, Test Acc: 0.8405, Test f1: 0.8582, AUC: class-0>>0.9645056895291784|class-1>>0.9564070526169651\n",
      "Saved model!\n",
      "Best thresholds ===>>> class-0>>0.5241541266441345|class-1>>0.33409056067466736\n",
      "Epoch: 010, Train Loss: 0.2398, Test Loss: 0.2625, Train Acc: 0.8145, Test Acc: 0.8240, Test f1: 0.8431, AUC: class-0>>0.966176010021923|class-1>>0.9493266694432875\n",
      "Epoch: 011, Train Loss: 0.2646, Test Loss: 0.2987, Train Acc: 0.8038, Test Acc: 0.8411, Test f1: 0.8519, AUC: class-0>>0.965027664683161|class-1>>0.9555740663612384\n",
      "Epoch: 012, Train Loss: 0.2694, Test Loss: 0.2858, Train Acc: 0.8053, Test Acc: 0.8129, Test f1: 0.8302, AUC: class-0>>0.9692034659150224|class-1>>0.9501596556990143\n",
      "Epoch: 013, Train Loss: 0.2715, Test Loss: 0.3015, Train Acc: 0.7916, Test Acc: 0.7960, Test f1: 0.8104, AUC: class-0>>0.969307860945819|class-1>>0.9480771900596974\n",
      "Epoch: 014, Train Loss: 0.2328, Test Loss: 0.2630, Train Acc: 0.8093, Test Acc: 0.8302, Test f1: 0.8490, AUC: class-0>>0.97108257646936|class-1>>0.9477995279744551\n",
      "Epoch: 015, Train Loss: 0.2356, Test Loss: 0.2695, Train Acc: 0.8118, Test Acc: 0.8184, Test f1: 0.8362, AUC: class-0>>0.9694122559766155|class-1>>0.9489101763154242\n",
      "Epoch: 016, Train Loss: 0.2567, Test Loss: 0.2932, Train Acc: 0.7854, Test Acc: 0.7850, Test f1: 0.8059, AUC: class-0>>0.966176010021923|class-1>>0.9383590170762182\n",
      "Epoch: 017, Train Loss: 0.2315, Test Loss: 0.2823, Train Acc: 0.8118, Test Acc: 0.8598, Test f1: 0.8768, AUC: class-0>>0.9692034659150225|class-1>>0.9519644592530889\n",
      "Saved model!\n",
      "Best thresholds ===>>> class-0>>0.30185115337371826|class-1>>0.4647327959537506\n",
      "Epoch: 018, Train Loss: 0.2007, Test Loss: 0.2436, Train Acc: 0.8174, Test Acc: 0.8350, Test f1: 0.8469, AUC: class-0>>0.9703518112537843|class-1>>0.9514091350826044\n",
      "Epoch: 019, Train Loss: 0.2126, Test Loss: 0.2563, Train Acc: 0.8358, Test Acc: 0.8151, Test f1: 0.8275, AUC: class-0>>0.969516651007412|class-1>>0.9539080938497848\n",
      "Epoch: 020, Train Loss: 0.2144, Test Loss: 0.2649, Train Acc: 0.7977, Test Acc: 0.8171, Test f1: 0.8299, AUC: class-0>>0.9699342311305982|class-1>>0.9529362765514369\n",
      "Epoch: 021, Train Loss: 0.2441, Test Loss: 0.2819, Train Acc: 0.8102, Test Acc: 0.8226, Test f1: 0.8362, AUC: class-0>>0.9735880572084769|class-1>>0.9472442038039705\n",
      "Epoch: 022, Train Loss: 0.2025, Test Loss: 0.2743, Train Acc: 0.8386, Test Acc: 0.8226, Test f1: 0.8397, AUC: class-0>>0.9715001565925462|class-1>>0.9521032902957101\n",
      "Epoch: 023, Train Loss: 0.2134, Test Loss: 0.2630, Train Acc: 0.8269, Test Acc: 0.8288, Test f1: 0.8416, AUC: class-0>>0.9744232174548492|class-1>>0.9522421213383312\n",
      "Epoch: 024, Train Loss: 0.2244, Test Loss: 0.2744, Train Acc: 0.7966, Test Acc: 0.8379, Test f1: 0.8519, AUC: class-0>>0.9727528969621047|class-1>>0.9489101763154242\n",
      "Epoch: 025, Train Loss: 0.2060, Test Loss: 0.2556, Train Acc: 0.8327, Test Acc: 0.7966, Test f1: 0.8097, AUC: class-0>>0.9731704770852907|class-1>>0.9529362765514369\n",
      "Epoch: 026, Train Loss: 0.2086, Test Loss: 0.2510, Train Acc: 0.8358, Test Acc: 0.8392, Test f1: 0.8505, AUC: class-0>>0.9741100323624595|class-1>>0.9523809523809522\n",
      "Epoch: 027, Train Loss: 0.1913, Test Loss: 0.2466, Train Acc: 0.8253, Test Acc: 0.8314, Test f1: 0.8470, AUC: class-0>>0.9732748721160873|class-1>>0.9534916007219213\n",
      "Epoch: 028, Train Loss: 0.2327, Test Loss: 0.2878, Train Acc: 0.8401, Test Acc: 0.8513, Test f1: 0.8663, AUC: class-0>>0.9748407975780353|class-1>>0.9550187421907539\n",
      "Epoch: 029, Train Loss: 0.2024, Test Loss: 0.2610, Train Acc: 0.8315, Test Acc: 0.8129, Test f1: 0.8297, AUC: class-0>>0.9718133416849357|class-1>>0.9529362765514368\n",
      "Epoch: 030, Train Loss: 0.1973, Test Loss: 0.2553, Train Acc: 0.8378, Test Acc: 0.8379, Test f1: 0.8518, AUC: class-0>>0.9731704770852907|class-1>>0.9507149798694988\n",
      "Epoch: 031, Train Loss: 0.1981, Test Loss: 0.2456, Train Acc: 0.8229, Test Acc: 0.8327, Test f1: 0.8501, AUC: class-0>>0.97379684727007|class-1>>0.9487713452728029\n",
      "Epoch: 032, Train Loss: 0.1982, Test Loss: 0.2491, Train Acc: 0.8318, Test Acc: 0.8385, Test f1: 0.8516, AUC: class-0>>0.97379684727007|class-1>>0.9526586144661946\n",
      "Epoch: 033, Train Loss: 0.1886, Test Loss: 0.2318, Train Acc: 0.8424, Test Acc: 0.8447, Test f1: 0.8570, AUC: class-0>>0.9758847478860007|class-1>>0.9541857559350271\n",
      "Epoch: 034, Train Loss: 0.1990, Test Loss: 0.2432, Train Acc: 0.8201, Test Acc: 0.8155, Test f1: 0.8313, AUC: class-0>>0.9748407975780352|class-1>>0.9515479661252255\n",
      "Epoch: 035, Train Loss: 0.1747, Test Loss: 0.2373, Train Acc: 0.8337, Test Acc: 0.8298, Test f1: 0.8447, AUC: class-0>>0.9750495876396283|class-1>>0.9521032902957102\n",
      "Epoch: 036, Train Loss: 0.2541, Test Loss: 0.3242, Train Acc: 0.8331, Test Acc: 0.7902, Test f1: 0.8100, AUC: class-0>>0.9718133416849358|class-1>>0.9315562959877828\n",
      "Epoch: 037, Train Loss: 0.2736, Test Loss: 0.3502, Train Acc: 0.8338, Test Acc: 0.7992, Test f1: 0.8114, AUC: class-0>>0.9707693913769704|class-1>>0.9534916007219214\n",
      "Epoch: 038, Train Loss: 0.1929, Test Loss: 0.2595, Train Acc: 0.8467, Test Acc: 0.8307, Test f1: 0.8429, AUC: class-0>>0.975362772732018|class-1>>0.9526586144661946\n",
      "Epoch: 039, Train Loss: 0.2303, Test Loss: 0.2961, Train Acc: 0.8506, Test Acc: 0.8298, Test f1: 0.8444, AUC: class-0>>0.9731704770852907|class-1>>0.9518256282104679\n",
      "Epoch: 040, Train Loss: 0.2081, Test Loss: 0.2652, Train Acc: 0.8569, Test Acc: 0.8226, Test f1: 0.8362, AUC: class-0>>0.9746320075164422|class-1>>0.9494655004859086\n",
      "Epoch: 041, Train Loss: 0.1830, Test Loss: 0.2489, Train Acc: 0.8366, Test Acc: 0.8207, Test f1: 0.8341, AUC: class-0>>0.9737968472700699|class-1>>0.9512703040399832\n",
      "Epoch: 042, Train Loss: 0.3973, Test Loss: 0.4166, Train Acc: 0.8591, Test Acc: 0.8526, Test f1: 0.8675, AUC: class-0>>0.9744232174548492|class-1>>0.9433569346105789\n",
      "Epoch: 043, Train Loss: 0.1720, Test Loss: 0.2491, Train Acc: 0.8576, Test Acc: 0.8366, Test f1: 0.8495, AUC: class-0>>0.9727528969621047|class-1>>0.9525197834235736\n",
      "Epoch: 044, Train Loss: 0.2007, Test Loss: 0.2519, Train Acc: 0.8708, Test Acc: 0.8360, Test f1: 0.8506, AUC: class-0>>0.9764067230399832|class-1>>0.9484936831875607\n",
      "Epoch: 045, Train Loss: 0.1819, Test Loss: 0.2758, Train Acc: 0.8503, Test Acc: 0.8334, Test f1: 0.8491, AUC: class-0>>0.974214427393256|class-1>>0.9502984867416354\n",
      "Epoch: 046, Train Loss: 0.1816, Test Loss: 0.2669, Train Acc: 0.8547, Test Acc: 0.8395, Test f1: 0.8550, AUC: class-0>>0.9746320075164422|class-1>>0.9522421213383312\n",
      "Epoch: 047, Train Loss: 0.2099, Test Loss: 0.2863, Train Acc: 0.8340, Test Acc: 0.8139, Test f1: 0.8290, AUC: class-0>>0.9745276124856457|class-1>>0.9488407607941135\n",
      "Epoch: 048, Train Loss: 0.1571, Test Loss: 0.2365, Train Acc: 0.8536, Test Acc: 0.8288, Test f1: 0.8416, AUC: class-0>>0.9759891429167972|class-1>>0.9507149798694988\n",
      "Epoch: 049, Train Loss: 0.1735, Test Loss: 0.2660, Train Acc: 0.8567, Test Acc: 0.8519, Test f1: 0.8655, AUC: class-0>>0.9709781814385635|class-1>>0.9443287519089268\n",
      "Epoch: 050, Train Loss: 0.2202, Test Loss: 0.3258, Train Acc: 0.8666, Test Acc: 0.8278, Test f1: 0.8428, AUC: class-0>>0.9736924522392734|class-1>>0.9533527696793002\n",
      "Epoch: 051, Train Loss: 0.2232, Test Loss: 0.2904, Train Acc: 0.8279, Test Acc: 0.8236, Test f1: 0.8392, AUC: class-0>>0.9759891429167971|class-1>>0.9455782312925168\n",
      "Epoch: 052, Train Loss: 0.1912, Test Loss: 0.2937, Train Acc: 0.8333, Test Acc: 0.8571, Test f1: 0.8672, AUC: class-0>>0.9732748721160873|class-1>>0.9536304317645425\n",
      "Epoch: 053, Train Loss: 0.1639, Test Loss: 0.2388, Train Acc: 0.8699, Test Acc: 0.8429, Test f1: 0.8639, AUC: class-0>>0.9754671677628145|class-1>>0.9480771900596973\n",
      "Epoch: 054, Train Loss: 0.1561, Test Loss: 0.2493, Train Acc: 0.8701, Test Acc: 0.8366, Test f1: 0.8495, AUC: class-0>>0.9743188224240525|class-1>>0.95085381091212\n",
      "Epoch: 055, Train Loss: 0.1604, Test Loss: 0.2494, Train Acc: 0.8783, Test Acc: 0.8256, Test f1: 0.8454, AUC: class-0>>0.9764067230399833|class-1>>0.9482160211023184\n",
      "Epoch: 056, Train Loss: 0.3188, Test Loss: 0.4593, Train Acc: 0.8346, Test Acc: 0.7882, Test f1: 0.8045, AUC: class-0>>0.972439711869715|class-1>>0.9496043315285297\n",
      "Epoch: 057, Train Loss: 0.1485, Test Loss: 0.2577, Train Acc: 0.8723, Test Acc: 0.8477, Test f1: 0.8612, AUC: class-0>>0.975571562793611|class-1>>0.9509926419547411\n",
      "Epoch: 058, Train Loss: 0.1567, Test Loss: 0.2406, Train Acc: 0.8615, Test Acc: 0.8083, Test f1: 0.8235, AUC: class-0>>0.9761979329783902|class-1>>0.9522421213383313\n",
      "Epoch: 059, Train Loss: 0.2340, Test Loss: 0.3333, Train Acc: 0.8521, Test Acc: 0.8175, Test f1: 0.8414, AUC: class-0>>0.9728572919929012|class-1>>0.9453005692072748\n",
      "Epoch: 060, Train Loss: 0.1618, Test Loss: 0.2639, Train Acc: 0.8627, Test Acc: 0.8182, Test f1: 0.8408, AUC: class-0>>0.9748407975780353|class-1>>0.952936276551437\n",
      "Epoch: 061, Train Loss: 0.1503, Test Loss: 0.2711, Train Acc: 0.8838, Test Acc: 0.8395, Test f1: 0.8546, AUC: class-0>>0.975571562793611|class-1>>0.9525197834235736\n",
      "Epoch: 062, Train Loss: 0.1534, Test Loss: 0.2446, Train Acc: 0.8587, Test Acc: 0.8200, Test f1: 0.8345, AUC: class-0>>0.9758847478860007|class-1>>0.9507149798694988\n",
      "Epoch: 063, Train Loss: 0.1432, Test Loss: 0.2344, Train Acc: 0.8674, Test Acc: 0.8360, Test f1: 0.8506, AUC: class-0>>0.9776594634095417|class-1>>0.9512703040399834\n",
      "Epoch: 064, Train Loss: 0.1740, Test Loss: 0.3154, Train Acc: 0.8624, Test Acc: 0.8340, Test f1: 0.8488, AUC: class-0>>0.974214427393256|class-1>>0.954046924892406\n",
      "Epoch: 065, Train Loss: 0.1534, Test Loss: 0.2614, Train Acc: 0.8738, Test Acc: 0.8242, Test f1: 0.8383, AUC: class-0>>0.9759891429167972|class-1>>0.9539080938497848\n",
      "Epoch: 066, Train Loss: 0.1360, Test Loss: 0.2400, Train Acc: 0.8822, Test Acc: 0.7992, Test f1: 0.8121, AUC: class-0>>0.9770330932247625|class-1>>0.9487713452728029\n",
      "Epoch: 067, Train Loss: 0.1504, Test Loss: 0.2483, Train Acc: 0.8807, Test Acc: 0.8226, Test f1: 0.8357, AUC: class-0>>0.9768243031631694|class-1>>0.9482160211023184\n",
      "Epoch: 068, Train Loss: 0.1772, Test Loss: 0.3304, Train Acc: 0.8770, Test Acc: 0.8210, Test f1: 0.8376, AUC: class-0>>0.9767199081323729|class-1>>0.9494655004859086\n",
      "Epoch: 069, Train Loss: 0.1312, Test Loss: 0.2437, Train Acc: 0.8870, Test Acc: 0.8109, Test f1: 0.8238, AUC: class-0>>0.9768243031631694|class-1>>0.9502984867416355\n",
      "Epoch: 070, Train Loss: 0.1519, Test Loss: 0.2607, Train Acc: 0.8722, Test Acc: 0.8194, Test f1: 0.8396, AUC: class-0>>0.975780352855204|class-1>>0.9471053727613494\n",
      "Epoch: 071, Train Loss: 0.1300, Test Loss: 0.2442, Train Acc: 0.8905, Test Acc: 0.8467, Test f1: 0.8580, AUC: class-0>>0.977346278317152|class-1>>0.9484936831875607\n",
      "Epoch: 072, Train Loss: 0.1273, Test Loss: 0.2622, Train Acc: 0.8826, Test Acc: 0.8268, Test f1: 0.8392, AUC: class-0>>0.9763023280091867|class-1>>0.9521032902957102\n",
      "Epoch: 073, Train Loss: 0.1336, Test Loss: 0.2670, Train Acc: 0.8763, Test Acc: 0.8181, Test f1: 0.8323, AUC: class-0>>0.9772418832863555|class-1>>0.9518256282104679\n",
      "Epoch: 074, Train Loss: 0.1262, Test Loss: 0.2602, Train Acc: 0.8872, Test Acc: 0.8304, Test f1: 0.8435, AUC: class-0>>0.9770330932247625|class-1>>0.9518256282104679\n",
      "Epoch: 075, Train Loss: 0.1590, Test Loss: 0.2918, Train Acc: 0.8798, Test Acc: 0.7846, Test f1: 0.8008, AUC: class-0>>0.9764067230399833|class-1>>0.9461335554630014\n",
      "Epoch: 076, Train Loss: 0.1258, Test Loss: 0.3020, Train Acc: 0.8799, Test Acc: 0.8232, Test f1: 0.8349, AUC: class-0>>0.975571562793611|class-1>>0.95085381091212\n",
      "Epoch: 077, Train Loss: 0.1247, Test Loss: 0.2572, Train Acc: 0.8830, Test Acc: 0.8109, Test f1: 0.8244, AUC: class-0>>0.9759891429167972|class-1>>0.9450229071220325\n",
      "Epoch: 078, Train Loss: 0.1454, Test Loss: 0.3224, Train Acc: 0.8988, Test Acc: 0.8158, Test f1: 0.8349, AUC: class-0>>0.976928698193966|class-1>>0.9465500485908648\n",
      "Epoch: 079, Train Loss: 0.1205, Test Loss: 0.2728, Train Acc: 0.8787, Test Acc: 0.8288, Test f1: 0.8421, AUC: class-0>>0.9763023280091867|class-1>>0.9468277106761072\n",
      "Epoch: 080, Train Loss: 0.1347, Test Loss: 0.2868, Train Acc: 0.8922, Test Acc: 0.8002, Test f1: 0.8132, AUC: class-0>>0.9747364025472387|class-1>>0.9509926419547411\n",
      "Epoch: 081, Train Loss: 0.1229, Test Loss: 0.2656, Train Acc: 0.8834, Test Acc: 0.7944, Test f1: 0.8120, AUC: class-0>>0.9765111180707798|class-1>>0.9471053727613494\n",
      "Epoch: 082, Train Loss: 0.1106, Test Loss: 0.2639, Train Acc: 0.8915, Test Acc: 0.8232, Test f1: 0.8354, AUC: class-0>>0.9780770435327278|class-1>>0.9482160211023185\n",
      "Epoch: 083, Train Loss: 0.1107, Test Loss: 0.2776, Train Acc: 0.8871, Test Acc: 0.8089, Test f1: 0.8219, AUC: class-0>>0.976928698193966|class-1>>0.9501596556990143\n",
      "Epoch: 084, Train Loss: 0.1223, Test Loss: 0.3139, Train Acc: 0.8843, Test Acc: 0.8165, Test f1: 0.8342, AUC: class-0>>0.9770330932247625|class-1>>0.9518256282104679\n",
      "Epoch: 085, Train Loss: 0.1030, Test Loss: 0.2701, Train Acc: 0.8891, Test Acc: 0.8177, Test f1: 0.8290, AUC: class-0>>0.9777638584403382|class-1>>0.9518256282104679\n",
      "Epoch: 086, Train Loss: 0.1141, Test Loss: 0.2690, Train Acc: 0.8911, Test Acc: 0.8304, Test f1: 0.8449, AUC: class-0>>0.9781814385635244|class-1>>0.945578231292517\n",
      "Epoch: 087, Train Loss: 0.1140, Test Loss: 0.2867, Train Acc: 0.9128, Test Acc: 0.8288, Test f1: 0.8421, AUC: class-0>>0.9785990186867105|class-1>>0.9480771900596974\n",
      "Epoch: 088, Train Loss: 0.0976, Test Loss: 0.2661, Train Acc: 0.9045, Test Acc: 0.8125, Test f1: 0.8255, AUC: class-0>>0.9772418832863555|class-1>>0.949881993613772\n",
      "Epoch: 089, Train Loss: 0.1043, Test Loss: 0.2886, Train Acc: 0.9153, Test Acc: 0.8093, Test f1: 0.8257, AUC: class-0>>0.976928698193966|class-1>>0.9487713452728029\n",
      "Epoch: 090, Train Loss: 0.0970, Test Loss: 0.3000, Train Acc: 0.9112, Test Acc: 0.8187, Test f1: 0.8317, AUC: class-0>>0.9759891429167972|class-1>>0.9493266694432875\n",
      "Epoch: 091, Train Loss: 0.1151, Test Loss: 0.3341, Train Acc: 0.8995, Test Acc: 0.8171, Test f1: 0.8305, AUC: class-0>>0.9750495876396283|class-1>>0.9466888796334859\n",
      "Epoch: 092, Train Loss: 0.1074, Test Loss: 0.3278, Train Acc: 0.9022, Test Acc: 0.7996, Test f1: 0.8146, AUC: class-0>>0.9739012423008665|class-1>>0.948771345272803\n",
      "Epoch: 093, Train Loss: 0.0954, Test Loss: 0.2792, Train Acc: 0.9138, Test Acc: 0.8275, Test f1: 0.8393, AUC: class-0>>0.9763023280091868|class-1>>0.9511314729973621\n",
      "Epoch: 094, Train Loss: 0.0932, Test Loss: 0.2828, Train Acc: 0.9181, Test Acc: 0.8310, Test f1: 0.8441, AUC: class-0>>0.9752583777012214|class-1>>0.95085381091212\n",
      "Epoch: 095, Train Loss: 0.0959, Test Loss: 0.3099, Train Acc: 0.8975, Test Acc: 0.7950, Test f1: 0.8100, AUC: class-0>>0.9768243031631694|class-1>>0.9516867971678468\n",
      "Epoch: 096, Train Loss: 0.0940, Test Loss: 0.3251, Train Acc: 0.9065, Test Acc: 0.8119, Test f1: 0.8266, AUC: class-0>>0.975362772732018|class-1>>0.9521032902957102\n",
      "Epoch: 097, Train Loss: 0.0859, Test Loss: 0.2934, Train Acc: 0.9240, Test Acc: 0.8125, Test f1: 0.8255, AUC: class-0>>0.9761979329783903|class-1>>0.9500208246563931\n",
      "Epoch: 098, Train Loss: 0.0842, Test Loss: 0.2874, Train Acc: 0.9268, Test Acc: 0.8249, Test f1: 0.8379, AUC: class-0>>0.9756759578244075|class-1>>0.9496043315285297\n",
      "Epoch: 099, Train Loss: 0.1525, Test Loss: 0.3972, Train Acc: 0.9102, Test Acc: 0.7976, Test f1: 0.8119, AUC: class-0>>0.974214427393256|class-1>>0.9517562126891573\n",
      "Epoch: 100, Train Loss: 0.0874, Test Loss: 0.3088, Train Acc: 0.9292, Test Acc: 0.8181, Test f1: 0.8327, AUC: class-0>>0.9763023280091868|class-1>>0.9472442038039706\n",
      "Epoch: 101, Train Loss: 0.1123, Test Loss: 0.3193, Train Acc: 0.9109, Test Acc: 0.8125, Test f1: 0.8275, AUC: class-0>>0.9724919093851132|class-1>>0.9514091350826044\n",
      "Epoch: 102, Train Loss: 0.0805, Test Loss: 0.3079, Train Acc: 0.9340, Test Acc: 0.8057, Test f1: 0.8214, AUC: class-0>>0.9754671677628145|class-1>>0.9475218658892128\n",
      "Epoch: 103, Train Loss: 0.0803, Test Loss: 0.3267, Train Acc: 0.9312, Test Acc: 0.8125, Test f1: 0.8255, AUC: class-0>>0.9760413404321955|class-1>>0.9490490073580452\n",
      "Epoch: 104, Train Loss: 0.0965, Test Loss: 0.3397, Train Acc: 0.9165, Test Acc: 0.8242, Test f1: 0.8388, AUC: class-0>>0.9758325503706023|class-1>>0.9515479661252256\n",
      "Epoch: 105, Train Loss: 0.0770, Test Loss: 0.3062, Train Acc: 0.9439, Test Acc: 0.8187, Test f1: 0.8323, AUC: class-0>>0.9770852907401608|class-1>>0.9494655004859087\n",
      "Epoch: 106, Train Loss: 0.0917, Test Loss: 0.3290, Train Acc: 0.9412, Test Acc: 0.8125, Test f1: 0.8262, AUC: class-0>>0.9741622298778578|class-1>>0.9484936831875607\n",
      "Epoch: 107, Train Loss: 0.0866, Test Loss: 0.3358, Train Acc: 0.9325, Test Acc: 0.8249, Test f1: 0.8385, AUC: class-0>>0.9759891429167972|class-1>>0.9521032902957102\n",
      "Epoch: 108, Train Loss: 0.0787, Test Loss: 0.3121, Train Acc: 0.9336, Test Acc: 0.8187, Test f1: 0.8323, AUC: class-0>>0.9747364025472388|class-1>>0.9479383590170762\n",
      "Epoch: 109, Train Loss: 0.0941, Test Loss: 0.3456, Train Acc: 0.9360, Test Acc: 0.8155, Test f1: 0.8313, AUC: class-0>>0.975780352855204|class-1>>0.952936276551437\n",
      "Epoch: 110, Train Loss: 0.0716, Test Loss: 0.3267, Train Acc: 0.9360, Test Acc: 0.8187, Test f1: 0.8323, AUC: class-0>>0.9760935379475937|class-1>>0.9482160211023185\n",
      "Epoch: 111, Train Loss: 0.0767, Test Loss: 0.3350, Train Acc: 0.9336, Test Acc: 0.8249, Test f1: 0.8385, AUC: class-0>>0.9748929950934335|class-1>>0.9483548521449395\n",
      "Epoch: 112, Train Loss: 0.0805, Test Loss: 0.3569, Train Acc: 0.9384, Test Acc: 0.8125, Test f1: 0.8260, AUC: class-0>>0.9770852907401608|class-1>>0.9504373177842567\n",
      "Epoch: 113, Train Loss: 0.0686, Test Loss: 0.3313, Train Acc: 0.9336, Test Acc: 0.8125, Test f1: 0.8260, AUC: class-0>>0.9760413404321955|class-1>>0.9489101763154241\n",
      "Epoch: 114, Train Loss: 0.0701, Test Loss: 0.3610, Train Acc: 0.9384, Test Acc: 0.8187, Test f1: 0.8323, AUC: class-0>>0.9760935379475937|class-1>>0.9496043315285297\n",
      "Epoch: 115, Train Loss: 0.0678, Test Loss: 0.3437, Train Acc: 0.9360, Test Acc: 0.8125, Test f1: 0.8260, AUC: class-0>>0.9754671677628145|class-1>>0.9497431625711509\n",
      "Epoch: 116, Train Loss: 0.0679, Test Loss: 0.3434, Train Acc: 0.9123, Test Acc: 0.8187, Test f1: 0.8323, AUC: class-0>>0.9735358596930785|class-1>>0.9482160211023185\n",
      "Epoch: 117, Train Loss: 0.0675, Test Loss: 0.3473, Train Acc: 0.9408, Test Acc: 0.8125, Test f1: 0.8260, AUC: class-0>>0.9741100323624595|class-1>>0.9494655004859086\n",
      "Epoch: 118, Train Loss: 0.0668, Test Loss: 0.3735, Train Acc: 0.9384, Test Acc: 0.8089, Test f1: 0.8223, AUC: class-0>>0.9746842050318405|class-1>>0.9484936831875608\n",
      "Epoch: 119, Train Loss: 0.0654, Test Loss: 0.3541, Train Acc: 0.9405, Test Acc: 0.8187, Test f1: 0.8323, AUC: class-0>>0.9720743292619272|class-1>>0.9490490073580452\n",
      "Epoch: 120, Train Loss: 0.0625, Test Loss: 0.3599, Train Acc: 0.9147, Test Acc: 0.8125, Test f1: 0.8262, AUC: class-0>>0.9736402547238752|class-1>>0.9482160211023185\n",
      "Epoch: 121, Train Loss: 0.0637, Test Loss: 0.3567, Train Acc: 0.9175, Test Acc: 0.8002, Test f1: 0.8146, AUC: class-0>>0.9741100323624595|class-1>>0.9475218658892128\n",
      "Epoch: 122, Train Loss: 0.0626, Test Loss: 0.3509, Train Acc: 0.9432, Test Acc: 0.8249, Test f1: 0.8385, AUC: class-0>>0.9729094895082994|class-1>>0.949396084964598\n",
      "Epoch: 123, Train Loss: 0.0609, Test Loss: 0.3529, Train Acc: 0.9147, Test Acc: 0.8187, Test f1: 0.8323, AUC: class-0>>0.9743710199394509|class-1>>0.9487713452728029\n",
      "Epoch: 124, Train Loss: 0.0603, Test Loss: 0.3535, Train Acc: 0.9381, Test Acc: 0.8213, Test f1: 0.8337, AUC: class-0>>0.974579810001044|class-1>>0.9485630987088712\n",
      "Epoch: 125, Train Loss: 0.0693, Test Loss: 0.4074, Train Acc: 0.9484, Test Acc: 0.8183, Test f1: 0.8299, AUC: class-0>>0.9734836621776803|class-1>>0.9501596556990143\n",
      "Epoch: 126, Train Loss: 0.0588, Test Loss: 0.3677, Train Acc: 0.9456, Test Acc: 0.8187, Test f1: 0.8323, AUC: class-0>>0.9729094895082994|class-1>>0.949396084964598\n",
      "Epoch: 127, Train Loss: 0.0586, Test Loss: 0.3728, Train Acc: 0.9195, Test Acc: 0.8089, Test f1: 0.8213, AUC: class-0>>0.9737446497546717|class-1>>0.949396084964598\n",
      "Epoch: 128, Train Loss: 0.0663, Test Loss: 0.3800, Train Acc: 0.9274, Test Acc: 0.7902, Test f1: 0.8083, AUC: class-0>>0.9720743292619272|class-1>>0.9476606969318339\n",
      "Epoch: 129, Train Loss: 0.0594, Test Loss: 0.3984, Train Acc: 0.9171, Test Acc: 0.8089, Test f1: 0.8213, AUC: class-0>>0.9733792671468837|class-1>>0.9492572539219769\n",
      "Epoch: 130, Train Loss: 0.0573, Test Loss: 0.3880, Train Acc: 0.9456, Test Acc: 0.8151, Test f1: 0.8275, AUC: class-0>>0.9730660820544942|class-1>>0.9494655004859086\n",
      "Epoch: 131, Train Loss: 0.0608, Test Loss: 0.3867, Train Acc: 0.9346, Test Acc: 0.8031, Test f1: 0.8192, AUC: class-0>>0.97379684727007|class-1>>0.9483548521449396\n",
      "Epoch: 132, Train Loss: 0.0564, Test Loss: 0.3879, Train Acc: 0.9432, Test Acc: 0.8213, Test f1: 0.8337, AUC: class-0>>0.9740578348470612|class-1>>0.9493266694432875\n",
      "Epoch: 133, Train Loss: 0.0562, Test Loss: 0.3884, Train Acc: 0.9346, Test Acc: 0.7960, Test f1: 0.8110, AUC: class-0>>0.9737446497546717|class-1>>0.9478689434957657\n",
      "Epoch: 134, Train Loss: 0.0551, Test Loss: 0.3876, Train Acc: 0.9453, Test Acc: 0.8275, Test f1: 0.8399, AUC: class-0>>0.9723875143543167|class-1>>0.9490490073580452\n",
      "Epoch: 135, Train Loss: 0.0544, Test Loss: 0.3863, Train Acc: 0.9477, Test Acc: 0.8213, Test f1: 0.8337, AUC: class-0>>0.9722831193235202|class-1>>0.9488407607941135\n",
      "Epoch: 136, Train Loss: 0.0541, Test Loss: 0.3872, Train Acc: 0.9477, Test Acc: 0.8213, Test f1: 0.8346, AUC: class-0>>0.9716567491387409|class-1>>0.9486325142301818\n",
      "Epoch: 137, Train Loss: 0.0539, Test Loss: 0.3927, Train Acc: 0.9477, Test Acc: 0.8213, Test f1: 0.8339, AUC: class-0>>0.972439711869715|class-1>>0.948771345272803\n",
      "Epoch: 138, Train Loss: 0.0535, Test Loss: 0.3905, Train Acc: 0.9477, Test Acc: 0.8213, Test f1: 0.8346, AUC: class-0>>0.9716567491387409|class-1>>0.9483548521449395\n",
      "Epoch: 139, Train Loss: 0.0529, Test Loss: 0.3987, Train Acc: 0.9477, Test Acc: 0.8151, Test f1: 0.8290, AUC: class-0>>0.972074329261927|class-1>>0.9483548521449395\n",
      "Epoch: 140, Train Loss: 0.0532, Test Loss: 0.3999, Train Acc: 0.9422, Test Acc: 0.8089, Test f1: 0.8227, AUC: class-0>>0.9722831193235202|class-1>>0.9476606969318339\n",
      "Epoch: 141, Train Loss: 0.0530, Test Loss: 0.4056, Train Acc: 0.9370, Test Acc: 0.8213, Test f1: 0.8342, AUC: class-0>>0.9727006994467063|class-1>>0.9487713452728029\n",
      "Epoch: 142, Train Loss: 0.0524, Test Loss: 0.4045, Train Acc: 0.9477, Test Acc: 0.8213, Test f1: 0.8346, AUC: class-0>>0.9715523541079445|class-1>>0.947799527974455\n",
      "Epoch: 143, Train Loss: 0.0518, Test Loss: 0.3981, Train Acc: 0.9264, Test Acc: 0.8151, Test f1: 0.8290, AUC: class-0>>0.9720743292619272|class-1>>0.9480771900596973\n",
      "Epoch: 144, Train Loss: 0.0524, Test Loss: 0.3976, Train Acc: 0.9497, Test Acc: 0.8151, Test f1: 0.8290, AUC: class-0>>0.9729094895082994|class-1>>0.9479383590170761\n",
      "Epoch: 145, Train Loss: 0.0513, Test Loss: 0.4038, Train Acc: 0.9497, Test Acc: 0.8151, Test f1: 0.8290, AUC: class-0>>0.9719699342311305|class-1>>0.9475218658892127\n",
      "Epoch: 146, Train Loss: 0.0526, Test Loss: 0.4043, Train Acc: 0.9494, Test Acc: 0.8151, Test f1: 0.8290, AUC: class-0>>0.972074329261927|class-1>>0.947313619325281\n",
      "Epoch: 147, Train Loss: 0.0509, Test Loss: 0.4059, Train Acc: 0.9470, Test Acc: 0.8151, Test f1: 0.8290, AUC: class-0>>0.9725963044159097|class-1>>0.9477301124531444\n",
      "Epoch: 148, Train Loss: 0.0519, Test Loss: 0.4041, Train Acc: 0.9494, Test Acc: 0.8151, Test f1: 0.8290, AUC: class-0>>0.9722831193235202|class-1>>0.9474524503679022\n",
      "Epoch: 149, Train Loss: 0.0507, Test Loss: 0.4112, Train Acc: 0.9494, Test Acc: 0.8213, Test f1: 0.8346, AUC: class-0>>0.971865539200334|class-1>>0.9477301124531444\n",
      "Running cross-validation fold: 03\n",
      "Epoch: 001, Train Loss: 0.4256, Test Loss: 0.4370, Train Acc: 0.7652, Test Acc: 0.6559, Test f1: 0.6962, AUC: class-0>>0.972592670376982|class-1>>0.9161479741632413\n",
      "Saved model!\n",
      "Best thresholds ===>>> class-0>>0.637376606464386|class-1>>0.717654824256897\n",
      "Epoch: 002, Train Loss: 0.2878, Test Loss: 0.2880, Train Acc: 0.7645, Test Acc: 0.6517, Test f1: 0.6877, AUC: class-0>>0.9760579649270188|class-1>>0.9170874926600118\n",
      "Epoch: 003, Train Loss: 0.3553, Test Loss: 0.3975, Train Acc: 0.8062, Test Acc: 0.7264, Test f1: 0.7793, AUC: class-0>>0.9760579649270188|class-1>>0.9362301820317087\n",
      "Saved model!\n",
      "Best thresholds ===>>> class-0>>0.711166262626648|class-1>>0.9304117560386658\n",
      "Epoch: 004, Train Loss: 0.2694, Test Loss: 0.2948, Train Acc: 0.7822, Test Acc: 0.6919, Test f1: 0.7464, AUC: class-0>>0.9757429381497427|class-1>>0.92730475631239\n",
      "Epoch: 005, Train Loss: 0.3218, Test Loss: 0.3418, Train Acc: 0.7944, Test Acc: 0.7689, Test f1: 0.8124, AUC: class-0>>0.9739577864118449|class-1>>0.9366999412800938\n",
      "Saved model!\n",
      "Best thresholds ===>>> class-0>>0.24110205471515656|class-1>>0.5567558407783508\n",
      "Epoch: 006, Train Loss: 0.3108, Test Loss: 0.3620, Train Acc: 0.7954, Test Acc: 0.7540, Test f1: 0.8027, AUC: class-0>>0.9751128845951905|class-1>>0.9324721080446271\n",
      "Epoch: 007, Train Loss: 0.2801, Test Loss: 0.3074, Train Acc: 0.8019, Test Acc: 0.7409, Test f1: 0.7959, AUC: class-0>>0.9784731702194687|class-1>>0.9347034644744568\n",
      "Epoch: 008, Train Loss: 0.3504, Test Loss: 0.3711, Train Acc: 0.7966, Test Acc: 0.7558, Test f1: 0.8065, AUC: class-0>>0.9754279113724666|class-1>>0.935995302407516\n",
      "Epoch: 009, Train Loss: 0.2498, Test Loss: 0.2745, Train Acc: 0.8075, Test Acc: 0.7652, Test f1: 0.8151, AUC: class-0>>0.9796282684028143|class-1>>0.9357604227833235\n",
      "Saved model!\n",
      "Best thresholds ===>>> class-0>>0.750652015209198|class-1>>0.7691731452941895\n",
      "Epoch: 010, Train Loss: 0.2418, Test Loss: 0.2548, Train Acc: 0.7947, Test Acc: 0.7390, Test f1: 0.7915, AUC: class-0>>0.9805733487346424|class-1>>0.9426893716970052\n",
      "Epoch: 011, Train Loss: 0.3190, Test Loss: 0.3663, Train Acc: 0.8010, Test Acc: 0.7353, Test f1: 0.7898, AUC: class-0>>0.9784731702194687|class-1>>0.930123311802701\n",
      "Epoch: 012, Train Loss: 0.2602, Test Loss: 0.3057, Train Acc: 0.8035, Test Acc: 0.7707, Test f1: 0.8161, AUC: class-0>>0.9728026882284994|class-1>>0.945742806811509\n",
      "Saved model!\n",
      "Best thresholds ===>>> class-0>>0.6433538198471069|class-1>>0.808621346950531\n",
      "Epoch: 013, Train Loss: 0.2833, Test Loss: 0.3249, Train Acc: 0.8058, Test Acc: 0.7549, Test f1: 0.8031, AUC: class-0>>0.9740627953376038|class-1>>0.9437463300058719\n",
      "Epoch: 014, Train Loss: 0.2161, Test Loss: 0.2471, Train Acc: 0.8092, Test Acc: 0.7512, Test f1: 0.8033, AUC: class-0>>0.980363330883125|class-1>>0.9421021726365237\n",
      "Epoch: 015, Train Loss: 0.2222, Test Loss: 0.2708, Train Acc: 0.8235, Test Acc: 0.7755, Test f1: 0.8245, AUC: class-0>>0.9816234379922293|class-1>>0.9398708162066941\n",
      "Saved model!\n",
      "Best thresholds ===>>> class-0>>0.6948971748352051|class-1>>0.8658130168914795\n",
      "Epoch: 016, Train Loss: 0.2186, Test Loss: 0.2667, Train Acc: 0.8170, Test Acc: 0.7862, Test f1: 0.8330, AUC: class-0>>0.981728446917988|class-1>>0.9384615384615385\n",
      "Saved model!\n",
      "Best thresholds ===>>> class-0>>0.7483701705932617|class-1>>0.8083140254020691\n",
      "Epoch: 017, Train Loss: 0.2266, Test Loss: 0.3263, Train Acc: 0.8286, Test Acc: 0.7642, Test f1: 0.8111, AUC: class-0>>0.9764780006300536|class-1>>0.946095126247798\n",
      "Epoch: 018, Train Loss: 0.2261, Test Loss: 0.2652, Train Acc: 0.8136, Test Acc: 0.7783, Test f1: 0.8264, AUC: class-0>>0.9810983933634359|class-1>>0.939401056958309\n",
      "Epoch: 019, Train Loss: 0.2181, Test Loss: 0.2488, Train Acc: 0.8246, Test Acc: 0.7820, Test f1: 0.8299, AUC: class-0>>0.9840386432846792|class-1>>0.9411626541397533\n",
      "Epoch: 020, Train Loss: 0.2028, Test Loss: 0.2619, Train Acc: 0.8365, Test Acc: 0.7904, Test f1: 0.8348, AUC: class-0>>0.9808883755119184|class-1>>0.9457428068115091\n",
      "Saved model!\n",
      "Best thresholds ===>>> class-0>>0.4834115505218506|class-1>>0.7290830016136169\n",
      "Epoch: 021, Train Loss: 0.2165, Test Loss: 0.2909, Train Acc: 0.8248, Test Acc: 0.8025, Test f1: 0.8423, AUC: class-0>>0.9795232594770555|class-1>>0.9493834409864945\n",
      "Saved model!\n",
      "Best thresholds ===>>> class-0>>0.851614236831665|class-1>>0.8698694109916687\n",
      "Epoch: 022, Train Loss: 0.2098, Test Loss: 0.2632, Train Acc: 0.8451, Test Acc: 0.7717, Test f1: 0.8205, AUC: class-0>>0.9809933844376773|class-1>>0.9410452143276571\n",
      "Epoch: 023, Train Loss: 0.2229, Test Loss: 0.2797, Train Acc: 0.8334, Test Acc: 0.7512, Test f1: 0.8038, AUC: class-0>>0.9822534915467814|class-1>>0.9439812096300646\n",
      "Epoch: 024, Train Loss: 0.3568, Test Loss: 0.4255, Train Acc: 0.8078, Test Acc: 0.7316, Test f1: 0.7854, AUC: class-0>>0.983093562952851|class-1>>0.9349383440986494\n",
      "Epoch: 025, Train Loss: 0.1952, Test Loss: 0.2307, Train Acc: 0.8117, Test Acc: 0.7908, Test f1: 0.8384, AUC: class-0>>0.9831985718786096|class-1>>0.9540810334703463\n",
      "Epoch: 026, Train Loss: 0.1952, Test Loss: 0.2740, Train Acc: 0.8081, Test Acc: 0.8216, Test f1: 0.8607, AUC: class-0>>0.9796282684028144|class-1>>0.9549031121550206\n",
      "Saved model!\n",
      "Best thresholds ===>>> class-0>>0.6345032453536987|class-1>>0.9232262372970581\n",
      "Epoch: 027, Train Loss: 0.1962, Test Loss: 0.2544, Train Acc: 0.8423, Test Acc: 0.7792, Test f1: 0.8278, AUC: class-0>>0.9803633308831251|class-1>>0.9493834409864945\n",
      "Epoch: 028, Train Loss: 0.1961, Test Loss: 0.2604, Train Acc: 0.8401, Test Acc: 0.7553, Test f1: 0.8067, AUC: class-0>>0.982043473695264|class-1>>0.947269524368761\n",
      "Epoch: 029, Train Loss: 0.1892, Test Loss: 0.2654, Train Acc: 0.8096, Test Acc: 0.8226, Test f1: 0.8555, AUC: class-0>>0.9792082326997795|class-1>>0.9523194362889018\n",
      "Epoch: 030, Train Loss: 0.2118, Test Loss: 0.2369, Train Acc: 0.8377, Test Acc: 0.8171, Test f1: 0.8540, AUC: class-0>>0.986768875354405|class-1>>0.9432765707574867\n",
      "Epoch: 031, Train Loss: 0.1963, Test Loss: 0.2406, Train Acc: 0.8465, Test Acc: 0.7811, Test f1: 0.8279, AUC: class-0>>0.9866638664286465|class-1>>0.9406928948913681\n",
      "Epoch: 032, Train Loss: 0.3265, Test Loss: 0.3416, Train Acc: 0.8177, Test Acc: 0.7526, Test f1: 0.8069, AUC: class-0>>0.9870839021316812|class-1>>0.9330593071051086\n",
      "Epoch: 033, Train Loss: 0.2025, Test Loss: 0.2522, Train Acc: 0.8215, Test Acc: 0.7708, Test f1: 0.8215, AUC: class-0>>0.9866638664286465|class-1>>0.9493834409864944\n",
      "Epoch: 034, Train Loss: 0.1979, Test Loss: 0.2721, Train Acc: 0.7974, Test Acc: 0.8128, Test f1: 0.8510, AUC: class-0>>0.9793132416255381|class-1>>0.9564298297122724\n",
      "Epoch: 035, Train Loss: 0.1863, Test Loss: 0.2342, Train Acc: 0.8389, Test Acc: 0.7941, Test f1: 0.8387, AUC: class-0>>0.986138821799853|class-1>>0.946095126247798\n",
      "Epoch: 036, Train Loss: 0.1942, Test Loss: 0.2745, Train Acc: 0.8140, Test Acc: 0.8273, Test f1: 0.8635, AUC: class-0>>0.9818334558437467|class-1>>0.9533763945977686\n",
      "Saved model!\n",
      "Best thresholds ===>>> class-0>>0.3433575928211212|class-1>>0.7148142457008362\n",
      "Epoch: 037, Train Loss: 0.1898, Test Loss: 0.2705, Train Acc: 0.8128, Test Acc: 0.7848, Test f1: 0.8304, AUC: class-0>>0.9759529560012601|class-1>>0.9544333529066353\n",
      "Epoch: 038, Train Loss: 0.2137, Test Loss: 0.2613, Train Acc: 0.8412, Test Acc: 0.7661, Test f1: 0.8168, AUC: class-0>>0.9843536700619553|class-1>>0.9429242513211978\n",
      "Epoch: 039, Train Loss: 0.2134, Test Loss: 0.3035, Train Acc: 0.8380, Test Acc: 0.8212, Test f1: 0.8551, AUC: class-0>>0.9848787146907487|class-1>>0.946095126247798\n",
      "Epoch: 040, Train Loss: 0.2922, Test Loss: 0.4076, Train Acc: 0.8379, Test Acc: 0.7973, Test f1: 0.8372, AUC: class-0>>0.9731177150057755|class-1>>0.9504403992953612\n",
      "Epoch: 041, Train Loss: 0.2552, Test Loss: 0.4129, Train Acc: 0.8239, Test Acc: 0.8184, Test f1: 0.8533, AUC: class-0>>0.9793132416255382|class-1>>0.955490311215502\n",
      "Epoch: 042, Train Loss: 0.2439, Test Loss: 0.3431, Train Acc: 0.8511, Test Acc: 0.7437, Test f1: 0.7950, AUC: class-0>>0.982778536175575|class-1>>0.9410452143276571\n",
      "Epoch: 043, Train Loss: 0.1712, Test Loss: 0.2589, Train Acc: 0.8338, Test Acc: 0.8352, Test f1: 0.8696, AUC: class-0>>0.9838286254331619|class-1>>0.9530240751614797\n",
      "Saved model!\n",
      "Best thresholds ===>>> class-0>>0.5341203808784485|class-1>>0.8118121027946472\n",
      "Epoch: 044, Train Loss: 0.1656, Test Loss: 0.2533, Train Acc: 0.8446, Test Acc: 0.8301, Test f1: 0.8649, AUC: class-0>>0.982463509398299|class-1>>0.9505578391074573\n",
      "Epoch: 045, Train Loss: 0.1717, Test Loss: 0.2517, Train Acc: 0.8284, Test Acc: 0.7871, Test f1: 0.8349, AUC: class-0>>0.983723616507403|class-1>>0.9538461538461539\n",
      "Epoch: 046, Train Loss: 0.1688, Test Loss: 0.2556, Train Acc: 0.8435, Test Acc: 0.7792, Test f1: 0.8278, AUC: class-0>>0.9839336343589206|class-1>>0.9532589547856724\n",
      "Epoch: 047, Train Loss: 0.1779, Test Loss: 0.2703, Train Acc: 0.8414, Test Acc: 0.7792, Test f1: 0.8278, AUC: class-0>>0.9817284469179879|class-1>>0.9516147974163242\n",
      "Epoch: 048, Train Loss: 0.1746, Test Loss: 0.2847, Train Acc: 0.8337, Test Acc: 0.8408, Test f1: 0.8714, AUC: class-0>>0.9831985718786097|class-1>>0.9527891955372872\n",
      "Saved model!\n",
      "Best thresholds ===>>> class-0>>0.8078345656394958|class-1>>0.856959879398346\n",
      "Epoch: 049, Train Loss: 0.1631, Test Loss: 0.2712, Train Acc: 0.8446, Test Acc: 0.7871, Test f1: 0.8349, AUC: class-0>>0.9818334558437467|class-1>>0.954315913094539\n",
      "Epoch: 050, Train Loss: 0.1613, Test Loss: 0.2641, Train Acc: 0.8459, Test Acc: 0.8408, Test f1: 0.8702, AUC: class-0>>0.9791032237740208|class-1>>0.9541984732824428\n",
      "Epoch: 051, Train Loss: 0.2508, Test Loss: 0.3744, Train Acc: 0.8371, Test Acc: 0.8291, Test f1: 0.8598, AUC: class-0>>0.9747978578179145|class-1>>0.9603053435114504\n",
      "Epoch: 052, Train Loss: 0.1513, Test Loss: 0.2842, Train Acc: 0.8731, Test Acc: 0.7848, Test f1: 0.8313, AUC: class-0>>0.9801533130316076|class-1>>0.9516147974163242\n",
      "Epoch: 053, Train Loss: 0.1659, Test Loss: 0.2643, Train Acc: 0.8600, Test Acc: 0.8048, Test f1: 0.8457, AUC: class-0>>0.9763729917042949|class-1>>0.9495008807985906\n",
      "Epoch: 054, Train Loss: 0.4447, Test Loss: 0.5749, Train Acc: 0.8249, Test Acc: 0.7945, Test f1: 0.8350, AUC: class-0>>0.9737477685603275|class-1>>0.9596007046388726\n",
      "Epoch: 055, Train Loss: 0.1588, Test Loss: 0.2581, Train Acc: 0.8818, Test Acc: 0.8236, Test f1: 0.8587, AUC: class-0>>0.9845636879134727|class-1>>0.9477392836171461\n",
      "Epoch: 056, Train Loss: 0.1713, Test Loss: 0.3006, Train Acc: 0.8676, Test Acc: 0.8170, Test f1: 0.8530, AUC: class-0>>0.9746928488921558|class-1>>0.9552554315913093\n",
      "Epoch: 057, Train Loss: 0.2002, Test Loss: 0.3355, Train Acc: 0.8437, Test Acc: 0.8245, Test f1: 0.8616, AUC: class-0>>0.9777381077391578|class-1>>0.9614797416324133\n",
      "Epoch: 058, Train Loss: 0.2229, Test Loss: 0.2922, Train Acc: 0.8635, Test Acc: 0.7568, Test f1: 0.8082, AUC: class-0>>0.9838286254331619|class-1>>0.9459776864357018\n",
      "Epoch: 059, Train Loss: 0.1520, Test Loss: 0.2767, Train Acc: 0.8665, Test Acc: 0.8156, Test f1: 0.8535, AUC: class-0>>0.9822534915467814|class-1>>0.9529066353493835\n",
      "Epoch: 060, Train Loss: 0.1431, Test Loss: 0.2514, Train Acc: 0.8722, Test Acc: 0.7965, Test f1: 0.8414, AUC: class-0>>0.9838286254331617|class-1>>0.951145038167939\n",
      "Epoch: 061, Train Loss: 0.1656, Test Loss: 0.2507, Train Acc: 0.8821, Test Acc: 0.7754, Test f1: 0.8233, AUC: class-0>>0.9847737057649901|class-1>>0.949970640046976\n",
      "Epoch: 062, Train Loss: 0.1595, Test Loss: 0.2584, Train Acc: 0.8689, Test Acc: 0.8006, Test f1: 0.8443, AUC: class-0>>0.9852987503937836|class-1>>0.9512624779800352\n",
      "Epoch: 063, Train Loss: 0.1465, Test Loss: 0.2671, Train Acc: 0.8811, Test Acc: 0.7792, Test f1: 0.8283, AUC: class-0>>0.9833035808043684|class-1>>0.9511450381679389\n",
      "Epoch: 064, Train Loss: 0.1670, Test Loss: 0.3029, Train Acc: 0.8757, Test Acc: 0.7838, Test f1: 0.8280, AUC: class-0>>0.9777381077391578|class-1>>0.9462125660598943\n",
      "Epoch: 065, Train Loss: 0.1910, Test Loss: 0.3368, Train Acc: 0.8659, Test Acc: 0.7768, Test f1: 0.8243, AUC: class-0>>0.9813084112149533|class-1>>0.954315913094539\n",
      "Epoch: 066, Train Loss: 0.1388, Test Loss: 0.2612, Train Acc: 0.8614, Test Acc: 0.8006, Test f1: 0.8434, AUC: class-0>>0.9813084112149533|class-1>>0.9558426306517909\n",
      "Epoch: 067, Train Loss: 0.1561, Test Loss: 0.3223, Train Acc: 0.8800, Test Acc: 0.7927, Test f1: 0.8368, AUC: class-0>>0.9760579649270188|class-1>>0.9554903112155021\n",
      "Epoch: 068, Train Loss: 0.1614, Test Loss: 0.2850, Train Acc: 0.8821, Test Acc: 0.8105, Test f1: 0.8483, AUC: class-0>>0.9841436522104379|class-1>>0.9526717557251908\n",
      "Epoch: 069, Train Loss: 0.1370, Test Loss: 0.2827, Train Acc: 0.8746, Test Acc: 0.7843, Test f1: 0.8331, AUC: class-0>>0.9833035808043684|class-1>>0.9539635936582501\n",
      "Epoch: 070, Train Loss: 0.1345, Test Loss: 0.2850, Train Acc: 0.8790, Test Acc: 0.8249, Test f1: 0.8583, AUC: class-0>>0.9764780006300535|class-1>>0.9561949500880799\n",
      "Epoch: 071, Train Loss: 0.1291, Test Loss: 0.2870, Train Acc: 0.8834, Test Acc: 0.8240, Test f1: 0.8562, AUC: class-0>>0.9785781791452273|class-1>>0.9547856723429242\n",
      "Epoch: 072, Train Loss: 0.1327, Test Loss: 0.2467, Train Acc: 0.8899, Test Acc: 0.8123, Test f1: 0.8550, AUC: class-0>>0.9860338128740942|class-1>>0.9556077510275983\n",
      "Epoch: 073, Train Loss: 0.1318, Test Loss: 0.2791, Train Acc: 0.8944, Test Acc: 0.8123, Test f1: 0.8540, AUC: class-0>>0.9823585004725401|class-1>>0.951967116852613\n",
      "Epoch: 074, Train Loss: 0.1298, Test Loss: 0.2611, Train Acc: 0.8954, Test Acc: 0.7899, Test f1: 0.8358, AUC: class-0>>0.9808883755119185|class-1>>0.955842630651791\n",
      "Epoch: 075, Train Loss: 0.1329, Test Loss: 0.2904, Train Acc: 0.9009, Test Acc: 0.8221, Test f1: 0.8583, AUC: class-0>>0.979733277328573|class-1>>0.9510275983558427\n",
      "Epoch: 076, Train Loss: 0.1251, Test Loss: 0.2637, Train Acc: 0.8856, Test Acc: 0.8352, Test f1: 0.8688, AUC: class-0>>0.9799432951800904|class-1>>0.9559600704638872\n",
      "Epoch: 077, Train Loss: 0.1389, Test Loss: 0.2667, Train Acc: 0.8966, Test Acc: 0.8203, Test f1: 0.8610, AUC: class-0>>0.9833035808043682|class-1>>0.9518496770405168\n",
      "Epoch: 078, Train Loss: 0.1253, Test Loss: 0.2957, Train Acc: 0.8911, Test Acc: 0.8235, Test f1: 0.8588, AUC: class-0>>0.9763729917042948|class-1>>0.9563123899001762\n",
      "Epoch: 079, Train Loss: 0.1230, Test Loss: 0.2789, Train Acc: 0.8682, Test Acc: 0.7903, Test f1: 0.8339, AUC: class-0>>0.9809933844376773|class-1>>0.955842630651791\n",
      "Epoch: 080, Train Loss: 0.1297, Test Loss: 0.3017, Train Acc: 0.8999, Test Acc: 0.7875, Test f1: 0.8328, AUC: class-0>>0.9818334558437467|class-1>>0.9551379917792132\n",
      "Epoch: 081, Train Loss: 0.1276, Test Loss: 0.2820, Train Acc: 0.8627, Test Acc: 0.7685, Test f1: 0.8196, AUC: class-0>>0.9851937414680249|class-1>>0.9479741632413388\n",
      "Epoch: 082, Train Loss: 0.1509, Test Loss: 0.2763, Train Acc: 0.8966, Test Acc: 0.7843, Test f1: 0.8330, AUC: class-0>>0.9854037593195422|class-1>>0.9522019964768056\n",
      "Epoch: 083, Train Loss: 0.1225, Test Loss: 0.2691, Train Acc: 0.9032, Test Acc: 0.8310, Test f1: 0.8674, AUC: class-0>>0.9833035808043682|class-1>>0.9564298297122724\n",
      "Epoch: 084, Train Loss: 0.1290, Test Loss: 0.2836, Train Acc: 0.8977, Test Acc: 0.8048, Test f1: 0.8462, AUC: class-0>>0.9849837236165073|class-1>>0.9525543159130945\n",
      "Epoch: 085, Train Loss: 0.1204, Test Loss: 0.2848, Train Acc: 0.9054, Test Acc: 0.7965, Test f1: 0.8414, AUC: class-0>>0.9825685183240576|class-1>>0.9524368761009983\n",
      "Epoch: 086, Train Loss: 0.1280, Test Loss: 0.3324, Train Acc: 0.8945, Test Acc: 0.8273, Test f1: 0.8627, AUC: class-0>>0.9760579649270187|class-1>>0.9563123899001762\n",
      "Epoch: 087, Train Loss: 0.1981, Test Loss: 0.3490, Train Acc: 0.9120, Test Acc: 0.7540, Test f1: 0.8055, AUC: class-0>>0.9836186075816444|class-1>>0.9479741632413389\n",
      "Epoch: 088, Train Loss: 0.1081, Test Loss: 0.2879, Train Acc: 0.8771, Test Acc: 0.7782, Test f1: 0.8253, AUC: class-0>>0.981413420140712|class-1>>0.954668232530828\n",
      "Epoch: 089, Train Loss: 0.1050, Test Loss: 0.2965, Train Acc: 0.9033, Test Acc: 0.7675, Test f1: 0.8172, AUC: class-0>>0.9828835451013336|class-1>>0.953141514973576\n",
      "Epoch: 090, Train Loss: 0.1105, Test Loss: 0.3061, Train Acc: 0.9197, Test Acc: 0.7806, Test f1: 0.8285, AUC: class-0>>0.9806783576604011|class-1>>0.9533763945977686\n",
      "Epoch: 091, Train Loss: 0.1131, Test Loss: 0.3211, Train Acc: 0.9198, Test Acc: 0.8062, Test f1: 0.8462, AUC: class-0>>0.9762679827785361|class-1>>0.955490311215502\n",
      "Epoch: 092, Train Loss: 0.1184, Test Loss: 0.3609, Train Acc: 0.8871, Test Acc: 0.8277, Test f1: 0.8597, AUC: class-0>>0.9745878399663971|class-1>>0.9601879036993541\n",
      "Epoch: 093, Train Loss: 0.1006, Test Loss: 0.2967, Train Acc: 0.8980, Test Acc: 0.7945, Test f1: 0.8355, AUC: class-0>>0.978683188070986|class-1>>0.958661186142102\n",
      "Epoch: 094, Train Loss: 0.1194, Test Loss: 0.3618, Train Acc: 0.9079, Test Acc: 0.8319, Test f1: 0.8607, AUC: class-0>>0.974272813189121|class-1>>0.9564298297122725\n",
      "Epoch: 095, Train Loss: 0.0931, Test Loss: 0.2927, Train Acc: 0.9090, Test Acc: 0.8128, Test f1: 0.8521, AUC: class-0>>0.9810983933634359|class-1>>0.9576042278332355\n",
      "Epoch: 096, Train Loss: 0.1261, Test Loss: 0.3469, Train Acc: 0.9134, Test Acc: 0.8352, Test f1: 0.8685, AUC: class-0>>0.978998214848262|class-1>>0.9557251908396946\n",
      "Epoch: 097, Train Loss: 0.0980, Test Loss: 0.2982, Train Acc: 0.9187, Test Acc: 0.7820, Test f1: 0.8294, AUC: class-0>>0.9829885540270924|class-1>>0.9530240751614797\n",
      "Epoch: 098, Train Loss: 0.0964, Test Loss: 0.3090, Train Acc: 0.9188, Test Acc: 0.7885, Test f1: 0.8355, AUC: class-0>>0.9810983933634359|class-1>>0.9530240751614797\n",
      "Epoch: 099, Train Loss: 0.0965, Test Loss: 0.3045, Train Acc: 0.9146, Test Acc: 0.7899, Test f1: 0.8358, AUC: class-0>>0.9787881969967447|class-1>>0.9511450381679389\n",
      "Epoch: 100, Train Loss: 0.0942, Test Loss: 0.3172, Train Acc: 0.9124, Test Acc: 0.7983, Test f1: 0.8401, AUC: class-0>>0.9784731702194686|class-1>>0.9567821491485614\n",
      "Epoch: 101, Train Loss: 0.0960, Test Loss: 0.3211, Train Acc: 0.9124, Test Acc: 0.7983, Test f1: 0.8401, AUC: class-0>>0.9791032237740207|class-1>>0.9574867880211392\n",
      "Epoch: 102, Train Loss: 0.0853, Test Loss: 0.3143, Train Acc: 0.9234, Test Acc: 0.7993, Test f1: 0.8433, AUC: class-0>>0.9813084112149533|class-1>>0.9552554315913094\n",
      "Epoch: 103, Train Loss: 0.0940, Test Loss: 0.3501, Train Acc: 0.9190, Test Acc: 0.8315, Test f1: 0.8644, AUC: class-0>>0.9785781791452273|class-1>>0.9553728714034057\n",
      "Epoch: 104, Train Loss: 0.0848, Test Loss: 0.3066, Train Acc: 0.9289, Test Acc: 0.7941, Test f1: 0.8380, AUC: class-0>>0.9780531345164338|class-1>>0.9550205519671169\n",
      "Epoch: 105, Train Loss: 0.0828, Test Loss: 0.3116, Train Acc: 0.9222, Test Acc: 0.7993, Test f1: 0.8433, AUC: class-0>>0.9828835451013336|class-1>>0.9540810334703466\n",
      "Epoch: 106, Train Loss: 0.1025, Test Loss: 0.3467, Train Acc: 0.9201, Test Acc: 0.8194, Test f1: 0.8569, AUC: class-0>>0.9814134201407119|class-1>>0.9518496770405167\n",
      "Epoch: 107, Train Loss: 0.0917, Test Loss: 0.2959, Train Acc: 0.9365, Test Acc: 0.8263, Test f1: 0.8598, AUC: class-0>>0.982463509398299|class-1>>0.9520845566647094\n",
      "Epoch: 108, Train Loss: 0.0819, Test Loss: 0.3180, Train Acc: 0.9093, Test Acc: 0.7871, Test f1: 0.8345, AUC: class-0>>0.9796282684028143|class-1>>0.9556077510275984\n",
      "Epoch: 109, Train Loss: 0.0825, Test Loss: 0.3259, Train Acc: 0.9278, Test Acc: 0.7941, Test f1: 0.8380, AUC: class-0>>0.982043473695264|class-1>>0.9537287140340576\n",
      "Epoch: 110, Train Loss: 0.0785, Test Loss: 0.3290, Train Acc: 0.9400, Test Acc: 0.7932, Test f1: 0.8355, AUC: class-0>>0.9793132416255381|class-1>>0.9532589547856724\n",
      "Epoch: 111, Train Loss: 0.0810, Test Loss: 0.3129, Train Acc: 0.9256, Test Acc: 0.8301, Test f1: 0.8637, AUC: class-0>>0.9834085897301271|class-1>>0.9523194362889019\n",
      "Epoch: 112, Train Loss: 0.1088, Test Loss: 0.3823, Train Acc: 0.9378, Test Acc: 0.8076, Test f1: 0.8466, AUC: class-0>>0.979733277328573|class-1>>0.9564298297122724\n",
      "Epoch: 113, Train Loss: 0.0883, Test Loss: 0.3511, Train Acc: 0.9411, Test Acc: 0.8011, Test f1: 0.8411, AUC: class-0>>0.9783681612937098|class-1>>0.9534938344098649\n",
      "Epoch: 114, Train Loss: 0.0801, Test Loss: 0.3172, Train Acc: 0.9431, Test Acc: 0.7740, Test f1: 0.8228, AUC: class-0>>0.9831985718786096|class-1>>0.951614797416324\n",
      "Epoch: 115, Train Loss: 0.0745, Test Loss: 0.3121, Train Acc: 0.9312, Test Acc: 0.7685, Test f1: 0.8195, AUC: class-0>>0.9831985718786097|class-1>>0.9512624779800353\n",
      "Epoch: 116, Train Loss: 0.0711, Test Loss: 0.3231, Train Acc: 0.9378, Test Acc: 0.8048, Test f1: 0.8457, AUC: class-0>>0.9791032237740208|class-1>>0.9549031121550206\n",
      "Epoch: 117, Train Loss: 0.0862, Test Loss: 0.3839, Train Acc: 0.9488, Test Acc: 0.8072, Test f1: 0.8489, AUC: class-0>>0.9758479470755014|class-1>>0.9563123899001762\n",
      "Epoch: 118, Train Loss: 0.0700, Test Loss: 0.3366, Train Acc: 0.9422, Test Acc: 0.7993, Test f1: 0.8433, AUC: class-0>>0.9809933844376771|class-1>>0.9524368761009983\n",
      "Epoch: 119, Train Loss: 0.0792, Test Loss: 0.3487, Train Acc: 0.9465, Test Acc: 0.8376, Test f1: 0.8729, AUC: class-0>>0.9826735272498162|class-1>>0.9536112742219612\n",
      "Epoch: 120, Train Loss: 0.0845, Test Loss: 0.3860, Train Acc: 0.9466, Test Acc: 0.7913, Test f1: 0.8363, AUC: class-0>>0.9757429381497427|class-1>>0.9533763945977687\n",
      "Epoch: 121, Train Loss: 0.0672, Test Loss: 0.3231, Train Acc: 0.9445, Test Acc: 0.8090, Test f1: 0.8475, AUC: class-0>>0.9821484826210227|class-1>>0.9523194362889018\n",
      "Epoch: 122, Train Loss: 0.0684, Test Loss: 0.3324, Train Acc: 0.9465, Test Acc: 0.7951, Test f1: 0.8413, AUC: class-0>>0.9831985718786097|class-1>>0.9511450381679388\n",
      "Epoch: 123, Train Loss: 0.0669, Test Loss: 0.3363, Train Acc: 0.9510, Test Acc: 0.7951, Test f1: 0.8417, AUC: class-0>>0.981413420140712|class-1>>0.9506752789195538\n",
      "Epoch: 124, Train Loss: 0.0655, Test Loss: 0.3482, Train Acc: 0.9400, Test Acc: 0.8422, Test f1: 0.8710, AUC: class-0>>0.9806783576604011|class-1>>0.9510275983558427\n",
      "Epoch: 125, Train Loss: 0.0686, Test Loss: 0.3558, Train Acc: 0.9575, Test Acc: 0.8259, Test f1: 0.8626, AUC: class-0>>0.9794182505512968|class-1>>0.951614797416324\n",
      "Epoch: 126, Train Loss: 0.0670, Test Loss: 0.3390, Train Acc: 0.9532, Test Acc: 0.7782, Test f1: 0.8253, AUC: class-0>>0.9826735272498163|class-1>>0.9500880798590722\n",
      "Epoch: 127, Train Loss: 0.0636, Test Loss: 0.3465, Train Acc: 0.9554, Test Acc: 0.7726, Test f1: 0.8222, AUC: class-0>>0.9808883755119184|class-1>>0.9511450381679389\n",
      "Epoch: 128, Train Loss: 0.0654, Test Loss: 0.3482, Train Acc: 0.9576, Test Acc: 0.7871, Test f1: 0.8345, AUC: class-0>>0.9804683398088837|class-1>>0.9514973576042278\n",
      "Epoch: 129, Train Loss: 0.0623, Test Loss: 0.3508, Train Acc: 0.9554, Test Acc: 0.7993, Test f1: 0.8433, AUC: class-0>>0.9807833665861598|class-1>>0.9510275983558427\n",
      "Epoch: 130, Train Loss: 0.0658, Test Loss: 0.3510, Train Acc: 0.9554, Test Acc: 0.7834, Test f1: 0.8307, AUC: class-0>>0.9825685183240574|class-1>>0.9491485613623019\n",
      "Epoch: 131, Train Loss: 0.0698, Test Loss: 0.3768, Train Acc: 0.9533, Test Acc: 0.8366, Test f1: 0.8692, AUC: class-0>>0.9798382862543317|class-1>>0.9504403992953612\n",
      "Epoch: 132, Train Loss: 0.0611, Test Loss: 0.3624, Train Acc: 0.9554, Test Acc: 0.8053, Test f1: 0.8434, AUC: class-0>>0.9792082326997794|class-1>>0.9518496770405167\n",
      "Epoch: 133, Train Loss: 0.0608, Test Loss: 0.3567, Train Acc: 0.9511, Test Acc: 0.7726, Test f1: 0.8222, AUC: class-0>>0.9806783576604011|class-1>>0.949618320610687\n",
      "Epoch: 134, Train Loss: 0.0603, Test Loss: 0.3681, Train Acc: 0.9555, Test Acc: 0.8021, Test f1: 0.8447, AUC: class-0>>0.9802583219573664|class-1>>0.9510275983558426\n",
      "Epoch: 135, Train Loss: 0.0595, Test Loss: 0.3662, Train Acc: 0.9533, Test Acc: 0.8287, Test f1: 0.8635, AUC: class-0>>0.9802583219573663|class-1>>0.9495008807985907\n",
      "Epoch: 136, Train Loss: 0.0585, Test Loss: 0.3661, Train Acc: 0.9489, Test Acc: 0.8217, Test f1: 0.8609, AUC: class-0>>0.9796282684028141|class-1>>0.9503229594832647\n",
      "Epoch: 137, Train Loss: 0.0588, Test Loss: 0.3701, Train Acc: 0.9643, Test Acc: 0.8319, Test f1: 0.8616, AUC: class-0>>0.9788932059225034|class-1>>0.9503229594832647\n",
      "Epoch: 138, Train Loss: 0.0591, Test Loss: 0.3720, Train Acc: 0.9621, Test Acc: 0.8259, Test f1: 0.8626, AUC: class-0>>0.9796282684028143|class-1>>0.9497357604227833\n",
      "Epoch: 139, Train Loss: 0.0587, Test Loss: 0.3701, Train Acc: 0.9555, Test Acc: 0.8217, Test f1: 0.8611, AUC: class-0>>0.9796282684028141|class-1>>0.9498532002348796\n",
      "Epoch: 140, Train Loss: 0.0577, Test Loss: 0.3648, Train Acc: 0.9533, Test Acc: 0.7913, Test f1: 0.8371, AUC: class-0>>0.9805733487346424|class-1>>0.9495008807985907\n",
      "Epoch: 141, Train Loss: 0.0569, Test Loss: 0.3672, Train Acc: 0.9533, Test Acc: 0.8259, Test f1: 0.8626, AUC: class-0>>0.9805733487346424|class-1>>0.949618320610687\n",
      "Epoch: 142, Train Loss: 0.0570, Test Loss: 0.3685, Train Acc: 0.9555, Test Acc: 0.8338, Test f1: 0.8684, AUC: class-0>>0.9804683398088837|class-1>>0.9495008807985907\n",
      "Epoch: 143, Train Loss: 0.0564, Test Loss: 0.3695, Train Acc: 0.9600, Test Acc: 0.8287, Test f1: 0.8635, AUC: class-0>>0.9802583219573664|class-1>>0.9493834409864944\n",
      "Epoch: 144, Train Loss: 0.0561, Test Loss: 0.3706, Train Acc: 0.9600, Test Acc: 0.8021, Test f1: 0.8447, AUC: class-0>>0.9801533130316077|class-1>>0.9499706400469758\n",
      "Epoch: 145, Train Loss: 0.0575, Test Loss: 0.3753, Train Acc: 0.9555, Test Acc: 0.8287, Test f1: 0.8635, AUC: class-0>>0.9804683398088837|class-1>>0.9491485613623017\n",
      "Epoch: 146, Train Loss: 0.0559, Test Loss: 0.3689, Train Acc: 0.9600, Test Acc: 0.8287, Test f1: 0.8635, AUC: class-0>>0.9808883755119185|class-1>>0.9491485613623019\n",
      "Epoch: 147, Train Loss: 0.0560, Test Loss: 0.3714, Train Acc: 0.9533, Test Acc: 0.8338, Test f1: 0.8684, AUC: class-0>>0.980048304105849|class-1>>0.9495008807985907\n",
      "Epoch: 148, Train Loss: 0.0558, Test Loss: 0.3739, Train Acc: 0.9555, Test Acc: 0.8338, Test f1: 0.8684, AUC: class-0>>0.980048304105849|class-1>>0.9493834409864943\n",
      "Epoch: 149, Train Loss: 0.0557, Test Loss: 0.3774, Train Acc: 0.9621, Test Acc: 0.8021, Test f1: 0.8447, AUC: class-0>>0.9804683398088837|class-1>>0.9492660011743981\n"
     ]
    }
   ],
   "source": [
    "torch.save(model.state_dict(), os.path.join(weights_path, 'init_distributed.pth'))\n",
    "for i in range(num_fold):\n",
    "    model.load_state_dict(torch.load(os.path.join(weights_path, 'init_distributed.pth')))\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, num_epochs, 0.00001)\n",
    "    optimal_score = 0\n",
    "    print(f'Running cross-validation fold: {i:02d}')\n",
    "    graph_train  = []\n",
    "    graph_test = []\n",
    "    for k in range(num_fold):\n",
    "        if k != i: graph_train += chunks_data[k]\n",
    "        else: graph_test = chunks_data[k]\n",
    "    n_sample = []\n",
    "    for graph in graph_train:\n",
    "        n_sample.append(graph.y.numpy())\n",
    "    n_sample = np.asarray(n_sample)\n",
    "    counts = np.count_nonzero(n_sample, axis=0)\n",
    "    counts = counts.max() / counts\n",
    "    criterion = torch.nn.BCEWithLogitsLoss(pos_weight=torch.from_numpy(counts)).cuda()   \n",
    "    # criterion = nn.MSELoss()\n",
    "    for epoch in range(1, num_epochs):\n",
    "        shuffle(graph_train)\n",
    "        train(graph_train)\n",
    "        scheduler.step()    \n",
    "        train_loss, train_acc, _, _, _ = test(graph_train)\n",
    "        test_loss, test_acc, avg_prc, aucs, thresh = test(graph_test)\n",
    "        log_msg = f'Epoch: {epoch:03d}, Train Loss: {train_loss:.4f}, Test Loss: {test_loss:.4f}, Train Acc: {train_acc:.4f}, Test Acc: {test_acc:.4f}, Test f1: {avg_prc:.4f}' + ', AUC: ' + '|'.join('class-{}>>{}'.format(*k) for k in enumerate(aucs))\n",
    "        print(log_msg)\n",
    "        sum_metrics = (avg_prc+test_acc+sum(aucs))/3\n",
    "        if sum_metrics > optimal_score:\n",
    "            optimal_score = sum_metrics\n",
    "            os.makedirs(weights_path, exist_ok=True)\n",
    "            torch.save(model.state_dict(), os.path.join(weights_path, f'{weights_name}_fold{i:02d}.pth'))\n",
    "            print('Saved model!')\n",
    "            print('Best thresholds ===>>> '+ '|'.join('class-{}>>{}'.format(*k) for k in enumerate(thresh)))\n",
    "            with open(os.path.join(weights_path, f'{weights_name}_fold{i:02d}.txt'), \"w\") as text_file:\n",
    "                print('Best thresholds ===>>> '+ '|'.join('class-{}>>{}'.format(*k) for k in enumerate(thresh))+'|||'+log_msg, file=text_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f31d9168",
   "metadata": {},
   "source": [
    "### Read result summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "10904f65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation results of GCN_TMA_Distributed: Acc 0.8638, f1 0.882275, AUC PDAC 0.9817209148686096, AUC CP 0.9548312861924901\n"
     ]
    }
   ],
   "source": [
    "log_files = glob(os.path.join(weights_path, f'{weights_name}_*.txt'))\n",
    "results = []\n",
    "for log_file in log_files:\n",
    "    with open(log_file) as text_file:\n",
    "        lines = text_file.readlines()[0]\n",
    "        results.append([float(i) for i in re.findall(\"\\d+\\.\\d+\", lines)[3:]])\n",
    "results = np.vstack(results).mean(0)\n",
    "print(f'Cross-validation results of {weights_name}: Acc {results[2]}, f1 {results[3]}, AUC PDAC {results[4]}, AUC CP {results[5]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "21802e70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation results of MLP_TMA_3way: Acc 0.834325, f1 0.860875, AUC PDAC 0.976553873508877, AUC CP 0.9510270537074669\n"
     ]
    }
   ],
   "source": [
    "log_files = glob(os.path.join(weights_path, f'{weights_name}_*.txt'))\n",
    "results = []\n",
    "for log_file in log_files:\n",
    "    with open(log_file) as text_file:\n",
    "        lines = text_file.readlines()[0]\n",
    "        results.append([float(i) for i in re.findall(\"\\d+\\.\\d+\", lines)[3:]])\n",
    "results = np.vstack(results).mean(0)\n",
    "print(f'Cross-validation results of {weights_name}: Acc {results[2]}, f1 {results[3]}, AUC PDAC {results[4]}, AUC CP {results[5]}')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "de88db708fd7569f2666ff37f921dc0f88a8459546879a55f00d65d0006c4c3e"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('pt')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
