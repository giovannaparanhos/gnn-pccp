{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0df9abff-b363-48fd-8cd4-873ce5a7145c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "from os.path import join, basename, splitext\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.transforms import RadiusGraph\n",
    "from skimage import io\n",
    "from tqdm import tqdm\n",
    "from skimage import img_as_float, exposure\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from torchvision import models\n",
    "import torch.nn as nn\n",
    "from random import shuffle\n",
    "from sklearn.metrics import roc_auc_score, f1_score, balanced_accuracy_score, roc_curve\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "from torch_geometric.data import Data\n",
    "from models import GraphNet, Preprocess, AttentionPool\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fdb892be-fc61-43b4-befb-80b56c5f9d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions for visualization.\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "import networkx as nx\n",
    "\n",
    "def visualize_points(pos, edge_index=None, index=None, edge_weights=None, node_weights=None, show=True, img=None, patch_size=1, fig_text=None, box_color='palegreen'):\n",
    "    fig = plt.figure(figsize=(torch.max(pos, 0)[0].numpy()[0], torch.max(pos, 0)[0].numpy()[1]))\n",
    "        # fig_w, fig_h = plt.gcf().get_size_inches()\n",
    "    if edge_index is not None:\n",
    "        if edge_weights is None:\n",
    "            for (src, dst) in edge_index.t().tolist():\n",
    "                src = pos[src].tolist()\n",
    "                dst = pos[dst].tolist()\n",
    "                plt.plot([src[0]*patch_size+int(patch_size/2), dst[0]*patch_size+int(patch_size/2)], [src[1]*patch_size+int(patch_size/2), dst[1]*patch_size+int(patch_size/2)], linewidth=3, color='royalblue')\n",
    "        else:\n",
    "            i = 0\n",
    "            for (s, d) in edge_index.t().tolist():\n",
    "                src = pos[s].tolist()\n",
    "                dst = pos[d].tolist()\n",
    "                plt.plot([src[0]*patch_size+int(patch_size/2), dst[0]*patch_size+int(patch_size/2)], [src[1]*patch_size+int(patch_size/2), dst[1]*patch_size+int(patch_size/2)], linewidth=widths[i]*3, color='royalblue')\n",
    "                i+=1\n",
    "    if index is None:\n",
    "        if node_weights is not None:\n",
    "            for p, w in zip(pos, node_weights):\n",
    "                plt.scatter(p[0]*patch_size+int(patch_size/2), p[1]*patch_size+int(patch_size/2), s=w*500, zorder=1000, color='red')\n",
    "        else:\n",
    "            plt.scatter(pos[:, 0]*patch_size+int(patch_size/2), pos[:, 1]*patch_size+int(patch_size/2), s=500, zorder=1000, color='red')\n",
    "    else:\n",
    "        mask = torch.zeros(pos.size(0), dtype=torch.bool)\n",
    "        mask[index] = True\n",
    "        plt.scatter(pos[~mask, 0], pos[~mask, 1], s=50, color='lightgray', zorder=1000)\n",
    "        plt.scatter(pos[mask, 0], pos[mask, 1], s=50, zorder=1000)\n",
    "    plt.axis('off')\n",
    "    plt.gca().invert_yaxis()\n",
    "    if img is not None:\n",
    "        im = plt.imread(img)\n",
    "        plt.imshow(im, alpha=0.5)\n",
    "    if fig_text is not None:\n",
    "        plt.figtext(0.5, 0.1, fig_text, ha=\"center\", fontsize=18, bbox={\"facecolor\":box_color, \"alpha\":0.5, \"pad\":5} )\n",
    "    if show:\n",
    "        plt.show()\n",
    "    plt.close()\n",
    "    return fig\n",
    "    \n",
    "def visualize_projection(h, color):\n",
    "    z = TSNE(n_components=2).fit_transform(h.detach().cpu().numpy())\n",
    "\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "\n",
    "    plt.scatter(z[:, 0], z[:, 1], s=70, c=color, cmap=\"Set2\")\n",
    "    plt.show()\n",
    "    \n",
    "def visualize_grid(pos, color):\n",
    "    color = color.detach().cpu().numpy()\n",
    "    pos = pos.detach().cpu().numpy()\n",
    "    # plt.figure(figsize=(10,10))\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "\n",
    "    plt.scatter(pos[:, 0], pos[:, 1], s=70, c=color, cmap=\"Set2\")\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.show()\n",
    "    \n",
    "def visualize_graph(h, color, epoch=None, loss=None):\n",
    "    plt.figure(figsize=(7,7))\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "\n",
    "    if torch.is_tensor(h):\n",
    "        h = h.detach().cpu().numpy()\n",
    "        plt.scatter(h[:, 0], h[:, 1], s=140, c=color, cmap=\"Set2\")\n",
    "        if epoch is not None and loss is not None:\n",
    "            plt.xlabel(f'Epoch: {epoch}, Loss: {loss.item():.4f}', fontsize=16)\n",
    "    else:\n",
    "        nx.draw_networkx(h, pos=nx.spring_layout(h, seed=42), with_labels=False,\n",
    "                         node_color=color, cmap=\"Set2\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "38d25aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_dataset_to_graph(csv, class_codes, num_feat=(1, 513), compute_edges=False, weight_func=cosine_similarity, radius=1.5, remove_empty=False, weights_exposure=False):\n",
    "    df = pd.read_csv(csv)\n",
    "    feat_arr = np.array(df.iloc[:, num_feat[0]:num_feat[1]])\n",
    "    pos_arr = np.array(df.iloc[:, -3:-1])\n",
    "    label_arr = np.array(class_codes.get(df.iloc[0, -1]))\n",
    "\n",
    "    # if remove_empty and os.path.splitext(os.path.basename(csv))[0]:\n",
    "    #     empty_idx = np.where( np.sum(feat_arr, 1) == 0)[0]\n",
    "    #     feat_arr[empty_idx, :] = 0\n",
    "    #     if len(empty_idx)>0: pos_arr[empty_idx, :] = pos_arr[empty_idx[0], :]\n",
    "\n",
    "    data = Data(x=torch.tensor(feat_arr, dtype=torch.float), pos=torch.tensor(pos_arr, dtype=torch.long), y=torch.tensor(label_arr, dtype=torch.long))\n",
    "    radius_graph = RadiusGraph(radius, loop=True) # 1.5 or 2\n",
    "    data = radius_graph(data) \n",
    "    if compute_edges:\n",
    "        weights = []\n",
    "        for i in range(data.edge_index.shape[1]):\n",
    "            edge = data.edge_index[:, i]\n",
    "            weight = weight_func( data.x[edge[0]].view(1, -1), data.x[edge[1]].view(1, -1) )\n",
    "            weights.append(weight)\n",
    "        if weights_exposure: weights = exposure.rescale_intensity( np.vstack(weights).squeeze(), in_range=(0.5, 1), out_range=(0, 1) )\n",
    "        weights = np.vstack(weights).squeeze()\n",
    "        data.edge_weight = torch.tensor(weights, dtype=torch.float)\n",
    "    data.name = os.path.splitext(os.path.basename(csv))[0]\n",
    "    data.label = df.iloc[0, -1]\n",
    "    return data\n",
    "\n",
    "def compute_dataset(embedder, path, class_codes, ext='png', opt_folder=False):\n",
    "    classes = glob(join(path, '*'))\n",
    "    # print(classes) \n",
    "    data_list = [] # pos, feats, node labels, node_mask, graph label\n",
    "    # return None\n",
    "    for idx, c in enumerate(classes):\n",
    "        class_name = basename(c)\n",
    "        if opt_folder:\n",
    "            regions = glob(join(path, class_name, '*', '*'))\n",
    "        else:\n",
    "            regions = glob(join(path, class_name, '*'))\n",
    "        regions = [x for x in regions if os.path.isdir(x)]\n",
    "        for region in tqdm(regions):\n",
    "            try:\n",
    "                pos_list = []\n",
    "                feat_list = []\n",
    "                node_list = []\n",
    "                mask_list = []\n",
    "                patches = glob(join(region, '*.'+ext))\n",
    "                for patch in patches:\n",
    "                    ### patch pos -> x, y\n",
    "                    patch_data = img_as_float(io.imread(patch))\n",
    "\n",
    "                    # feat = np.mean(patch_data)\n",
    "                    with torch.no_grad():\n",
    "                        patch_tensor = torch.FloatTensor(patch_data.transpose(2, 0, 1))[None, :].cuda()\n",
    "                        feat_tensor = embedder(patch_tensor)\n",
    "                        feat = feat_tensor.detach().cpu().numpy().squeeze()\n",
    "                    masked = False\n",
    "                    node_label = np.nan\n",
    "\n",
    "                    x = int(splitext(basename(patch))[0].split('_')[0])\n",
    "                    y = int(splitext(basename(patch))[0].split('_')[1])\n",
    "                    pos_list.append((x, y))\n",
    "                    feat_list.append(feat)  \n",
    "                    node_list.append(node_label)\n",
    "                    mask_list.append(masked)\n",
    "                pos_arr = np.vstack(pos_list)\n",
    "                feat_arr = np.vstack(feat_list)\n",
    "                node_arr = np.vstack(node_list)\n",
    "                mask_arr = np.vstack(mask_list)\n",
    "                graph_label = np.array(class_codes.get(class_name))\n",
    "                graph_name = region\n",
    "                data_list.append((pos_arr, feat_arr, node_arr, mask_arr, graph_label, graph_name))\n",
    "            except:\n",
    "                print(region)\n",
    "    return data_list\n",
    "\n",
    "def sample_weights(graph_train):\n",
    "    n_sample = []\n",
    "    for graph in graph_train:\n",
    "        n_sample.append(graph.y.numpy())\n",
    "    n_sample = np.asarray(n_sample)\n",
    "    _, counts = np.unique(n_sample, return_counts=True)\n",
    "    counts = counts.max() / (10e-3+counts)\n",
    "    return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd710312-a6b0-4d71-aff4-bd597820fe2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "distributed_codes = {\n",
    "    'normal': (0, 0),\n",
    "    'pancreatitis': (0, 1),\n",
    "    'pdac': (1, 1)\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a669b6",
   "metadata": {},
   "source": [
    "### Compute representations of patches of every ROI, save them as csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5851a6b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_path = 'data_bags/TMA' # bags of patches\n",
    "patch_ext = 'png' # extension of patch file\n",
    "out_path = 'dataframes/TMA' # feature files output directory\n",
    "resnet = models.resnet18(pretrained=True) # embedder\n",
    "resnet.fc = nn.Identity() # remove fc layer\n",
    "resnet = resnet.cuda()\n",
    "resnet.eval()\n",
    "data_list = compute_dataset(resnet, bag_path, distributed_codes, ext='png')\n",
    "for data in tqdm(data_list):\n",
    "    pos = data[0]\n",
    "    feat = data[1]\n",
    "    label = data[5].split(os.sep)[-2]\n",
    "    name = os.sep.join(data[5].split(os.sep)[-2:])\n",
    "    save_name = os.path.join(out_path, name)\n",
    "    os.makedirs(os.sep.join(save_name.split(os.sep)[:-1]), exist_ok=True)\n",
    "    df = pd.DataFrame(data=feat)\n",
    "    df = df.assign(pos_x=pd.Series(pos[:, 0]).values)\n",
    "    df = df.assign(pos_y=pd.Series(pos[:, 1]).values)\n",
    "    df = df.assign(label=pd.Series([label]*feat.shape[0]).values)\n",
    "    df.to_csv(save_name+'.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e2144da",
   "metadata": {},
   "source": [
    "### Read representations of ROIs from csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3e5e2c6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 786/786 [02:36<00:00,  5.02it/s]\n"
     ]
    }
   ],
   "source": [
    "csv_path = 'dataframes/TMA'  # directoyr of feature files\n",
    "num_feat = (1, 513) # 512x1 vector\n",
    "csvs = glob(os.path.join(csv_path, '*', '*.csv'))\n",
    "train_graph_list = []\n",
    "fov_normal = []\n",
    "fov_pancreatitis = []\n",
    "fov_pdac = []\n",
    "for csv in tqdm(csvs):\n",
    "    data = read_dataset_to_graph(csv, distributed_codes, num_feat, compute_edges=True, weight_func=cosine_similarity, radius=1.5, weights_exposure=False)\n",
    "    train_graph_list.append(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27182867",
   "metadata": {},
   "source": [
    "### Core level separation for cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d6a1355d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 576x648 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_fold = 4\n",
    "while(True):\n",
    "    weight_mean = max(sample_weights(train_graph_list))\n",
    "    shuffle(train_graph_list)\n",
    "    cores_per_chunk = int(len(train_graph_list)/num_fold)\n",
    "    chunks_data = [train_graph_list[i*cores_per_chunk:(i+1)*cores_per_chunk] for i in range(num_fold)]\n",
    "    weights = []\n",
    "    for chunk_data in chunks_data:\n",
    "        weights.append(sample_weights(chunk_data))\n",
    "    max_weight = max([max(i) for i in weights])\n",
    "    if max_weight<weight_mean*1.2: break\n",
    "fig = visualize_points(train_graph_list[0].pos, edge_index=train_graph_list[0].edge_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d00191a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_label_roc(labels, predictions, pos_label=1):\n",
    "    num_class = labels.shape[1]\n",
    "    fprs = []\n",
    "    tprs = []\n",
    "    thresholds = []\n",
    "    thresholds_optimal = []\n",
    "    aucs = []\n",
    "    if len(predictions.shape)==1:\n",
    "        predictions = predictions[:, None]\n",
    "    for c in range(0, num_class):\n",
    "        label = labels[:, c]\n",
    "        prediction = predictions[:, c]\n",
    "        fpr, tpr, threshold = roc_curve(label, prediction, pos_label=pos_label)\n",
    "        fpr_optimal, tpr_optimal, threshold_optimal = optimal_thresh(fpr, tpr, threshold)\n",
    "        c_auc = roc_auc_score(label, prediction)\n",
    "        aucs.append(c_auc)\n",
    "        thresholds.append(threshold)\n",
    "        thresholds_optimal.append(threshold_optimal)\n",
    "    return aucs, thresholds, thresholds_optimal\n",
    "\n",
    "def optimal_thresh(fpr, tpr, thresholds, p=0):\n",
    "    loss = (fpr - tpr) - p * tpr / (fpr + tpr + 1)\n",
    "    idx = np.argmin(loss, axis=0)\n",
    "    return fpr[idx], tpr[idx], thresholds[idx]\n",
    "\n",
    "def accuracy_from_thresh(labels, predictions, thresh, priority_func=None): # numpy array, 2D, 2D, 1D\n",
    "    labels = copy.deepcopy(labels)\n",
    "    predictions = copy.deepcopy(predictions)\n",
    "    num_class = labels.shape[1]\n",
    "    if num_class==1:\n",
    "        class_prediction_bag = copy.deepcopy(predictions)\n",
    "        class_prediction_bag[predictions>=thresh[0]] = 1\n",
    "        class_prediction_bag[predictions<thresh[0]] = 0\n",
    "        predictions = class_prediction_bag\n",
    "    else:        \n",
    "        for i in range(num_class):\n",
    "            class_prediction_bag = copy.deepcopy(predictions[:, i])\n",
    "            class_prediction_bag[predictions[:, i]>=thresh[i]] = 1\n",
    "            class_prediction_bag[predictions[:, i]<thresh[i]] = 0\n",
    "            predictions[:, i] = class_prediction_bag\n",
    "    if priority_func is not None:\n",
    "        predictions = priority_func(predictions)\n",
    "        labels = priority_func(labels)\n",
    "        # print(predictions)\n",
    "        # print(labels)\n",
    "    # zeros = np.zeros((labels.shape[0], 1))\n",
    "    # labels = np.argmax(np.concatenate((zeros, labels), 1), 1)\n",
    "    # predictions = np.argmax(np.concatenate((zeros, predictions), 1), 1)\n",
    "    acc = balanced_accuracy_score(labels, predictions)\n",
    "    f1 = f1_score(labels, predictions, average='weighted')\n",
    "    return acc, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d2db9d53-7fce-43b2-adb2-48ca6a1295fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(loader):\n",
    "    model.train()\n",
    "    loss_avg = 0\n",
    "    count = 0\n",
    "    for data in loader:  \n",
    "        optimizer.zero_grad()  \n",
    "        out = model(\n",
    "            data.x.float().cuda(), \n",
    "            edge_index=data.edge_index.cuda(), \n",
    "            batch=torch.LongTensor(np.zeros(data.x.shape[0])).cuda(), \n",
    "            edge_weight= data.edge_weight.squeeze().cuda() if data.edge_weight is not None else None,\n",
    "            )\n",
    "        loss = criterion(out, data.y.float().cuda().view(1, -1)) \n",
    "        loss.backward()  \n",
    "        optimizer.step() \n",
    "        loss_avg += loss.item()\n",
    "        count += 1\n",
    "    return loss_avg / count\n",
    "\n",
    "def test(loader):\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        labels = []\n",
    "        preds = []\n",
    "        loss_avg = 0\n",
    "        for data in loader:   \n",
    "            out = model(\n",
    "                data.x.float().cuda(), \n",
    "                edge_index=data.edge_index.long().cuda(), \n",
    "                batch=torch.LongTensor(np.zeros(data.x.shape[0])).cuda(),\n",
    "                edge_weight= data.edge_weight.squeeze().cuda() if data.edge_weight is not None else None,\n",
    "                )  \n",
    "            loss = criterion(out, data.y.float().cuda().view(1, -1)) \n",
    "            preds.append(torch.sigmoid(out).cpu().numpy().squeeze())\n",
    "            labels.append(data.y.numpy().squeeze())\n",
    "            loss_avg += loss.item()\n",
    "        preds = np.array(preds)\n",
    "        labels = np.array(labels)\n",
    "        aucs, thresholds, thresholds_optimal = multi_label_roc(labels, preds)\n",
    "        def priority_func(pred):\n",
    "            codes = []\n",
    "            for x in pred:\n",
    "                if x[0]==1: code = 2\n",
    "                if x[0]==0 and x[1]==0: code = 0\n",
    "                if x[0]==0 and x[1]==1: code = 1\n",
    "                codes.append(code)\n",
    "            return codes\n",
    "        # priority_func = None\n",
    "        acc, f1 = accuracy_from_thresh(labels, preds, thresholds_optimal, priority_func)\n",
    "    return loss_avg / len(loader), acc, f1, aucs, thresholds_optimal "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9026da89",
   "metadata": {},
   "source": [
    "### Define networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1a2bfcff-147b-49dc-9a57-05963a3cca25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Preprocess(\n",
       "  (lin1): Sequential(\n",
       "    (0): Linear(in_features=512, out_features=1024, bias=True)\n",
       "    (1): PReLU(num_parameters=1)\n",
       "  )\n",
       "  (layers): Sequential(\n",
       "    (0): Sequential(\n",
       "      (0): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      (1): PReLU(num_parameters=1)\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      (1): PReLU(num_parameters=1)\n",
       "    )\n",
       "    (2): Sequential(\n",
       "      (0): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      (1): PReLU(num_parameters=1)\n",
       "    )\n",
       "  )\n",
       "  (lin2): Sequential(\n",
       "    (0): Linear(in_features=1024, out_features=512, bias=True)\n",
       "    (1): PReLU(num_parameters=1)\n",
       "  )\n",
       "  (classifier): Linear(in_features=512, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_epochs = 150\n",
    "weight_decay = 0.0001\n",
    "weights_name = 'MLP_TMA_Distributed'\n",
    "weights_path = 'weights_cv'\n",
    "os.makedirs('weights_cv', exist_ok=True)\n",
    "model = GraphNet(\n",
    "        in_channels=512, \n",
    "        out_channels=2, \n",
    "        hidden_channels=512, \n",
    "        gcn_layer='GCN', \n",
    "        graph_pool='mean', \n",
    "        drop_p0=0.0,\n",
    "        drop_p1=0.25, \n",
    "        preprocess=Preprocess(in_channels=512, hidden_channels=512, out_channels=512, n_layers=0),\n",
    "        )\n",
    "model = Preprocess(in_channels=512, hidden_channels=1024, out_channels=512, out_class=2, n_layers=3, head=True)\n",
    "lr = 0.0002\n",
    "model = model.cuda()\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5bbca3b",
   "metadata": {},
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "af793c0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running cross-validation fold: 00\n",
      "Epoch: 001, Train Loss: 0.1096, Test Loss: 0.0998, Train Acc: 0.7793, Test Acc: 0.7598, Test f1: 0.7846, AUC: class-0>>0.9679166666666668|class-1>>0.9653616548660281\n",
      "Saved model!\n",
      "Best thresholds ===>>> class-0>>0.6822259426116943|class-1>>0.6385437250137329\n",
      "Epoch: 002, Train Loss: 0.1027, Test Loss: 0.1008, Train Acc: 0.7893, Test Acc: 0.7950, Test f1: 0.8146, AUC: class-0>>0.9715625|class-1>>0.9636956823545745\n",
      "Saved model!\n",
      "Best thresholds ===>>> class-0>>0.6284773349761963|class-1>>0.5957505106925964\n",
      "Epoch: 003, Train Loss: 0.0776, Test Loss: 0.0717, Train Acc: 0.7869, Test Acc: 0.7736, Test f1: 0.7944, AUC: class-0>>0.9785416666666668|class-1>>0.9657781479938914\n",
      "Epoch: 004, Train Loss: 0.1045, Test Loss: 0.1067, Train Acc: 0.7812, Test Acc: 0.7275, Test f1: 0.7397, AUC: class-0>>0.9666666666666667|class-1>>0.9638345133971956\n",
      "Epoch: 005, Train Loss: 0.0859, Test Loss: 0.0860, Train Acc: 0.7838, Test Acc: 0.7503, Test f1: 0.7724, AUC: class-0>>0.9802083333333333|class-1>>0.9668887963348605\n",
      "Epoch: 006, Train Loss: 0.0707, Test Loss: 0.0728, Train Acc: 0.8118, Test Acc: 0.7670, Test f1: 0.7855, AUC: class-0>>0.9830208333333333|class-1>>0.9667499652922393\n",
      "Epoch: 007, Train Loss: 0.0729, Test Loss: 0.0722, Train Acc: 0.7920, Test Acc: 0.7670, Test f1: 0.7855, AUC: class-0>>0.9855208333333334|class-1>>0.9668887963348605\n",
      "Epoch: 008, Train Loss: 0.0823, Test Loss: 0.0892, Train Acc: 0.7802, Test Acc: 0.7510, Test f1: 0.7673, AUC: class-0>>0.975|class-1>>0.9648063306955436\n",
      "Epoch: 009, Train Loss: 0.0852, Test Loss: 0.0902, Train Acc: 0.8144, Test Acc: 0.7764, Test f1: 0.7992, AUC: class-0>>0.9761458333333334|class-1>>0.9666111342496182\n",
      "Epoch: 010, Train Loss: 0.0770, Test Loss: 0.0738, Train Acc: 0.8137, Test Acc: 0.8076, Test f1: 0.8257, AUC: class-0>>0.985625|class-1>>0.9688324309315564\n",
      "Saved model!\n",
      "Best thresholds ===>>> class-0>>0.6900972723960876|class-1>>0.6715763807296753\n",
      "Epoch: 011, Train Loss: 0.0847, Test Loss: 0.0910, Train Acc: 0.8294, Test Acc: 0.7724, Test f1: 0.7904, AUC: class-0>>0.9826041666666667|class-1>>0.9673052894627239\n",
      "Epoch: 012, Train Loss: 0.0634, Test Loss: 0.0663, Train Acc: 0.8170, Test Acc: 0.7798, Test f1: 0.8003, AUC: class-0>>0.9858333333333333|class-1>>0.9676523670692767\n",
      "Epoch: 013, Train Loss: 0.0755, Test Loss: 0.0719, Train Acc: 0.8300, Test Acc: 0.7632, Test f1: 0.7827, AUC: class-0>>0.9845833333333334|class-1>>0.9664723032069971\n",
      "Epoch: 014, Train Loss: 0.0594, Test Loss: 0.0671, Train Acc: 0.8315, Test Acc: 0.8212, Test f1: 0.8429, AUC: class-0>>0.9858333333333333|class-1>>0.9698042482299043\n",
      "Saved model!\n",
      "Best thresholds ===>>> class-0>>0.6568716168403625|class-1>>0.6142008900642395\n",
      "Epoch: 015, Train Loss: 0.0730, Test Loss: 0.0835, Train Acc: 0.8223, Test Acc: 0.7798, Test f1: 0.7996, AUC: class-0>>0.9841666666666666|class-1>>0.9698042482299043\n",
      "Epoch: 016, Train Loss: 0.0639, Test Loss: 0.0733, Train Acc: 0.8385, Test Acc: 0.7920, Test f1: 0.8095, AUC: class-0>>0.9850000000000001|class-1>>0.9689712619741775\n",
      "Epoch: 017, Train Loss: 0.0556, Test Loss: 0.0621, Train Acc: 0.8284, Test Acc: 0.7635, Test f1: 0.7801, AUC: class-0>>0.984375|class-1>>0.9706372344856309\n",
      "Epoch: 018, Train Loss: 0.0938, Test Loss: 0.1024, Train Acc: 0.8482, Test Acc: 0.7664, Test f1: 0.7881, AUC: class-0>>0.9816666666666667|class-1>>0.9688324309315562\n",
      "Epoch: 019, Train Loss: 0.0573, Test Loss: 0.0648, Train Acc: 0.8259, Test Acc: 0.8375, Test f1: 0.8597, AUC: class-0>>0.9858333333333333|class-1>>0.9720255449118422\n",
      "Saved model!\n",
      "Best thresholds ===>>> class-0>>0.6587514281272888|class-1>>0.609399139881134\n",
      "Epoch: 020, Train Loss: 0.0612, Test Loss: 0.0680, Train Acc: 0.8337, Test Acc: 0.8308, Test f1: 0.8499, AUC: class-0>>0.9850000000000001|class-1>>0.9720255449118422\n",
      "Epoch: 021, Train Loss: 0.0622, Test Loss: 0.0712, Train Acc: 0.8567, Test Acc: 0.8347, Test f1: 0.8574, AUC: class-0>>0.9868750000000001|class-1>>0.9709148965708733\n",
      "Epoch: 022, Train Loss: 0.0599, Test Loss: 0.0710, Train Acc: 0.8564, Test Acc: 0.8339, Test f1: 0.8517, AUC: class-0>>0.9854166666666666|class-1>>0.9720255449118422\n",
      "Epoch: 023, Train Loss: 0.0608, Test Loss: 0.0680, Train Acc: 0.8481, Test Acc: 0.7992, Test f1: 0.8238, AUC: class-0>>0.9835416666666666|class-1>>0.9718867138692211\n",
      "Epoch: 024, Train Loss: 0.0575, Test Loss: 0.0658, Train Acc: 0.8626, Test Acc: 0.7821, Test f1: 0.8031, AUC: class-0>>0.9855208333333334|class-1>>0.9711925586561155\n",
      "Epoch: 025, Train Loss: 0.0510, Test Loss: 0.0584, Train Acc: 0.8452, Test Acc: 0.7823, Test f1: 0.7988, AUC: class-0>>0.9867708333333334|class-1>>0.9714702207413577\n",
      "Epoch: 026, Train Loss: 0.0539, Test Loss: 0.0579, Train Acc: 0.8584, Test Acc: 0.7506, Test f1: 0.7699, AUC: class-0>>0.9859374999999999|class-1>>0.9713313896987367\n",
      "Epoch: 027, Train Loss: 0.0567, Test Loss: 0.0601, Train Acc: 0.8631, Test Acc: 0.8442, Test f1: 0.8689, AUC: class-0>>0.984375|class-1>>0.9727197001249479\n",
      "Saved model!\n",
      "Best thresholds ===>>> class-0>>0.6132485866546631|class-1>>0.6576449871063232\n",
      "Epoch: 028, Train Loss: 0.0536, Test Loss: 0.0577, Train Acc: 0.8679, Test Acc: 0.8079, Test f1: 0.8253, AUC: class-0>>0.9861458333333333|class-1>>0.970637234485631\n",
      "Epoch: 029, Train Loss: 0.0549, Test Loss: 0.0588, Train Acc: 0.8806, Test Acc: 0.8317, Test f1: 0.8554, AUC: class-0>>0.9863541666666668|class-1>>0.9710537276134944\n",
      "Epoch: 030, Train Loss: 0.0507, Test Loss: 0.0623, Train Acc: 0.8731, Test Acc: 0.8468, Test f1: 0.8587, AUC: class-0>>0.9882291666666667|class-1>>0.9723032069970846\n",
      "Epoch: 031, Train Loss: 0.0476, Test Loss: 0.0634, Train Acc: 0.8704, Test Acc: 0.8058, Test f1: 0.8305, AUC: class-0>>0.9869791666666667|class-1>>0.9713313896987367\n",
      "Epoch: 032, Train Loss: 0.0495, Test Loss: 0.0625, Train Acc: 0.8730, Test Acc: 0.8018, Test f1: 0.8215, AUC: class-0>>0.9856250000000001|class-1>>0.9686935998889352\n",
      "Epoch: 033, Train Loss: 0.0535, Test Loss: 0.0746, Train Acc: 0.9022, Test Acc: 0.8060, Test f1: 0.8287, AUC: class-0>>0.984375|class-1>>0.9688324309315562\n",
      "Epoch: 034, Train Loss: 0.0551, Test Loss: 0.0710, Train Acc: 0.8696, Test Acc: 0.7766, Test f1: 0.7943, AUC: class-0>>0.9860416666666667|class-1>>0.969526586144662\n",
      "Epoch: 035, Train Loss: 0.0473, Test Loss: 0.0616, Train Acc: 0.9013, Test Acc: 0.8247, Test f1: 0.8471, AUC: class-0>>0.9848958333333333|class-1>>0.9711925586561155\n",
      "Epoch: 036, Train Loss: 0.0522, Test Loss: 0.0711, Train Acc: 0.9072, Test Acc: 0.8186, Test f1: 0.8444, AUC: class-0>>0.9862500000000001|class-1>>0.972858531167569\n",
      "Epoch: 037, Train Loss: 0.0441, Test Loss: 0.0599, Train Acc: 0.9095, Test Acc: 0.8346, Test f1: 0.8532, AUC: class-0>>0.9852083333333332|class-1>>0.9707760655282521\n",
      "Epoch: 038, Train Loss: 0.0451, Test Loss: 0.0626, Train Acc: 0.8763, Test Acc: 0.8439, Test f1: 0.8613, AUC: class-0>>0.9851041666666667|class-1>>0.972719700124948\n",
      "Epoch: 039, Train Loss: 0.0497, Test Loss: 0.0623, Train Acc: 0.9036, Test Acc: 0.7640, Test f1: 0.7835, AUC: class-0>>0.9869791666666667|class-1>>0.9748021657642649\n",
      "Epoch: 040, Train Loss: 0.0514, Test Loss: 0.0691, Train Acc: 0.9160, Test Acc: 0.8219, Test f1: 0.8452, AUC: class-0>>0.9866666666666667|class-1>>0.9700819103151465\n",
      "Epoch: 041, Train Loss: 0.0475, Test Loss: 0.0636, Train Acc: 0.9068, Test Acc: 0.8289, Test f1: 0.8534, AUC: class-0>>0.9875|class-1>>0.972858531167569\n",
      "Epoch: 042, Train Loss: 0.0484, Test Loss: 0.0706, Train Acc: 0.9136, Test Acc: 0.7998, Test f1: 0.8223, AUC: class-0>>0.9884375000000001|class-1>>0.969665417187283\n",
      "Epoch: 043, Train Loss: 0.0408, Test Loss: 0.0565, Train Acc: 0.9149, Test Acc: 0.8317, Test f1: 0.8561, AUC: class-0>>0.9858333333333333|class-1>>0.9731361932528113\n",
      "Epoch: 044, Train Loss: 0.0504, Test Loss: 0.0757, Train Acc: 0.8904, Test Acc: 0.8058, Test f1: 0.8299, AUC: class-0>>0.9836458333333333|class-1>>0.9703595724003887\n",
      "Epoch: 045, Train Loss: 0.0467, Test Loss: 0.0583, Train Acc: 0.9138, Test Acc: 0.7926, Test f1: 0.8152, AUC: class-0>>0.9875|class-1>>0.9731361932528112\n",
      "Epoch: 046, Train Loss: 0.0403, Test Loss: 0.0626, Train Acc: 0.8995, Test Acc: 0.8215, Test f1: 0.8417, AUC: class-0>>0.9860416666666667|class-1>>0.9689712619741774\n",
      "Epoch: 047, Train Loss: 0.0476, Test Loss: 0.0802, Train Acc: 0.9286, Test Acc: 0.8022, Test f1: 0.8304, AUC: class-0>>0.9772916666666667|class-1>>0.9666111342496182\n",
      "Epoch: 048, Train Loss: 0.0440, Test Loss: 0.0656, Train Acc: 0.9236, Test Acc: 0.8242, Test f1: 0.8413, AUC: class-0>>0.9881249999999999|class-1>>0.9691100930167985\n",
      "Epoch: 049, Train Loss: 0.0429, Test Loss: 0.0669, Train Acc: 0.9172, Test Acc: 0.7931, Test f1: 0.8187, AUC: class-0>>0.9801041666666666|class-1>>0.9709148965708733\n",
      "Epoch: 050, Train Loss: 0.0386, Test Loss: 0.0598, Train Acc: 0.9206, Test Acc: 0.7894, Test f1: 0.8098, AUC: class-0>>0.9857291666666665|class-1>>0.9721643759544634\n",
      "Epoch: 051, Train Loss: 0.0353, Test Loss: 0.0585, Train Acc: 0.9327, Test Acc: 0.8346, Test f1: 0.8547, AUC: class-0>>0.9864583333333334|class-1>>0.9714702207413578\n",
      "Epoch: 052, Train Loss: 0.0469, Test Loss: 0.0661, Train Acc: 0.9323, Test Acc: 0.8188, Test f1: 0.8431, AUC: class-0>>0.989375|class-1>>0.9721643759544634\n",
      "Epoch: 053, Train Loss: 0.0417, Test Loss: 0.0591, Train Acc: 0.9241, Test Acc: 0.8243, Test f1: 0.8449, AUC: class-0>>0.9859374999999999|class-1>>0.9718867138692211\n",
      "Epoch: 054, Train Loss: 0.0343, Test Loss: 0.0607, Train Acc: 0.9383, Test Acc: 0.7962, Test f1: 0.8226, AUC: class-0>>0.9872916666666667|class-1>>0.9718867138692212\n",
      "Epoch: 055, Train Loss: 0.0456, Test Loss: 0.0618, Train Acc: 0.9244, Test Acc: 0.7867, Test f1: 0.8082, AUC: class-0>>0.9833333333333333|class-1>>0.9742468415937803\n",
      "Epoch: 056, Train Loss: 0.0384, Test Loss: 0.0652, Train Acc: 0.9244, Test Acc: 0.8398, Test f1: 0.8535, AUC: class-0>>0.9845833333333334|class-1>>0.9710537276134944\n",
      "Epoch: 057, Train Loss: 0.0357, Test Loss: 0.0671, Train Acc: 0.9371, Test Acc: 0.8219, Test f1: 0.8454, AUC: class-0>>0.9847916666666667|class-1>>0.9686935998889351\n",
      "Epoch: 058, Train Loss: 0.0356, Test Loss: 0.0622, Train Acc: 0.9321, Test Acc: 0.7931, Test f1: 0.8197, AUC: class-0>>0.9880208333333333|class-1>>0.9704984034430099\n",
      "Epoch: 059, Train Loss: 0.0369, Test Loss: 0.0704, Train Acc: 0.9394, Test Acc: 0.7800, Test f1: 0.7980, AUC: class-0>>0.9864583333333334|class-1>>0.968415937803693\n",
      "Epoch: 060, Train Loss: 0.0424, Test Loss: 0.0796, Train Acc: 0.9224, Test Acc: 0.7963, Test f1: 0.8164, AUC: class-0>>0.983125|class-1>>0.9691100930167985\n",
      "Epoch: 061, Train Loss: 0.0367, Test Loss: 0.0612, Train Acc: 0.9300, Test Acc: 0.8637, Test f1: 0.8815, AUC: class-0>>0.9863541666666666|class-1>>0.9732750242954323\n",
      "Saved model!\n",
      "Best thresholds ===>>> class-0>>0.5890512466430664|class-1>>0.623188316822052\n",
      "Epoch: 062, Train Loss: 0.0294, Test Loss: 0.0577, Train Acc: 0.9417, Test Acc: 0.8702, Test f1: 0.8871, AUC: class-0>>0.985625|class-1>>0.9724420380397057\n",
      "Saved model!\n",
      "Best thresholds ===>>> class-0>>0.6414774060249329|class-1>>0.6045223474502563\n",
      "Epoch: 063, Train Loss: 0.0317, Test Loss: 0.0634, Train Acc: 0.9273, Test Acc: 0.7870, Test f1: 0.8056, AUC: class-0>>0.9860416666666667|class-1>>0.9700819103151465\n",
      "Epoch: 064, Train Loss: 0.0320, Test Loss: 0.0681, Train Acc: 0.9388, Test Acc: 0.7446, Test f1: 0.7622, AUC: class-0>>0.9805208333333333|class-1>>0.9716090517839788\n",
      "Epoch: 065, Train Loss: 0.0316, Test Loss: 0.0640, Train Acc: 0.9373, Test Acc: 0.8276, Test f1: 0.8448, AUC: class-0>>0.9870833333333333|class-1>>0.9718867138692212\n",
      "Epoch: 066, Train Loss: 0.0481, Test Loss: 0.0870, Train Acc: 0.9386, Test Acc: 0.8220, Test f1: 0.8403, AUC: class-0>>0.985625|class-1>>0.9641121754824379\n",
      "Epoch: 067, Train Loss: 0.0276, Test Loss: 0.0616, Train Acc: 0.9444, Test Acc: 0.7966, Test f1: 0.8243, AUC: class-0>>0.9857291666666667|class-1>>0.9718867138692211\n",
      "Epoch: 068, Train Loss: 0.0309, Test Loss: 0.0636, Train Acc: 0.9473, Test Acc: 0.7992, Test f1: 0.8163, AUC: class-0>>0.9867708333333334|class-1>>0.9684159378036928\n",
      "Epoch: 069, Train Loss: 0.0282, Test Loss: 0.0653, Train Acc: 0.9390, Test Acc: 0.8062, Test f1: 0.8322, AUC: class-0>>0.9834375|class-1>>0.9716090517839789\n",
      "Epoch: 070, Train Loss: 0.0322, Test Loss: 0.0587, Train Acc: 0.9419, Test Acc: 0.8118, Test f1: 0.8321, AUC: class-0>>0.9852083333333332|class-1>>0.9703595724003888\n",
      "Epoch: 071, Train Loss: 0.0241, Test Loss: 0.0585, Train Acc: 0.9555, Test Acc: 0.7999, Test f1: 0.8243, AUC: class-0>>0.984375|class-1>>0.9718867138692212\n",
      "Epoch: 072, Train Loss: 0.0226, Test Loss: 0.0574, Train Acc: 0.9608, Test Acc: 0.8219, Test f1: 0.8371, AUC: class-0>>0.9865625|class-1>>0.9698042482299042\n",
      "Epoch: 073, Train Loss: 0.0235, Test Loss: 0.0590, Train Acc: 0.9534, Test Acc: 0.7771, Test f1: 0.7977, AUC: class-0>>0.9871875|class-1>>0.9732750242954324\n",
      "Epoch: 074, Train Loss: 0.0354, Test Loss: 0.0736, Train Acc: 0.9359, Test Acc: 0.7904, Test f1: 0.8101, AUC: class-0>>0.9870833333333333|class-1>>0.9692489240594196\n",
      "Epoch: 075, Train Loss: 0.0265, Test Loss: 0.0674, Train Acc: 0.9555, Test Acc: 0.7738, Test f1: 0.7968, AUC: class-0>>0.9857291666666667|class-1>>0.969665417187283\n",
      "Epoch: 076, Train Loss: 0.0359, Test Loss: 0.0800, Train Acc: 0.9490, Test Acc: 0.7772, Test f1: 0.8011, AUC: class-0>>0.9793750000000001|class-1>>0.9707760655282521\n",
      "Epoch: 077, Train Loss: 0.0207, Test Loss: 0.0568, Train Acc: 0.9605, Test Acc: 0.7772, Test f1: 0.8014, AUC: class-0>>0.9866666666666667|class-1>>0.9714702207413577\n",
      "Epoch: 078, Train Loss: 0.0238, Test Loss: 0.0605, Train Acc: 0.9642, Test Acc: 0.8026, Test f1: 0.8335, AUC: class-0>>0.9847916666666666|class-1>>0.9671664584201027\n",
      "Epoch: 079, Train Loss: 0.0215, Test Loss: 0.0622, Train Acc: 0.9649, Test Acc: 0.7900, Test f1: 0.8175, AUC: class-0>>0.9864583333333333|class-1>>0.9691100930167985\n",
      "Epoch: 080, Train Loss: 0.0183, Test Loss: 0.0595, Train Acc: 0.9692, Test Acc: 0.7806, Test f1: 0.8016, AUC: class-0>>0.9841666666666666|class-1>>0.9718867138692212\n",
      "Epoch: 081, Train Loss: 0.0188, Test Loss: 0.0617, Train Acc: 0.9719, Test Acc: 0.8031, Test f1: 0.8222, AUC: class-0>>0.9870833333333333|class-1>>0.9699430792725253\n",
      "Epoch: 082, Train Loss: 0.0181, Test Loss: 0.0607, Train Acc: 0.9766, Test Acc: 0.8062, Test f1: 0.8235, AUC: class-0>>0.9832291666666668|class-1>>0.9703595724003887\n",
      "Epoch: 083, Train Loss: 0.0175, Test Loss: 0.0581, Train Acc: 0.9722, Test Acc: 0.7736, Test f1: 0.7939, AUC: class-0>>0.986875|class-1>>0.970637234485631\n",
      "Epoch: 084, Train Loss: 0.0165, Test Loss: 0.0605, Train Acc: 0.9769, Test Acc: 0.8127, Test f1: 0.8304, AUC: class-0>>0.9872916666666667|class-1>>0.9673052894627239\n",
      "Epoch: 085, Train Loss: 0.0165, Test Loss: 0.0570, Train Acc: 0.9763, Test Acc: 0.7899, Test f1: 0.8139, AUC: class-0>>0.9865625|class-1>>0.9714702207413577\n",
      "Epoch: 086, Train Loss: 0.0219, Test Loss: 0.0616, Train Acc: 0.9780, Test Acc: 0.7800, Test f1: 0.8064, AUC: class-0>>0.9864583333333334|class-1>>0.9623073719283631\n",
      "Epoch: 087, Train Loss: 0.0163, Test Loss: 0.0630, Train Acc: 0.9757, Test Acc: 0.7838, Test f1: 0.8081, AUC: class-0>>0.9855208333333334|class-1>>0.9632791892267111\n",
      "Epoch: 088, Train Loss: 0.0147, Test Loss: 0.0598, Train Acc: 0.9772, Test Acc: 0.7867, Test f1: 0.8069, AUC: class-0>>0.9865624999999999|class-1>>0.9675829515479661\n",
      "Epoch: 089, Train Loss: 0.0159, Test Loss: 0.0583, Train Acc: 0.9736, Test Acc: 0.7967, Test f1: 0.8188, AUC: class-0>>0.9869791666666666|class-1>>0.9668887963348605\n",
      "Epoch: 090, Train Loss: 0.0149, Test Loss: 0.0593, Train Acc: 0.9725, Test Acc: 0.7963, Test f1: 0.8167, AUC: class-0>>0.9865625|class-1>>0.9699430792725253\n",
      "Epoch: 091, Train Loss: 0.0165, Test Loss: 0.0658, Train Acc: 0.9763, Test Acc: 0.7900, Test f1: 0.8080, AUC: class-0>>0.9850000000000001|class-1>>0.9670276273774816\n",
      "Epoch: 092, Train Loss: 0.0141, Test Loss: 0.0582, Train Acc: 0.9792, Test Acc: 0.8094, Test f1: 0.8306, AUC: class-0>>0.9877083333333334|class-1>>0.9683465222823823\n",
      "Epoch: 093, Train Loss: 0.0121, Test Loss: 0.0575, Train Acc: 0.9815, Test Acc: 0.7967, Test f1: 0.8194, AUC: class-0>>0.9888541666666667|class-1>>0.9706372344856309\n",
      "Epoch: 094, Train Loss: 0.0155, Test Loss: 0.0656, Train Acc: 0.9815, Test Acc: 0.8475, Test f1: 0.8699, AUC: class-0>>0.9857291666666667|class-1>>0.968554768846314\n",
      "Epoch: 095, Train Loss: 0.0133, Test Loss: 0.0652, Train Acc: 0.9772, Test Acc: 0.7934, Test f1: 0.8177, AUC: class-0>>0.9841666666666666|class-1>>0.9685547688463141\n",
      "Epoch: 096, Train Loss: 0.0127, Test Loss: 0.0623, Train Acc: 0.9836, Test Acc: 0.7862, Test f1: 0.8111, AUC: class-0>>0.9845833333333334|class-1>>0.9666111342496181\n",
      "Epoch: 097, Train Loss: 0.0104, Test Loss: 0.0627, Train Acc: 0.9812, Test Acc: 0.7864, Test f1: 0.8103, AUC: class-0>>0.9877083333333334|class-1>>0.9681382757184506\n",
      "Epoch: 098, Train Loss: 0.0111, Test Loss: 0.0634, Train Acc: 0.9859, Test Acc: 0.7930, Test f1: 0.8162, AUC: class-0>>0.9846874999999999|class-1>>0.9678606136332084\n",
      "Epoch: 099, Train Loss: 0.0107, Test Loss: 0.0623, Train Acc: 0.9839, Test Acc: 0.8161, Test f1: 0.8403, AUC: class-0>>0.9877083333333333|class-1>>0.968554768846314\n",
      "Epoch: 100, Train Loss: 0.0093, Test Loss: 0.0612, Train Acc: 0.9812, Test Acc: 0.7895, Test f1: 0.8122, AUC: class-0>>0.9863541666666666|class-1>>0.967582951547966\n",
      "Epoch: 101, Train Loss: 0.0092, Test Loss: 0.0625, Train Acc: 0.9836, Test Acc: 0.7576, Test f1: 0.7793, AUC: class-0>>0.9865625|class-1>>0.9663334721643759\n",
      "Epoch: 102, Train Loss: 0.0085, Test Loss: 0.0631, Train Acc: 0.9836, Test Acc: 0.7864, Test f1: 0.8091, AUC: class-0>>0.9861458333333334|class-1>>0.9655004859086491\n",
      "Epoch: 103, Train Loss: 0.0087, Test Loss: 0.0637, Train Acc: 0.9859, Test Acc: 0.7964, Test f1: 0.8208, AUC: class-0>>0.9853125|class-1>>0.9659169790365126\n",
      "Epoch: 104, Train Loss: 0.0084, Test Loss: 0.0633, Train Acc: 0.9883, Test Acc: 0.7964, Test f1: 0.8214, AUC: class-0>>0.986875|class-1>>0.9603637373316674\n",
      "Epoch: 105, Train Loss: 0.0078, Test Loss: 0.0607, Train Acc: 0.9883, Test Acc: 0.7864, Test f1: 0.8103, AUC: class-0>>0.9865625|class-1>>0.9668887963348605\n",
      "Epoch: 106, Train Loss: 0.0094, Test Loss: 0.0665, Train Acc: 0.9836, Test Acc: 0.7860, Test f1: 0.8085, AUC: class-0>>0.9865625000000001|class-1>>0.9663334721643759\n",
      "Epoch: 107, Train Loss: 0.0071, Test Loss: 0.0623, Train Acc: 0.9883, Test Acc: 0.8060, Test f1: 0.8294, AUC: class-0>>0.9854166666666666|class-1>>0.9642510065250589\n",
      "Epoch: 108, Train Loss: 0.0072, Test Loss: 0.0637, Train Acc: 0.9883, Test Acc: 0.7899, Test f1: 0.8142, AUC: class-0>>0.9855208333333334|class-1>>0.9671664584201027\n",
      "Epoch: 109, Train Loss: 0.0065, Test Loss: 0.0627, Train Acc: 0.9930, Test Acc: 0.7668, Test f1: 0.7896, AUC: class-0>>0.985625|class-1>>0.9627238650562265\n",
      "Epoch: 110, Train Loss: 0.0067, Test Loss: 0.0654, Train Acc: 0.9930, Test Acc: 0.7734, Test f1: 0.7957, AUC: class-0>>0.9838541666666667|class-1>>0.9611967235873942\n",
      "Epoch: 111, Train Loss: 0.0064, Test Loss: 0.0647, Train Acc: 0.9930, Test Acc: 0.7699, Test f1: 0.7919, AUC: class-0>>0.9855208333333334|class-1>>0.9607802304595308\n",
      "Epoch: 112, Train Loss: 0.0056, Test Loss: 0.0619, Train Acc: 0.9930, Test Acc: 0.7834, Test f1: 0.8076, AUC: class-0>>0.9861458333333333|class-1>>0.9657781479938915\n",
      "Epoch: 113, Train Loss: 0.0058, Test Loss: 0.0639, Train Acc: 0.9953, Test Acc: 0.8058, Test f1: 0.8310, AUC: class-0>>0.9854166666666666|class-1>>0.9582812716923504\n",
      "Epoch: 114, Train Loss: 0.0050, Test Loss: 0.0651, Train Acc: 0.9930, Test Acc: 0.7864, Test f1: 0.8097, AUC: class-0>>0.9865625|class-1>>0.9635568513119532\n",
      "Epoch: 115, Train Loss: 0.0071, Test Loss: 0.0629, Train Acc: 0.9953, Test Acc: 0.8249, Test f1: 0.8509, AUC: class-0>>0.9861458333333333|class-1>>0.9528668610301263\n",
      "Epoch: 116, Train Loss: 0.0048, Test Loss: 0.0651, Train Acc: 0.9953, Test Acc: 0.7707, Test f1: 0.7934, AUC: class-0>>0.9863541666666668|class-1>>0.9627238650562265\n",
      "Epoch: 117, Train Loss: 0.0044, Test Loss: 0.0651, Train Acc: 0.9977, Test Acc: 0.7664, Test f1: 0.7875, AUC: class-0>>0.9856250000000001|class-1>>0.9591142579480773\n",
      "Epoch: 118, Train Loss: 0.0049, Test Loss: 0.0666, Train Acc: 0.9977, Test Acc: 0.8126, Test f1: 0.8356, AUC: class-0>>0.9851041666666668|class-1>>0.962168540885742\n",
      "Epoch: 119, Train Loss: 0.0045, Test Loss: 0.0669, Train Acc: 0.9977, Test Acc: 0.7862, Test f1: 0.8106, AUC: class-0>>0.9842708333333334|class-1>>0.9532833541579897\n",
      "Epoch: 120, Train Loss: 0.0039, Test Loss: 0.0660, Train Acc: 0.9977, Test Acc: 0.7699, Test f1: 0.7919, AUC: class-0>>0.9853125|class-1>>0.9552269887546855\n",
      "Epoch: 121, Train Loss: 0.0040, Test Loss: 0.0657, Train Acc: 0.9977, Test Acc: 0.7960, Test f1: 0.8185, AUC: class-0>>0.9845833333333334|class-1>>0.9613355546300153\n",
      "Epoch: 122, Train Loss: 0.0042, Test Loss: 0.0667, Train Acc: 0.9977, Test Acc: 0.7704, Test f1: 0.7958, AUC: class-0>>0.9836458333333333|class-1>>0.9623073719283631\n",
      "Epoch: 123, Train Loss: 0.0039, Test Loss: 0.0653, Train Acc: 1.0000, Test Acc: 0.7892, Test f1: 0.8129, AUC: class-0>>0.9839583333333334|class-1>>0.953422185200611\n",
      "Epoch: 124, Train Loss: 0.0035, Test Loss: 0.0661, Train Acc: 1.0000, Test Acc: 0.8023, Test f1: 0.8264, AUC: class-0>>0.9846875|class-1>>0.953561016243232\n",
      "Epoch: 125, Train Loss: 0.0036, Test Loss: 0.0644, Train Acc: 1.0000, Test Acc: 0.7958, Test f1: 0.8197, AUC: class-0>>0.9848958333333333|class-1>>0.9539775093710953\n",
      "Epoch: 126, Train Loss: 0.0031, Test Loss: 0.0666, Train Acc: 1.0000, Test Acc: 0.7956, Test f1: 0.8163, AUC: class-0>>0.9827083333333334|class-1>>0.9543940024989587\n",
      "Epoch: 127, Train Loss: 0.0031, Test Loss: 0.0660, Train Acc: 1.0000, Test Acc: 0.7891, Test f1: 0.8100, AUC: class-0>>0.9836458333333333|class-1>>0.9570317923087602\n",
      "Epoch: 128, Train Loss: 0.0035, Test Loss: 0.0659, Train Acc: 1.0000, Test Acc: 0.7923, Test f1: 0.8156, AUC: class-0>>0.9839583333333334|class-1>>0.9509232264334305\n",
      "Epoch: 129, Train Loss: 0.0035, Test Loss: 0.0693, Train Acc: 1.0000, Test Acc: 0.7834, Test f1: 0.8059, AUC: class-0>>0.9834375000000001|class-1>>0.9532833541579896\n",
      "Epoch: 130, Train Loss: 0.0030, Test Loss: 0.0649, Train Acc: 1.0000, Test Acc: 0.7895, Test f1: 0.8116, AUC: class-0>>0.985625|class-1>>0.9550881577120643\n",
      "Epoch: 131, Train Loss: 0.0029, Test Loss: 0.0671, Train Acc: 1.0000, Test Acc: 0.7860, Test f1: 0.8078, AUC: class-0>>0.9840625000000001|class-1>>0.9582812716923503\n",
      "Epoch: 132, Train Loss: 0.0025, Test Loss: 0.0664, Train Acc: 1.0000, Test Acc: 0.7760, Test f1: 0.7967, AUC: class-0>>0.9829166666666667|class-1>>0.9549493266694432\n",
      "Epoch: 133, Train Loss: 0.0027, Test Loss: 0.0682, Train Acc: 1.0000, Test Acc: 0.7860, Test f1: 0.8076, AUC: class-0>>0.9830208333333333|class-1>>0.9561988060530334\n",
      "Epoch: 134, Train Loss: 0.0024, Test Loss: 0.0665, Train Acc: 1.0000, Test Acc: 0.7764, Test f1: 0.7986, AUC: class-0>>0.9845833333333334|class-1>>0.9527280299875052\n",
      "Epoch: 135, Train Loss: 0.0024, Test Loss: 0.0670, Train Acc: 1.0000, Test Acc: 0.7791, Test f1: 0.7993, AUC: class-0>>0.9828125000000001|class-1>>0.953699847285853\n",
      "Epoch: 136, Train Loss: 0.0030, Test Loss: 0.0653, Train Acc: 1.0000, Test Acc: 0.7923, Test f1: 0.8162, AUC: class-0>>0.9850000000000001|class-1>>0.9492572539219769\n",
      "Epoch: 137, Train Loss: 0.0023, Test Loss: 0.0668, Train Acc: 1.0000, Test Acc: 0.7704, Test f1: 0.7958, AUC: class-0>>0.9832291666666667|class-1>>0.9570317923087602\n",
      "Epoch: 138, Train Loss: 0.0022, Test Loss: 0.0670, Train Acc: 1.0000, Test Acc: 0.7639, Test f1: 0.7886, AUC: class-0>>0.9842708333333334|class-1>>0.9532833541579896\n",
      "Epoch: 139, Train Loss: 0.0022, Test Loss: 0.0663, Train Acc: 1.0000, Test Acc: 0.7831, Test f1: 0.8081, AUC: class-0>>0.9832291666666668|class-1>>0.9520338747743995\n",
      "Epoch: 140, Train Loss: 0.0021, Test Loss: 0.0676, Train Acc: 1.0000, Test Acc: 0.7725, Test f1: 0.7930, AUC: class-0>>0.9836458333333334|class-1>>0.9531445231153686\n",
      "Epoch: 141, Train Loss: 0.0019, Test Loss: 0.0671, Train Acc: 1.0000, Test Acc: 0.7703, Test f1: 0.7922, AUC: class-0>>0.9835416666666668|class-1>>0.9559211439677912\n",
      "Epoch: 142, Train Loss: 0.0019, Test Loss: 0.0669, Train Acc: 1.0000, Test Acc: 0.7923, Test f1: 0.8156, AUC: class-0>>0.9841666666666666|class-1>>0.9523115368596418\n",
      "Epoch: 143, Train Loss: 0.0019, Test Loss: 0.0667, Train Acc: 1.0000, Test Acc: 0.7923, Test f1: 0.8151, AUC: class-0>>0.9840625000000001|class-1>>0.9532833541579898\n",
      "Epoch: 144, Train Loss: 0.0019, Test Loss: 0.0675, Train Acc: 1.0000, Test Acc: 0.7795, Test f1: 0.8006, AUC: class-0>>0.9833333333333333|class-1>>0.953213938636679\n",
      "Epoch: 145, Train Loss: 0.0019, Test Loss: 0.0674, Train Acc: 1.0000, Test Acc: 0.7831, Test f1: 0.8081, AUC: class-0>>0.9827083333333334|class-1>>0.9524503679022629\n",
      "Epoch: 146, Train Loss: 0.0018, Test Loss: 0.0677, Train Acc: 1.0000, Test Acc: 0.7762, Test f1: 0.8001, AUC: class-0>>0.9826041666666667|class-1>>0.9528668610301264\n",
      "Epoch: 147, Train Loss: 0.0018, Test Loss: 0.0671, Train Acc: 1.0000, Test Acc: 0.8019, Test f1: 0.8237, AUC: class-0>>0.9836458333333333|class-1>>0.9518950437317784\n",
      "Epoch: 148, Train Loss: 0.0018, Test Loss: 0.0676, Train Acc: 1.0000, Test Acc: 0.7831, Test f1: 0.8081, AUC: class-0>>0.9830208333333335|class-1>>0.949396084964598\n",
      "Epoch: 149, Train Loss: 0.0016, Test Loss: 0.0678, Train Acc: 1.0000, Test Acc: 0.7703, Test f1: 0.7922, AUC: class-0>>0.9829166666666668|class-1>>0.9520338747743996\n",
      "Running cross-validation fold: 01\n",
      "Epoch: 001, Train Loss: 0.1211, Test Loss: 0.1238, Train Acc: 0.7273, Test Acc: 0.6819, Test f1: 0.7295, AUC: class-0>>0.9561229807191246|class-1>>0.932444913576989\n",
      "Saved model!\n",
      "Best thresholds ===>>> class-0>>0.6564301252365112|class-1>>0.7247353196144104\n",
      "Epoch: 002, Train Loss: 0.1153, Test Loss: 0.1152, Train Acc: 0.7875, Test Acc: 0.7755, Test f1: 0.8189, AUC: class-0>>0.9501823866597185|class-1>>0.9402295817390157\n",
      "Saved model!\n",
      "Best thresholds ===>>> class-0>>0.6910207271575928|class-1>>0.6873065829277039\n",
      "Epoch: 003, Train Loss: 0.1010, Test Loss: 0.1060, Train Acc: 0.8033, Test Acc: 0.7695, Test f1: 0.8194, AUC: class-0>>0.9634184471078686|class-1>>0.9381184852882966\n",
      "Saved model!\n",
      "Best thresholds ===>>> class-0>>0.6592221260070801|class-1>>0.6514084935188293\n",
      "Epoch: 004, Train Loss: 0.0852, Test Loss: 0.0830, Train Acc: 0.7827, Test Acc: 0.7487, Test f1: 0.8002, AUC: class-0>>0.9694632621156851|class-1>>0.9440559440559441\n",
      "Epoch: 005, Train Loss: 0.1026, Test Loss: 0.1084, Train Acc: 0.7965, Test Acc: 0.7665, Test f1: 0.8190, AUC: class-0>>0.9683168316831683|class-1>>0.9433962264150944\n",
      "Saved model!\n",
      "Best thresholds ===>>> class-0>>0.5687853693962097|class-1>>0.6516903638839722\n",
      "Epoch: 006, Train Loss: 0.0823, Test Loss: 0.0851, Train Acc: 0.7988, Test Acc: 0.7375, Test f1: 0.7875, AUC: class-0>>0.9696717040125066|class-1>>0.9468267581475128\n",
      "Epoch: 007, Train Loss: 0.0939, Test Loss: 0.0992, Train Acc: 0.8027, Test Acc: 0.7768, Test f1: 0.8174, AUC: class-0>>0.9742574257425742|class-1>>0.9472225887320227\n",
      "Saved model!\n",
      "Best thresholds ===>>> class-0>>0.6236265897750854|class-1>>0.6645412445068359\n",
      "Epoch: 008, Train Loss: 0.1007, Test Loss: 0.1228, Train Acc: 0.8384, Test Acc: 0.7986, Test f1: 0.8358, AUC: class-0>>0.9689421573736321|class-1>>0.8212165193297268\n",
      "Epoch: 009, Train Loss: 0.0727, Test Loss: 0.0792, Train Acc: 0.8182, Test Acc: 0.7593, Test f1: 0.8045, AUC: class-0>>0.9726941115164148|class-1>>0.9444517746404538\n",
      "Epoch: 010, Train Loss: 0.0698, Test Loss: 0.0806, Train Acc: 0.8168, Test Acc: 0.7672, Test f1: 0.8111, AUC: class-0>>0.9735278791036999|class-1>>0.9445837181686239\n",
      "Epoch: 011, Train Loss: 0.0816, Test Loss: 0.0850, Train Acc: 0.8168, Test Acc: 0.7533, Test f1: 0.8037, AUC: class-0>>0.9725898905680042|class-1>>0.9439240005277741\n",
      "Epoch: 012, Train Loss: 0.0688, Test Loss: 0.0809, Train Acc: 0.8342, Test Acc: 0.7563, Test f1: 0.8036, AUC: class-0>>0.9716519020323084|class-1>>0.9433962264150944\n",
      "Epoch: 013, Train Loss: 0.0600, Test Loss: 0.0705, Train Acc: 0.8312, Test Acc: 0.7593, Test f1: 0.8040, AUC: class-0>>0.9755080771235018|class-1>>0.9453753793376435\n",
      "Epoch: 014, Train Loss: 0.0610, Test Loss: 0.0736, Train Acc: 0.8672, Test Acc: 0.7583, Test f1: 0.8097, AUC: class-0>>0.9745700885878061|class-1>>0.9481461934292124\n",
      "Epoch: 015, Train Loss: 0.0731, Test Loss: 0.0929, Train Acc: 0.8397, Test Acc: 0.7890, Test f1: 0.8374, AUC: class-0>>0.9761334028139657|class-1>>0.9269032853938515\n",
      "Saved model!\n",
      "Best thresholds ===>>> class-0>>0.5944150686264038|class-1>>0.7184827923774719\n",
      "Epoch: 016, Train Loss: 0.0587, Test Loss: 0.0748, Train Acc: 0.8305, Test Acc: 0.7599, Test f1: 0.8115, AUC: class-0>>0.9745700885878061|class-1>>0.944187887584114\n",
      "Epoch: 017, Train Loss: 0.0663, Test Loss: 0.0814, Train Acc: 0.8399, Test Acc: 0.7586, Test f1: 0.8140, AUC: class-0>>0.9773840541948933|class-1>>0.9353476711967278\n",
      "Epoch: 018, Train Loss: 0.0563, Test Loss: 0.0672, Train Acc: 0.8314, Test Acc: 0.7487, Test f1: 0.8009, AUC: class-0>>0.9770713913496614|class-1>>0.946562871091173\n",
      "Epoch: 019, Train Loss: 0.0618, Test Loss: 0.0720, Train Acc: 0.8436, Test Acc: 0.7679, Test f1: 0.8192, AUC: class-0>>0.9782178217821782|class-1>>0.9439240005277741\n",
      "Epoch: 020, Train Loss: 0.0580, Test Loss: 0.0707, Train Acc: 0.8468, Test Acc: 0.7665, Test f1: 0.8205, AUC: class-0>>0.9770713913496614|class-1>>0.9433962264150944\n",
      "Epoch: 021, Train Loss: 0.0560, Test Loss: 0.0696, Train Acc: 0.8406, Test Acc: 0.7741, Test f1: 0.8220, AUC: class-0>>0.977696717040125|class-1>>0.9294102124290804\n",
      "Epoch: 022, Train Loss: 0.0543, Test Loss: 0.0689, Train Acc: 0.8490, Test Acc: 0.7520, Test f1: 0.8061, AUC: class-0>>0.977696717040125|class-1>>0.9439240005277741\n",
      "Epoch: 023, Train Loss: 0.0592, Test Loss: 0.0766, Train Acc: 0.8446, Test Acc: 0.7837, Test f1: 0.8289, AUC: class-0>>0.9794684731631058|class-1>>0.944187887584114\n",
      "Saved model!\n",
      "Best thresholds ===>>> class-0>>0.5971395373344421|class-1>>0.692634105682373\n",
      "Epoch: 024, Train Loss: 0.0584, Test Loss: 0.0761, Train Acc: 0.8441, Test Acc: 0.7556, Test f1: 0.8133, AUC: class-0>>0.9793642522146951|class-1>>0.9316532524079694\n",
      "Epoch: 025, Train Loss: 0.0505, Test Loss: 0.0681, Train Acc: 0.8531, Test Acc: 0.7599, Test f1: 0.8115, AUC: class-0>>0.97686294945284|class-1>>0.9432642828869243\n",
      "Epoch: 026, Train Loss: 0.0507, Test Loss: 0.0696, Train Acc: 0.8643, Test Acc: 0.7616, Test f1: 0.8154, AUC: class-0>>0.9783220427305888|class-1>>0.941812904077055\n",
      "Epoch: 027, Train Loss: 0.0543, Test Loss: 0.0671, Train Acc: 0.8661, Test Acc: 0.7709, Test f1: 0.8187, AUC: class-0>>0.974882751433038|class-1>>0.9501253463517615\n",
      "Epoch: 028, Train Loss: 0.0502, Test Loss: 0.0655, Train Acc: 0.8514, Test Acc: 0.7626, Test f1: 0.8097, AUC: class-0>>0.9801980198019802|class-1>>0.9461670405066632\n",
      "Epoch: 029, Train Loss: 0.0500, Test Loss: 0.0699, Train Acc: 0.8541, Test Acc: 0.8161, Test f1: 0.8557, AUC: class-0>>0.9784262636789994|class-1>>0.9294102124290803\n",
      "Saved model!\n",
      "Best thresholds ===>>> class-0>>0.6338538527488708|class-1>>0.6702488660812378\n",
      "Epoch: 030, Train Loss: 0.0533, Test Loss: 0.0722, Train Acc: 0.8697, Test Acc: 0.7580, Test f1: 0.8073, AUC: class-0>>0.9800937988535695|class-1>>0.9398337511545058\n",
      "Epoch: 031, Train Loss: 0.0464, Test Loss: 0.0683, Train Acc: 0.8672, Test Acc: 0.7411, Test f1: 0.7962, AUC: class-0>>0.9775924960917145|class-1>>0.9435281699432643\n",
      "Epoch: 032, Train Loss: 0.0509, Test Loss: 0.0693, Train Acc: 0.8619, Test Acc: 0.7900, Test f1: 0.8363, AUC: class-0>>0.9818655549765503|class-1>>0.9381184852882966\n",
      "Epoch: 033, Train Loss: 0.0455, Test Loss: 0.0660, Train Acc: 0.8757, Test Acc: 0.7553, Test f1: 0.8097, AUC: class-0>>0.9779051589369463|class-1>>0.9439240005277741\n",
      "Epoch: 034, Train Loss: 0.0430, Test Loss: 0.0674, Train Acc: 0.8867, Test Acc: 0.7566, Test f1: 0.8083, AUC: class-0>>0.9787389265242314|class-1>>0.941549017020715\n",
      "Epoch: 035, Train Loss: 0.0439, Test Loss: 0.0680, Train Acc: 0.8854, Test Acc: 0.7520, Test f1: 0.8054, AUC: class-0>>0.978009379885357|class-1>>0.9428684523024145\n",
      "Epoch: 036, Train Loss: 0.0457, Test Loss: 0.0715, Train Acc: 0.8956, Test Acc: 0.7632, Test f1: 0.8166, AUC: class-0>>0.9792600312662845|class-1>>0.9427365087742446\n",
      "Epoch: 037, Train Loss: 0.0409, Test Loss: 0.0689, Train Acc: 0.8786, Test Acc: 0.7629, Test f1: 0.8135, AUC: class-0>>0.9791558103178739|class-1>>0.9453753793376435\n",
      "Epoch: 038, Train Loss: 0.0424, Test Loss: 0.0689, Train Acc: 0.8878, Test Acc: 0.7745, Test f1: 0.8258, AUC: class-0>>0.9782178217821782|class-1>>0.9410212429080353\n",
      "Epoch: 039, Train Loss: 0.0527, Test Loss: 0.0766, Train Acc: 0.8828, Test Acc: 0.8214, Test f1: 0.8466, AUC: class-0>>0.9760291818655549|class-1>>0.9436601134714342\n",
      "Saved model!\n",
      "Best thresholds ===>>> class-0>>0.6412835717201233|class-1>>0.6057248115539551\n",
      "Epoch: 040, Train Loss: 0.0401, Test Loss: 0.0682, Train Acc: 0.9013, Test Acc: 0.7870, Test f1: 0.8351, AUC: class-0>>0.9727983324648254|class-1>>0.9470906452038527\n",
      "Epoch: 041, Train Loss: 0.0413, Test Loss: 0.0704, Train Acc: 0.9033, Test Acc: 0.7821, Test f1: 0.8303, AUC: class-0>>0.9775924960917145|class-1>>0.9501253463517614\n",
      "Epoch: 042, Train Loss: 0.0439, Test Loss: 0.0742, Train Acc: 0.9031, Test Acc: 0.7613, Test f1: 0.8104, AUC: class-0>>0.9729025534132361|class-1>>0.9433962264150944\n",
      "Epoch: 043, Train Loss: 0.0378, Test Loss: 0.0706, Train Acc: 0.8943, Test Acc: 0.7589, Test f1: 0.8172, AUC: class-0>>0.9719645648775403|class-1>>0.9382504288164666\n",
      "Epoch: 044, Train Loss: 0.0439, Test Loss: 0.0732, Train Acc: 0.9045, Test Acc: 0.7755, Test f1: 0.8212, AUC: class-0>>0.9779051589369464|class-1>>0.951312838105291\n",
      "Epoch: 045, Train Loss: 0.0478, Test Loss: 0.0850, Train Acc: 0.9130, Test Acc: 0.8122, Test f1: 0.8512, AUC: class-0>>0.9736321000521104|class-1>>0.9427365087742446\n",
      "Epoch: 046, Train Loss: 0.0418, Test Loss: 0.0710, Train Acc: 0.8929, Test Acc: 0.7788, Test f1: 0.8251, AUC: class-0>>0.9766545075560188|class-1>>0.9422087346615649\n",
      "Epoch: 047, Train Loss: 0.0377, Test Loss: 0.0708, Train Acc: 0.9145, Test Acc: 0.7785, Test f1: 0.8219, AUC: class-0>>0.9757165190203231|class-1>>0.9459031534503233\n",
      "Epoch: 048, Train Loss: 0.0350, Test Loss: 0.0674, Train Acc: 0.9153, Test Acc: 0.7878, Test f1: 0.8279, AUC: class-0>>0.9769671704012507|class-1>>0.9383823723446366\n",
      "Epoch: 049, Train Loss: 0.0368, Test Loss: 0.0714, Train Acc: 0.9120, Test Acc: 0.8039, Test f1: 0.8440, AUC: class-0>>0.9729025534132361|class-1>>0.9424726217179047\n",
      "Epoch: 050, Train Loss: 0.0346, Test Loss: 0.0656, Train Acc: 0.9100, Test Acc: 0.7705, Test f1: 0.8158, AUC: class-0>>0.9783220427305889|class-1>>0.9457712099221532\n",
      "Epoch: 051, Train Loss: 0.0367, Test Loss: 0.0683, Train Acc: 0.9086, Test Acc: 0.7914, Test f1: 0.8331, AUC: class-0>>0.9775924960917144|class-1>>0.9466948146193429\n",
      "Epoch: 052, Train Loss: 0.0405, Test Loss: 0.0759, Train Acc: 0.9336, Test Acc: 0.7804, Test f1: 0.8272, AUC: class-0>>0.9779051589369463|class-1>>0.9473545322601926\n",
      "Epoch: 053, Train Loss: 0.0327, Test Loss: 0.0691, Train Acc: 0.9229, Test Acc: 0.7656, Test f1: 0.8099, AUC: class-0>>0.976029181865555|class-1>>0.9390420899854862\n",
      "Epoch: 054, Train Loss: 0.0335, Test Loss: 0.0713, Train Acc: 0.9276, Test Acc: 0.7867, Test f1: 0.8312, AUC: class-0>>0.9767587285044294|class-1>>0.9480142499010423\n",
      "Epoch: 055, Train Loss: 0.0326, Test Loss: 0.0718, Train Acc: 0.9364, Test Acc: 0.7950, Test f1: 0.8397, AUC: class-0>>0.9724856696195936|class-1>>0.9366671064784272\n",
      "Epoch: 056, Train Loss: 0.0311, Test Loss: 0.0732, Train Acc: 0.9397, Test Acc: 0.7947, Test f1: 0.8400, AUC: class-0>>0.9750911933298593|class-1>>0.9484100804855522\n",
      "Epoch: 057, Train Loss: 0.0882, Test Loss: 0.1283, Train Acc: 0.9206, Test Acc: 0.7870, Test f1: 0.8347, AUC: class-0>>0.9574778530484627|class-1>>0.9377226547037868\n",
      "Epoch: 058, Train Loss: 0.0302, Test Loss: 0.0711, Train Acc: 0.9329, Test Acc: 0.7738, Test f1: 0.8199, AUC: class-0>>0.9769671704012507|class-1>>0.951048951048951\n",
      "Epoch: 059, Train Loss: 0.0303, Test Loss: 0.0681, Train Acc: 0.9427, Test Acc: 0.7963, Test f1: 0.8382, AUC: class-0>>0.9742574257425742|class-1>>0.9515767251616308\n",
      "Epoch: 060, Train Loss: 0.0301, Test Loss: 0.0702, Train Acc: 0.9339, Test Acc: 0.7834, Test f1: 0.8282, AUC: class-0>>0.9739447628973423|class-1>>0.9440559440559441\n",
      "Epoch: 061, Train Loss: 0.0291, Test Loss: 0.0699, Train Acc: 0.9251, Test Acc: 0.7785, Test f1: 0.8224, AUC: class-0>>0.9754038561750911|class-1>>0.9309935347671197\n",
      "Epoch: 062, Train Loss: 0.0548, Test Loss: 0.0849, Train Acc: 0.9505, Test Acc: 0.8155, Test f1: 0.8543, AUC: class-0>>0.9765502866076081|class-1>>0.9459031534503233\n",
      "Saved model!\n",
      "Best thresholds ===>>> class-0>>0.6933543086051941|class-1>>0.6967373490333557\n",
      "Epoch: 063, Train Loss: 0.0254, Test Loss: 0.0672, Train Acc: 0.9541, Test Acc: 0.7834, Test f1: 0.8286, AUC: class-0>>0.9779051589369463|class-1>>0.9269032853938514\n",
      "Epoch: 064, Train Loss: 0.0264, Test Loss: 0.0703, Train Acc: 0.9507, Test Acc: 0.8092, Test f1: 0.8477, AUC: class-0>>0.9781136008337676|class-1>>0.9406254123235256\n",
      "Epoch: 065, Train Loss: 0.0374, Test Loss: 0.0761, Train Acc: 0.9420, Test Acc: 0.8237, Test f1: 0.8582, AUC: class-0>>0.9784262636789994|class-1>>0.890486871618947\n",
      "Epoch: 066, Train Loss: 0.0444, Test Loss: 0.0822, Train Acc: 0.9495, Test Acc: 0.7920, Test f1: 0.8383, AUC: class-0>>0.9717561229807191|class-1>>0.9080353608655495\n",
      "Epoch: 067, Train Loss: 0.0237, Test Loss: 0.0692, Train Acc: 0.9574, Test Acc: 0.8059, Test f1: 0.8450, AUC: class-0>>0.9745700885878061|class-1>>0.9324449135769891\n",
      "Epoch: 068, Train Loss: 0.0252, Test Loss: 0.0717, Train Acc: 0.9563, Test Acc: 0.8346, Test f1: 0.8567, AUC: class-0>>0.9729025534132361|class-1>>0.9199102784008444\n",
      "Epoch: 069, Train Loss: 0.0253, Test Loss: 0.0698, Train Acc: 0.9364, Test Acc: 0.7815, Test f1: 0.8219, AUC: class-0>>0.9752996352266805|class-1>>0.9407573558516955\n",
      "Epoch: 070, Train Loss: 0.0230, Test Loss: 0.0691, Train Acc: 0.9587, Test Acc: 0.7947, Test f1: 0.8376, AUC: class-0>>0.9767587285044295|class-1>>0.9342921229713682\n",
      "Epoch: 071, Train Loss: 0.0232, Test Loss: 0.0740, Train Acc: 0.9608, Test Acc: 0.7867, Test f1: 0.8324, AUC: class-0>>0.9765502866076081|class-1>>0.9240005277741127\n",
      "Epoch: 072, Train Loss: 0.0217, Test Loss: 0.0739, Train Acc: 0.9609, Test Acc: 0.7900, Test f1: 0.8348, AUC: class-0>>0.9770713913496614|class-1>>0.9142367066895368\n",
      "Epoch: 073, Train Loss: 0.0204, Test Loss: 0.0719, Train Acc: 0.9540, Test Acc: 0.8475, Test f1: 0.8671, AUC: class-0>>0.9781136008337675|class-1>>0.9150283678585566\n",
      "Saved model!\n",
      "Best thresholds ===>>> class-0>>0.6174978613853455|class-1>>0.6110073328018188\n",
      "Epoch: 074, Train Loss: 0.0201, Test Loss: 0.0693, Train Acc: 0.9697, Test Acc: 0.8135, Test f1: 0.8469, AUC: class-0>>0.97519541427827|class-1>>0.9210977701543739\n",
      "Epoch: 075, Train Loss: 0.0247, Test Loss: 0.0785, Train Acc: 0.9696, Test Acc: 0.8363, Test f1: 0.8577, AUC: class-0>>0.9764460656591976|class-1>>0.8830980340414303\n",
      "Epoch: 076, Train Loss: 0.0236, Test Loss: 0.0842, Train Acc: 0.9740, Test Acc: 0.7930, Test f1: 0.8348, AUC: class-0>>0.9726941115164148|class-1>>0.9180630690064653\n",
      "Epoch: 077, Train Loss: 0.0172, Test Loss: 0.0733, Train Acc: 0.9762, Test Acc: 0.7725, Test f1: 0.8217, AUC: class-0>>0.9759249609171443|class-1>>0.920833883098034\n",
      "Epoch: 078, Train Loss: 0.0200, Test Loss: 0.0757, Train Acc: 0.9697, Test Acc: 0.7818, Test f1: 0.8246, AUC: class-0>>0.9756122980719124|class-1>>0.90697981264019\n",
      "Epoch: 079, Train Loss: 0.0200, Test Loss: 0.0727, Train Acc: 0.9606, Test Acc: 0.7735, Test f1: 0.8160, AUC: class-0>>0.9749869723814487|class-1>>0.9193825042881646\n",
      "Epoch: 080, Train Loss: 0.0192, Test Loss: 0.0714, Train Acc: 0.9718, Test Acc: 0.8059, Test f1: 0.8425, AUC: class-0>>0.9764460656591976|class-1>>0.9110700620134582\n",
      "Epoch: 081, Train Loss: 0.0184, Test Loss: 0.0755, Train Acc: 0.9716, Test Acc: 0.8234, Test f1: 0.8576, AUC: class-0>>0.9704012506513808|class-1>>0.9094867396754189\n",
      "Epoch: 082, Train Loss: 0.0159, Test Loss: 0.0714, Train Acc: 0.9782, Test Acc: 0.7533, Test f1: 0.8044, AUC: class-0>>0.9745700885878061|class-1>>0.9196463913445045\n",
      "Epoch: 083, Train Loss: 0.0177, Test Loss: 0.0723, Train Acc: 0.9740, Test Acc: 0.7801, Test f1: 0.8233, AUC: class-0>>0.9727983324648255|class-1>>0.9303338171262701\n",
      "Epoch: 084, Train Loss: 0.0158, Test Loss: 0.0755, Train Acc: 0.9760, Test Acc: 0.8231, Test f1: 0.8530, AUC: class-0>>0.9747785304846274|class-1>>0.9316532524079694\n",
      "Epoch: 085, Train Loss: 0.0152, Test Loss: 0.0764, Train Acc: 0.9760, Test Acc: 0.7626, Test f1: 0.8089, AUC: class-0>>0.9730067743616467|class-1>>0.9188547301754849\n",
      "Epoch: 086, Train Loss: 0.0134, Test Loss: 0.0747, Train Acc: 0.9804, Test Acc: 0.7785, Test f1: 0.8219, AUC: class-0>>0.971235018238666|class-1>>0.9188547301754849\n",
      "Epoch: 087, Train Loss: 0.0161, Test Loss: 0.0724, Train Acc: 0.9692, Test Acc: 0.8310, Test f1: 0.8583, AUC: class-0>>0.9758207399687338|class-1>>0.9241324713022826\n",
      "Epoch: 088, Train Loss: 0.0126, Test Loss: 0.0731, Train Acc: 0.9782, Test Acc: 0.8244, Test f1: 0.8494, AUC: class-0>>0.9752996352266805|class-1>>0.9279588336192109\n",
      "Epoch: 089, Train Loss: 0.0110, Test Loss: 0.0733, Train Acc: 0.9804, Test Acc: 0.7815, Test f1: 0.8219, AUC: class-0>>0.9747785304846274|class-1>>0.9255838501121519\n",
      "Epoch: 090, Train Loss: 0.0175, Test Loss: 0.0782, Train Acc: 0.9782, Test Acc: 0.7815, Test f1: 0.8219, AUC: class-0>>0.9730067743616466|class-1>>0.9283546642037208\n",
      "Epoch: 091, Train Loss: 0.0102, Test Loss: 0.0763, Train Acc: 0.9826, Test Acc: 0.7831, Test f1: 0.8233, AUC: class-0>>0.9710265763418446|class-1>>0.9242644148304525\n",
      "Epoch: 092, Train Loss: 0.0118, Test Loss: 0.0816, Train Acc: 0.9826, Test Acc: 0.7894, Test f1: 0.8278, AUC: class-0>>0.9683168316831683|class-1>>0.9302018735981001\n",
      "Epoch: 093, Train Loss: 0.0104, Test Loss: 0.0751, Train Acc: 0.9801, Test Acc: 0.7897, Test f1: 0.8323, AUC: class-0>>0.9752996352266805|class-1>>0.925715793640322\n",
      "Epoch: 094, Train Loss: 0.0117, Test Loss: 0.0809, Train Acc: 0.9801, Test Acc: 0.8327, Test f1: 0.8588, AUC: class-0>>0.9742574257425743|class-1>>0.9121256102388179\n",
      "Epoch: 095, Train Loss: 0.0136, Test Loss: 0.0804, Train Acc: 0.9848, Test Acc: 0.7768, Test f1: 0.8203, AUC: class-0>>0.968942157373632|class-1>>0.9305977041826099\n",
      "Epoch: 096, Train Loss: 0.0104, Test Loss: 0.0753, Train Acc: 0.9892, Test Acc: 0.7914, Test f1: 0.8355, AUC: class-0>>0.971235018238666|class-1>>0.9321810265206492\n",
      "Epoch: 097, Train Loss: 0.0094, Test Loss: 0.0752, Train Acc: 0.9823, Test Acc: 0.8072, Test f1: 0.8422, AUC: class-0>>0.9729025534132361|class-1>>0.9237366407177727\n",
      "Epoch: 098, Train Loss: 0.0085, Test Loss: 0.0772, Train Acc: 0.9823, Test Acc: 0.7864, Test f1: 0.8278, AUC: class-0>>0.9745700885878061|class-1>>0.9345560100277082\n",
      "Epoch: 099, Train Loss: 0.0088, Test Loss: 0.0774, Train Acc: 0.9845, Test Acc: 0.7738, Test f1: 0.8189, AUC: class-0>>0.9719645648775403|class-1>>0.9259796806966618\n",
      "Epoch: 100, Train Loss: 0.0081, Test Loss: 0.0782, Train Acc: 0.9870, Test Acc: 0.8009, Test f1: 0.8376, AUC: class-0>>0.9743616466909849|class-1>>0.9207019395698641\n",
      "Epoch: 101, Train Loss: 0.0074, Test Loss: 0.0773, Train Acc: 0.9845, Test Acc: 0.8056, Test f1: 0.8440, AUC: class-0>>0.9741532047941637|class-1>>0.9323129700488191\n",
      "Epoch: 102, Train Loss: 0.0126, Test Loss: 0.0775, Train Acc: 0.9826, Test Acc: 0.7702, Test f1: 0.8111, AUC: class-0>>0.9741532047941636|class-1>>0.921097770154374\n",
      "Epoch: 103, Train Loss: 0.0077, Test Loss: 0.0771, Train Acc: 0.9869, Test Acc: 0.7848, Test f1: 0.8246, AUC: class-0>>0.9738405419489318|class-1>>0.9253199630558122\n",
      "Epoch: 104, Train Loss: 0.0065, Test Loss: 0.0754, Train Acc: 0.9823, Test Acc: 0.7768, Test f1: 0.8195, AUC: class-0>>0.9738405419489318|class-1>>0.9303338171262701\n",
      "Epoch: 105, Train Loss: 0.0108, Test Loss: 0.0830, Train Acc: 0.9934, Test Acc: 0.8313, Test f1: 0.8533, AUC: class-0>>0.9730067743616466|class-1>>0.8941812904077054\n",
      "Epoch: 106, Train Loss: 0.0062, Test Loss: 0.0765, Train Acc: 0.9912, Test Acc: 0.7943, Test f1: 0.8337, AUC: class-0>>0.9746743095362168|class-1>>0.9208338830980343\n",
      "Epoch: 107, Train Loss: 0.0062, Test Loss: 0.0764, Train Acc: 0.9934, Test Acc: 0.7884, Test f1: 0.8315, AUC: class-0>>0.973215216258468|class-1>>0.9287504947882306\n",
      "Epoch: 108, Train Loss: 0.0072, Test Loss: 0.0818, Train Acc: 0.9934, Test Acc: 0.8039, Test f1: 0.8370, AUC: class-0>>0.9723814486711828|class-1>>0.9195144478163346\n",
      "Epoch: 109, Train Loss: 0.0049, Test Loss: 0.0758, Train Acc: 0.9934, Test Acc: 0.8053, Test f1: 0.8394, AUC: class-0>>0.9743616466909848|class-1>>0.9315213088797996\n",
      "Epoch: 110, Train Loss: 0.0054, Test Loss: 0.0764, Train Acc: 0.9934, Test Acc: 0.8102, Test f1: 0.8422, AUC: class-0>>0.9731109953100574|class-1>>0.9331046312178388\n",
      "Epoch: 111, Train Loss: 0.0048, Test Loss: 0.0780, Train Acc: 0.9934, Test Acc: 0.7976, Test f1: 0.8377, AUC: class-0>>0.9747785304846275|class-1>>0.9229449795487532\n",
      "Epoch: 112, Train Loss: 0.0045, Test Loss: 0.0765, Train Acc: 0.9934, Test Acc: 0.8102, Test f1: 0.8451, AUC: class-0>>0.9750911933298593|class-1>>0.9284866077318907\n",
      "Epoch: 113, Train Loss: 0.0050, Test Loss: 0.0810, Train Acc: 0.9934, Test Acc: 0.7864, Test f1: 0.8230, AUC: class-0>>0.9733194372068785|class-1>>0.9251880195276422\n",
      "Epoch: 114, Train Loss: 0.0048, Test Loss: 0.0803, Train Acc: 0.9934, Test Acc: 0.8069, Test f1: 0.8405, AUC: class-0>>0.9726941115164147|class-1>>0.9319171394643093\n",
      "Epoch: 115, Train Loss: 0.0046, Test Loss: 0.0802, Train Acc: 0.9934, Test Acc: 0.8036, Test f1: 0.8357, AUC: class-0>>0.9734236581552892|class-1>>0.9245283018867925\n",
      "Epoch: 116, Train Loss: 0.0046, Test Loss: 0.0794, Train Acc: 0.9934, Test Acc: 0.7735, Test f1: 0.8158, AUC: class-0>>0.9725898905680042|class-1>>0.925583850112152\n",
      "Epoch: 117, Train Loss: 0.0041, Test Loss: 0.0770, Train Acc: 0.9934, Test Acc: 0.8135, Test f1: 0.8491, AUC: class-0>>0.9743616466909849|class-1>>0.9323129700488191\n",
      "Epoch: 118, Train Loss: 0.0035, Test Loss: 0.0798, Train Acc: 0.9934, Test Acc: 0.8119, Test f1: 0.8461, AUC: class-0>>0.9735278791036998|class-1>>0.9298060430135902\n",
      "Epoch: 119, Train Loss: 0.0032, Test Loss: 0.0779, Train Acc: 0.9934, Test Acc: 0.8039, Test f1: 0.8399, AUC: class-0>>0.9735278791036999|class-1>>0.9338962923868583\n",
      "Epoch: 120, Train Loss: 0.0032, Test Loss: 0.0776, Train Acc: 0.9956, Test Acc: 0.7864, Test f1: 0.8271, AUC: class-0>>0.9741532047941637|class-1>>0.932840744161499\n",
      "Epoch: 121, Train Loss: 0.0040, Test Loss: 0.0769, Train Acc: 0.9956, Test Acc: 0.7877, Test f1: 0.8244, AUC: class-0>>0.9738405419489318|class-1>>0.9320490829924792\n",
      "Epoch: 122, Train Loss: 0.0032, Test Loss: 0.0766, Train Acc: 0.9956, Test Acc: 0.7957, Test f1: 0.8314, AUC: class-0>>0.9734236581552892|class-1>>0.9344240664995382\n",
      "Epoch: 123, Train Loss: 0.0032, Test Loss: 0.0776, Train Acc: 0.9956, Test Acc: 0.8149, Test f1: 0.8461, AUC: class-0>>0.9745700885878061|class-1>>0.9336324053305186\n",
      "Epoch: 124, Train Loss: 0.0034, Test Loss: 0.0784, Train Acc: 0.9956, Test Acc: 0.7745, Test f1: 0.8258, AUC: class-0>>0.9746743095362169|class-1>>0.932840744161499\n",
      "Epoch: 125, Train Loss: 0.0027, Test Loss: 0.0782, Train Acc: 0.9956, Test Acc: 0.7976, Test f1: 0.8353, AUC: class-0>>0.973215216258468|class-1>>0.9309935347671197\n",
      "Epoch: 126, Train Loss: 0.0028, Test Loss: 0.0787, Train Acc: 0.9956, Test Acc: 0.8023, Test f1: 0.8368, AUC: class-0>>0.9734236581552893|class-1>>0.9332365747460087\n",
      "Epoch: 127, Train Loss: 0.0027, Test Loss: 0.0787, Train Acc: 0.9956, Test Acc: 0.7881, Test f1: 0.8284, AUC: class-0>>0.9733194372068786|class-1>>0.9323129700488192\n",
      "Epoch: 128, Train Loss: 0.0026, Test Loss: 0.0784, Train Acc: 0.9956, Test Acc: 0.7943, Test f1: 0.8337, AUC: class-0>>0.9743616466909848|class-1>>0.932708800633329\n",
      "Epoch: 129, Train Loss: 0.0026, Test Loss: 0.0795, Train Acc: 0.9956, Test Acc: 0.7910, Test f1: 0.8290, AUC: class-0>>0.973215216258468|class-1>>0.9243963583586225\n",
      "Epoch: 130, Train Loss: 0.0026, Test Loss: 0.0769, Train Acc: 0.9956, Test Acc: 0.8119, Test f1: 0.8461, AUC: class-0>>0.9746743095362168|class-1>>0.9332365747460087\n",
      "Epoch: 131, Train Loss: 0.0026, Test Loss: 0.0782, Train Acc: 0.9956, Test Acc: 0.7930, Test f1: 0.8342, AUC: class-0>>0.974048983845753|class-1>>0.9319171394643093\n",
      "Epoch: 132, Train Loss: 0.0023, Test Loss: 0.0783, Train Acc: 0.9956, Test Acc: 0.7990, Test f1: 0.8348, AUC: class-0>>0.9736321000521104|class-1>>0.9316532524079694\n",
      "Epoch: 133, Train Loss: 0.0030, Test Loss: 0.0787, Train Acc: 0.9956, Test Acc: 0.7864, Test f1: 0.8271, AUC: class-0>>0.9727983324648254|class-1>>0.93046576065444\n",
      "Epoch: 134, Train Loss: 0.0023, Test Loss: 0.0766, Train Acc: 0.9956, Test Acc: 0.7943, Test f1: 0.8330, AUC: class-0>>0.9739447628973423|class-1>>0.9337643488586885\n",
      "Epoch: 135, Train Loss: 0.0022, Test Loss: 0.0784, Train Acc: 0.9956, Test Acc: 0.7815, Test f1: 0.8199, AUC: class-0>>0.9731109953100574|class-1>>0.9332365747460089\n",
      "Epoch: 136, Train Loss: 0.0021, Test Loss: 0.0787, Train Acc: 0.9956, Test Acc: 0.7960, Test f1: 0.8342, AUC: class-0>>0.974048983845753|class-1>>0.9309935347671198\n",
      "Epoch: 137, Train Loss: 0.0021, Test Loss: 0.0789, Train Acc: 0.9956, Test Acc: 0.7976, Test f1: 0.8353, AUC: class-0>>0.9727983324648255|class-1>>0.9299379865417602\n",
      "Epoch: 138, Train Loss: 0.0023, Test Loss: 0.0777, Train Acc: 0.9956, Test Acc: 0.7976, Test f1: 0.8353, AUC: class-0>>0.9733194372068786|class-1>>0.9336324053305187\n",
      "Epoch: 139, Train Loss: 0.0021, Test Loss: 0.0780, Train Acc: 0.9956, Test Acc: 0.7976, Test f1: 0.8353, AUC: class-0>>0.9730067743616466|class-1>>0.9337643488586884\n",
      "Epoch: 140, Train Loss: 0.0021, Test Loss: 0.0788, Train Acc: 0.9956, Test Acc: 0.7976, Test f1: 0.8353, AUC: class-0>>0.973215216258468|class-1>>0.9300699300699301\n",
      "Epoch: 141, Train Loss: 0.0021, Test Loss: 0.0787, Train Acc: 0.9956, Test Acc: 0.7914, Test f1: 0.8307, AUC: class-0>>0.9735278791036999|class-1>>0.9294102124290804\n",
      "Epoch: 142, Train Loss: 0.0020, Test Loss: 0.0777, Train Acc: 0.9956, Test Acc: 0.7864, Test f1: 0.8271, AUC: class-0>>0.9726941115164147|class-1>>0.9336324053305187\n",
      "Epoch: 143, Train Loss: 0.0020, Test Loss: 0.0774, Train Acc: 0.9956, Test Acc: 0.7897, Test f1: 0.8296, AUC: class-0>>0.9733194372068785|class-1>>0.9337643488586886\n",
      "Epoch: 144, Train Loss: 0.0019, Test Loss: 0.0779, Train Acc: 0.9956, Test Acc: 0.7976, Test f1: 0.8353, AUC: class-0>>0.9734236581552893|class-1>>0.9316532524079696\n",
      "Epoch: 145, Train Loss: 0.0019, Test Loss: 0.0788, Train Acc: 0.9956, Test Acc: 0.8009, Test f1: 0.8371, AUC: class-0>>0.9724856696195936|class-1>>0.9298060430135903\n",
      "Epoch: 146, Train Loss: 0.0019, Test Loss: 0.0782, Train Acc: 0.9956, Test Acc: 0.7752, Test f1: 0.8166, AUC: class-0>>0.9723814486711829|class-1>>0.9294102124290804\n",
      "Epoch: 147, Train Loss: 0.0019, Test Loss: 0.0780, Train Acc: 0.9956, Test Acc: 0.7897, Test f1: 0.8296, AUC: class-0>>0.9727983324648255|class-1>>0.9309935347671197\n",
      "Epoch: 148, Train Loss: 0.0019, Test Loss: 0.0774, Train Acc: 0.9956, Test Acc: 0.7910, Test f1: 0.8284, AUC: class-0>>0.9727983324648255|class-1>>0.9321810265206493\n",
      "Epoch: 149, Train Loss: 0.0021, Test Loss: 0.0798, Train Acc: 0.9956, Test Acc: 0.8023, Test f1: 0.8362, AUC: class-0>>0.9725898905680042|class-1>>0.9315213088797994\n",
      "Running cross-validation fold: 02\n",
      "Epoch: 001, Train Loss: 0.1434, Test Loss: 0.1524, Train Acc: 0.7846, Test Acc: 0.7690, Test f1: 0.7942, AUC: class-0>>0.9353051839464883|class-1>>0.9432397959183675\n",
      "Saved model!\n",
      "Best thresholds ===>>> class-0>>0.5742737054824829|class-1>>0.6109966039657593\n",
      "Epoch: 002, Train Loss: 0.0804, Test Loss: 0.0923, Train Acc: 0.8090, Test Acc: 0.7723, Test f1: 0.7963, AUC: class-0>>0.9555811036789297|class-1>>0.9468112244897959\n",
      "Saved model!\n",
      "Best thresholds ===>>> class-0>>0.641342282295227|class-1>>0.6690003871917725\n",
      "Epoch: 003, Train Loss: 0.0775, Test Loss: 0.0902, Train Acc: 0.8074, Test Acc: 0.7422, Test f1: 0.7706, AUC: class-0>>0.9568352842809364|class-1>>0.9418367346938775\n",
      "Epoch: 004, Train Loss: 0.0867, Test Loss: 0.0991, Train Acc: 0.7846, Test Acc: 0.7402, Test f1: 0.7749, AUC: class-0>>0.9582984949832776|class-1>>0.9358418367346939\n",
      "Epoch: 005, Train Loss: 0.0718, Test Loss: 0.0845, Train Acc: 0.8313, Test Acc: 0.7207, Test f1: 0.7480, AUC: class-0>>0.9624790969899666|class-1>>0.9401785714285715\n",
      "Epoch: 006, Train Loss: 0.0904, Test Loss: 0.1064, Train Acc: 0.8142, Test Acc: 0.7531, Test f1: 0.7827, AUC: class-0>>0.9600752508361203|class-1>>0.9442602040816327\n",
      "Epoch: 007, Train Loss: 0.0667, Test Loss: 0.0831, Train Acc: 0.8064, Test Acc: 0.7368, Test f1: 0.7680, AUC: class-0>>0.9647784280936454|class-1>>0.9424744897959184\n",
      "Epoch: 008, Train Loss: 0.0727, Test Loss: 0.0866, Train Acc: 0.8232, Test Acc: 0.7554, Test f1: 0.7904, AUC: class-0>>0.9670777591973244|class-1>>0.9366071428571429\n",
      "Epoch: 009, Train Loss: 0.0806, Test Loss: 0.0944, Train Acc: 0.8107, Test Acc: 0.7435, Test f1: 0.7746, AUC: class-0>>0.9643603678929766|class-1>>0.9418367346938775\n",
      "Epoch: 010, Train Loss: 0.0749, Test Loss: 0.0886, Train Acc: 0.8113, Test Acc: 0.7246, Test f1: 0.7509, AUC: class-0>>0.9641513377926422|class-1>>0.9423469387755102\n",
      "Epoch: 011, Train Loss: 0.0745, Test Loss: 0.0888, Train Acc: 0.8017, Test Acc: 0.7601, Test f1: 0.7812, AUC: class-0>>0.9640468227424749|class-1>>0.939795918367347\n",
      "Epoch: 012, Train Loss: 0.0697, Test Loss: 0.0851, Train Acc: 0.8338, Test Acc: 0.7657, Test f1: 0.7933, AUC: class-0>>0.9701086956521738|class-1>>0.9441326530612245\n",
      "Saved model!\n",
      "Best thresholds ===>>> class-0>>0.6304600834846497|class-1>>0.6782678365707397\n",
      "Epoch: 013, Train Loss: 0.0805, Test Loss: 0.1023, Train Acc: 0.8360, Test Acc: 0.7749, Test f1: 0.8072, AUC: class-0>>0.9647784280936456|class-1>>0.9002551020408164\n",
      "Epoch: 014, Train Loss: 0.0866, Test Loss: 0.1057, Train Acc: 0.8294, Test Acc: 0.8216, Test f1: 0.8404, AUC: class-0>>0.9736622073578596|class-1>>0.8869897959183674\n",
      "Saved model!\n",
      "Best thresholds ===>>> class-0>>0.6675784587860107|class-1>>0.7056712508201599\n",
      "Epoch: 015, Train Loss: 0.0671, Test Loss: 0.0852, Train Acc: 0.8402, Test Acc: 0.7541, Test f1: 0.7850, AUC: class-0>>0.9670777591973245|class-1>>0.9327806122448979\n",
      "Epoch: 016, Train Loss: 0.0722, Test Loss: 0.0897, Train Acc: 0.8311, Test Acc: 0.7534, Test f1: 0.7839, AUC: class-0>>0.9706312709030099|class-1>>0.9418367346938775\n",
      "Epoch: 017, Train Loss: 0.0707, Test Loss: 0.0983, Train Acc: 0.8318, Test Acc: 0.7991, Test f1: 0.8216, AUC: class-0>>0.9703177257525084|class-1>>0.8807397959183674\n",
      "Epoch: 018, Train Loss: 0.0646, Test Loss: 0.0882, Train Acc: 0.8334, Test Acc: 0.7825, Test f1: 0.8106, AUC: class-0>>0.9711538461538461|class-1>>0.9363520408163266\n",
      "Epoch: 019, Train Loss: 0.0594, Test Loss: 0.0780, Train Acc: 0.8353, Test Acc: 0.7733, Test f1: 0.7996, AUC: class-0>>0.9712583612040133|class-1>>0.9386479591836735\n",
      "Epoch: 020, Train Loss: 0.0728, Test Loss: 0.0921, Train Acc: 0.8019, Test Acc: 0.7697, Test f1: 0.7957, AUC: class-0>>0.9694816053511706|class-1>>0.9338010204081633\n",
      "Epoch: 021, Train Loss: 0.0589, Test Loss: 0.0763, Train Acc: 0.8363, Test Acc: 0.7554, Test f1: 0.7848, AUC: class-0>>0.9756479933110368|class-1>>0.9392857142857144\n",
      "Epoch: 022, Train Loss: 0.0627, Test Loss: 0.0865, Train Acc: 0.8531, Test Acc: 0.7839, Test f1: 0.8096, AUC: class-0>>0.97251254180602|class-1>>0.9380102040816326\n",
      "Epoch: 023, Train Loss: 0.0629, Test Loss: 0.0862, Train Acc: 0.8453, Test Acc: 0.7917, Test f1: 0.8260, AUC: class-0>>0.9788879598662207|class-1>>0.8844387755102041\n",
      "Epoch: 024, Train Loss: 0.0616, Test Loss: 0.0844, Train Acc: 0.8588, Test Acc: 0.7990, Test f1: 0.8261, AUC: class-0>>0.9774247491638796|class-1>>0.9329081632653061\n",
      "Saved model!\n",
      "Best thresholds ===>>> class-0>>0.6629790663719177|class-1>>0.7218666672706604\n",
      "Epoch: 025, Train Loss: 0.0509, Test Loss: 0.0733, Train Acc: 0.8603, Test Acc: 0.8037, Test f1: 0.8269, AUC: class-0>>0.9760660535117057|class-1>>0.9348214285714286\n",
      "Saved model!\n",
      "Best thresholds ===>>> class-0>>0.6266320943832397|class-1>>0.6953569054603577\n",
      "Epoch: 026, Train Loss: 0.0514, Test Loss: 0.0717, Train Acc: 0.8589, Test Acc: 0.7824, Test f1: 0.8167, AUC: class-0>>0.978992474916388|class-1>>0.938265306122449\n",
      "Epoch: 027, Train Loss: 0.0549, Test Loss: 0.0743, Train Acc: 0.8496, Test Acc: 0.7855, Test f1: 0.8157, AUC: class-0>>0.9741847826086956|class-1>>0.9392857142857143\n",
      "Epoch: 028, Train Loss: 0.0614, Test Loss: 0.0794, Train Acc: 0.8565, Test Acc: 0.7862, Test f1: 0.8112, AUC: class-0>>0.9755434782608696|class-1>>0.9387755102040816\n",
      "Epoch: 029, Train Loss: 0.0706, Test Loss: 0.0913, Train Acc: 0.8559, Test Acc: 0.7947, Test f1: 0.8229, AUC: class-0>>0.976693143812709|class-1>>0.9436224489795918\n",
      "Epoch: 030, Train Loss: 0.0485, Test Loss: 0.0733, Train Acc: 0.8581, Test Acc: 0.7802, Test f1: 0.8057, AUC: class-0>>0.9723035117056856|class-1>>0.932015306122449\n",
      "Epoch: 031, Train Loss: 0.0504, Test Loss: 0.0705, Train Acc: 0.8761, Test Acc: 0.8116, Test f1: 0.8351, AUC: class-0>>0.9780518394648829|class-1>>0.9387755102040816\n",
      "Saved model!\n",
      "Best thresholds ===>>> class-0>>0.6667430400848389|class-1>>0.6702225804328918\n",
      "Epoch: 032, Train Loss: 0.0470, Test Loss: 0.0681, Train Acc: 0.8797, Test Acc: 0.7914, Test f1: 0.8211, AUC: class-0>>0.9790969899665551|class-1>>0.9350765306122449\n",
      "Epoch: 033, Train Loss: 0.0510, Test Loss: 0.0727, Train Acc: 0.8809, Test Acc: 0.8037, Test f1: 0.8275, AUC: class-0>>0.9790969899665551|class-1>>0.9400510204081634\n",
      "Epoch: 034, Train Loss: 0.0572, Test Loss: 0.0739, Train Acc: 0.8636, Test Acc: 0.8080, Test f1: 0.8291, AUC: class-0>>0.9773202341137123|class-1>>0.9404336734693878\n",
      "Epoch: 035, Train Loss: 0.0512, Test Loss: 0.0769, Train Acc: 0.8823, Test Acc: 0.8311, Test f1: 0.8519, AUC: class-0>>0.982232441471572|class-1>>0.9298469387755103\n",
      "Saved model!\n",
      "Best thresholds ===>>> class-0>>0.6160150170326233|class-1>>0.6836667656898499\n",
      "Epoch: 036, Train Loss: 0.0474, Test Loss: 0.0745, Train Acc: 0.8724, Test Acc: 0.8185, Test f1: 0.8435, AUC: class-0>>0.9780518394648829|class-1>>0.9230867346938776\n",
      "Epoch: 037, Train Loss: 0.0430, Test Loss: 0.0670, Train Acc: 0.8821, Test Acc: 0.7963, Test f1: 0.8294, AUC: class-0>>0.9839046822742474|class-1>>0.9383928571428571\n",
      "Epoch: 038, Train Loss: 0.0489, Test Loss: 0.0694, Train Acc: 0.8878, Test Acc: 0.7897, Test f1: 0.8247, AUC: class-0>>0.9825459866220736|class-1>>0.9434948979591837\n",
      "Epoch: 039, Train Loss: 0.0419, Test Loss: 0.0688, Train Acc: 0.8924, Test Acc: 0.7788, Test f1: 0.8116, AUC: class-0>>0.9803511705685619|class-1>>0.9354591836734694\n",
      "Epoch: 040, Train Loss: 0.0416, Test Loss: 0.0716, Train Acc: 0.8890, Test Acc: 0.7930, Test f1: 0.8273, AUC: class-0>>0.9807692307692307|class-1>>0.9354591836734694\n",
      "Epoch: 041, Train Loss: 0.0445, Test Loss: 0.0687, Train Acc: 0.8924, Test Acc: 0.7791, Test f1: 0.8137, AUC: class-0>>0.9823369565217391|class-1>>0.9443877551020409\n",
      "Epoch: 042, Train Loss: 0.0430, Test Loss: 0.0677, Train Acc: 0.8770, Test Acc: 0.8265, Test f1: 0.8469, AUC: class-0>>0.9813963210702341|class-1>>0.9376275510204082\n",
      "Epoch: 043, Train Loss: 0.0447, Test Loss: 0.0722, Train Acc: 0.9048, Test Acc: 0.8222, Test f1: 0.8486, AUC: class-0>>0.9825459866220735|class-1>>0.9382653061224491\n",
      "Epoch: 044, Train Loss: 0.0415, Test Loss: 0.0678, Train Acc: 0.8902, Test Acc: 0.8069, Test f1: 0.8403, AUC: class-0>>0.9819188963210701|class-1>>0.9465561224489796\n",
      "Epoch: 045, Train Loss: 0.0429, Test Loss: 0.0666, Train Acc: 0.8785, Test Acc: 0.8060, Test f1: 0.8325, AUC: class-0>>0.9840091973244147|class-1>>0.9355867346938777\n",
      "Epoch: 046, Train Loss: 0.0470, Test Loss: 0.0821, Train Acc: 0.8921, Test Acc: 0.8089, Test f1: 0.8429, AUC: class-0>>0.9832775919732442|class-1>>0.9248724489795919\n",
      "Epoch: 047, Train Loss: 0.0395, Test Loss: 0.0694, Train Acc: 0.9028, Test Acc: 0.7927, Test f1: 0.8248, AUC: class-0>>0.9812918060200668|class-1>>0.9346938775510204\n",
      "Epoch: 048, Train Loss: 0.0400, Test Loss: 0.0729, Train Acc: 0.9193, Test Acc: 0.8132, Test f1: 0.8452, AUC: class-0>>0.9794105351170569|class-1>>0.939795918367347\n",
      "Epoch: 049, Train Loss: 0.0368, Test Loss: 0.0669, Train Acc: 0.9097, Test Acc: 0.7999, Test f1: 0.8337, AUC: class-0>>0.9824414715719064|class-1>>0.9403061224489796\n",
      "Epoch: 050, Train Loss: 0.0401, Test Loss: 0.0712, Train Acc: 0.9189, Test Acc: 0.8066, Test f1: 0.8375, AUC: class-0>>0.9803511705685618|class-1>>0.9376275510204082\n",
      "Epoch: 051, Train Loss: 0.0370, Test Loss: 0.0686, Train Acc: 0.9120, Test Acc: 0.8033, Test f1: 0.8357, AUC: class-0>>0.9821279264214047|class-1>>0.9389030612244897\n",
      "Epoch: 052, Train Loss: 0.0495, Test Loss: 0.0759, Train Acc: 0.8782, Test Acc: 0.7788, Test f1: 0.8110, AUC: class-0>>0.9775292642140468|class-1>>0.9410714285714287\n",
      "Epoch: 053, Train Loss: 0.0431, Test Loss: 0.0687, Train Acc: 0.8908, Test Acc: 0.8198, Test f1: 0.8482, AUC: class-0>>0.9846362876254181|class-1>>0.944515306122449\n",
      "Saved model!\n",
      "Best thresholds ===>>> class-0>>0.6498712301254272|class-1>>0.6276329159736633\n",
      "Epoch: 054, Train Loss: 0.0445, Test Loss: 0.0830, Train Acc: 0.9212, Test Acc: 0.8345, Test f1: 0.8525, AUC: class-0>>0.9827550167224081|class-1>>0.9098214285714286\n",
      "Epoch: 055, Train Loss: 0.0356, Test Loss: 0.0733, Train Acc: 0.8970, Test Acc: 0.7960, Test f1: 0.8268, AUC: class-0>>0.975752508361204|class-1>>0.9396683673469388\n",
      "Epoch: 056, Train Loss: 0.0562, Test Loss: 0.0798, Train Acc: 0.9062, Test Acc: 0.8232, Test f1: 0.8456, AUC: class-0>>0.9811872909698998|class-1>>0.9433673469387756\n",
      "Epoch: 057, Train Loss: 0.0376, Test Loss: 0.0780, Train Acc: 0.9226, Test Acc: 0.8047, Test f1: 0.8272, AUC: class-0>>0.9692725752508361|class-1>>0.9395408163265306\n",
      "Epoch: 058, Train Loss: 0.0351, Test Loss: 0.0657, Train Acc: 0.9065, Test Acc: 0.8069, Test f1: 0.8403, AUC: class-0>>0.9839046822742475|class-1>>0.9493622448979593\n",
      "Epoch: 059, Train Loss: 0.0401, Test Loss: 0.0728, Train Acc: 0.8981, Test Acc: 0.7990, Test f1: 0.8324, AUC: class-0>>0.9816053511705685|class-1>>0.9456632653061225\n",
      "Epoch: 060, Train Loss: 0.0318, Test Loss: 0.0698, Train Acc: 0.9209, Test Acc: 0.8006, Test f1: 0.8324, AUC: class-0>>0.9817098662207357|class-1>>0.9422193877551022\n",
      "Epoch: 061, Train Loss: 0.0329, Test Loss: 0.0713, Train Acc: 0.9284, Test Acc: 0.8079, Test f1: 0.8415, AUC: class-0>>0.9821279264214047|class-1>>0.9434948979591837\n",
      "Epoch: 062, Train Loss: 0.0346, Test Loss: 0.0665, Train Acc: 0.9140, Test Acc: 0.7871, Test f1: 0.8222, AUC: class-0>>0.9834866220735786|class-1>>0.9502551020408163\n",
      "Epoch: 063, Train Loss: 0.0371, Test Loss: 0.0846, Train Acc: 0.9269, Test Acc: 0.7917, Test f1: 0.8287, AUC: class-0>>0.978992474916388|class-1>>0.9372448979591838\n",
      "Epoch: 064, Train Loss: 0.0302, Test Loss: 0.0706, Train Acc: 0.9239, Test Acc: 0.8056, Test f1: 0.8363, AUC: class-0>>0.9811872909698997|class-1>>0.938265306122449\n",
      "Epoch: 065, Train Loss: 0.0311, Test Loss: 0.0765, Train Acc: 0.9235, Test Acc: 0.8006, Test f1: 0.8332, AUC: class-0>>0.9811872909698998|class-1>>0.9394132653061225\n",
      "Epoch: 066, Train Loss: 0.0291, Test Loss: 0.0691, Train Acc: 0.9260, Test Acc: 0.8228, Test f1: 0.8550, AUC: class-0>>0.9825459866220736|class-1>>0.9399234693877552\n",
      "Saved model!\n",
      "Best thresholds ===>>> class-0>>0.5789088606834412|class-1>>0.6671998500823975\n",
      "Epoch: 067, Train Loss: 0.0299, Test Loss: 0.0722, Train Acc: 0.9355, Test Acc: 0.8400, Test f1: 0.8679, AUC: class-0>>0.9826505016722409|class-1>>0.9321428571428572\n",
      "Saved model!\n",
      "Best thresholds ===>>> class-0>>0.5881302952766418|class-1>>0.6862461566925049\n",
      "Epoch: 068, Train Loss: 0.0270, Test Loss: 0.0685, Train Acc: 0.9258, Test Acc: 0.7917, Test f1: 0.8232, AUC: class-0>>0.9819188963210703|class-1>>0.9432397959183674\n",
      "Epoch: 069, Train Loss: 0.0279, Test Loss: 0.0738, Train Acc: 0.9365, Test Acc: 0.8006, Test f1: 0.8324, AUC: class-0>>0.9815008361204013|class-1>>0.939030612244898\n",
      "Epoch: 070, Train Loss: 0.0255, Test Loss: 0.0681, Train Acc: 0.9281, Test Acc: 0.8092, Test f1: 0.8410, AUC: class-0>>0.9819188963210703|class-1>>0.9450255102040817\n",
      "Epoch: 071, Train Loss: 0.0358, Test Loss: 0.0727, Train Acc: 0.9225, Test Acc: 0.8261, Test f1: 0.8564, AUC: class-0>>0.9848453177257526|class-1>>0.9540816326530613\n",
      "Epoch: 072, Train Loss: 0.0370, Test Loss: 0.0860, Train Acc: 0.9390, Test Acc: 0.8000, Test f1: 0.8274, AUC: class-0>>0.9828595317725752|class-1>>0.9035714285714287\n",
      "Epoch: 073, Train Loss: 0.0236, Test Loss: 0.0690, Train Acc: 0.9411, Test Acc: 0.7990, Test f1: 0.8324, AUC: class-0>>0.981814381270903|class-1>>0.9396683673469388\n",
      "Epoch: 074, Train Loss: 0.0247, Test Loss: 0.0694, Train Acc: 0.9508, Test Acc: 0.8009, Test f1: 0.8350, AUC: class-0>>0.9827550167224081|class-1>>0.9418367346938775\n",
      "Epoch: 075, Train Loss: 0.0521, Test Loss: 0.1114, Train Acc: 0.9261, Test Acc: 0.8105, Test f1: 0.8458, AUC: class-0>>0.9826505016722408|class-1>>0.8404336734693878\n",
      "Epoch: 076, Train Loss: 0.0361, Test Loss: 0.0820, Train Acc: 0.9273, Test Acc: 0.8219, Test f1: 0.8451, AUC: class-0>>0.9796195652173912|class-1>>0.9427295918367348\n",
      "Epoch: 077, Train Loss: 0.0222, Test Loss: 0.0719, Train Acc: 0.9507, Test Acc: 0.7887, Test f1: 0.8228, AUC: class-0>>0.9821279264214047|class-1>>0.9404336734693878\n",
      "Epoch: 078, Train Loss: 0.0246, Test Loss: 0.0776, Train Acc: 0.9524, Test Acc: 0.8311, Test f1: 0.8555, AUC: class-0>>0.9777382943143813|class-1>>0.9378826530612244\n",
      "Epoch: 079, Train Loss: 0.0214, Test Loss: 0.0704, Train Acc: 0.9595, Test Acc: 0.7920, Test f1: 0.8258, AUC: class-0>>0.9819188963210703|class-1>>0.946811224489796\n",
      "Epoch: 080, Train Loss: 0.0296, Test Loss: 0.0756, Train Acc: 0.9549, Test Acc: 0.8298, Test f1: 0.8517, AUC: class-0>>0.9829640468227425|class-1>>0.9415816326530613\n",
      "Epoch: 081, Train Loss: 0.0199, Test Loss: 0.0678, Train Acc: 0.9454, Test Acc: 0.7861, Test f1: 0.8207, AUC: class-0>>0.9834866220735786|class-1>>0.9461734693877552\n",
      "Epoch: 082, Train Loss: 0.0216, Test Loss: 0.0681, Train Acc: 0.9620, Test Acc: 0.8376, Test f1: 0.8676, AUC: class-0>>0.983173076923077|class-1>>0.9456632653061225\n",
      "Saved model!\n",
      "Best thresholds ===>>> class-0>>0.5773779153823853|class-1>>0.6350343227386475\n",
      "Epoch: 083, Train Loss: 0.0202, Test Loss: 0.0783, Train Acc: 0.9708, Test Acc: 0.8043, Test f1: 0.8375, AUC: class-0>>0.983173076923077|class-1>>0.9238520408163265\n",
      "Epoch: 084, Train Loss: 0.0197, Test Loss: 0.0795, Train Acc: 0.9710, Test Acc: 0.7887, Test f1: 0.8232, AUC: class-0>>0.9825459866220736|class-1>>0.9187500000000001\n",
      "Epoch: 085, Train Loss: 0.0178, Test Loss: 0.0665, Train Acc: 0.9662, Test Acc: 0.8271, Test f1: 0.8572, AUC: class-0>>0.9847408026755853|class-1>>0.9478316326530613\n",
      "Epoch: 086, Train Loss: 0.0230, Test Loss: 0.0832, Train Acc: 0.9685, Test Acc: 0.7874, Test f1: 0.8240, AUC: class-0>>0.9834866220735787|class-1>>0.8919642857142858\n",
      "Epoch: 087, Train Loss: 0.0184, Test Loss: 0.0791, Train Acc: 0.9706, Test Acc: 0.8016, Test f1: 0.8344, AUC: class-0>>0.9835911371237458|class-1>>0.9039540816326531\n",
      "Epoch: 088, Train Loss: 0.0188, Test Loss: 0.0813, Train Acc: 0.9777, Test Acc: 0.7911, Test f1: 0.8239, AUC: class-0>>0.9829640468227424|class-1>>0.900765306122449\n",
      "Epoch: 089, Train Loss: 0.0162, Test Loss: 0.0733, Train Acc: 0.9731, Test Acc: 0.7983, Test f1: 0.8320, AUC: class-0>>0.9817098662207359|class-1>>0.934311224489796\n",
      "Epoch: 090, Train Loss: 0.0177, Test Loss: 0.0680, Train Acc: 0.9781, Test Acc: 0.8235, Test f1: 0.8528, AUC: class-0>>0.983173076923077|class-1>>0.9410714285714286\n",
      "Epoch: 091, Train Loss: 0.0153, Test Loss: 0.0739, Train Acc: 0.9775, Test Acc: 0.8155, Test f1: 0.8462, AUC: class-0>>0.9824414715719063|class-1>>0.9390306122448979\n",
      "Epoch: 092, Train Loss: 0.0201, Test Loss: 0.0731, Train Acc: 0.9618, Test Acc: 0.7540, Test f1: 0.7875, AUC: class-0>>0.9843227424749164|class-1>>0.9426020408163266\n",
      "Epoch: 093, Train Loss: 0.0151, Test Loss: 0.0676, Train Acc: 0.9682, Test Acc: 0.8204, Test f1: 0.8545, AUC: class-0>>0.9831730769230769|class-1>>0.9480867346938776\n",
      "Epoch: 094, Train Loss: 0.0147, Test Loss: 0.0766, Train Acc: 0.9800, Test Acc: 0.7917, Test f1: 0.8287, AUC: class-0>>0.9832775919732442|class-1>>0.9188775510204081\n",
      "Epoch: 095, Train Loss: 0.0133, Test Loss: 0.0722, Train Acc: 0.9728, Test Acc: 0.8109, Test f1: 0.8449, AUC: class-0>>0.9832775919732442|class-1>>0.9323979591836735\n",
      "Epoch: 096, Train Loss: 0.0133, Test Loss: 0.0755, Train Acc: 0.9818, Test Acc: 0.7867, Test f1: 0.8214, AUC: class-0>>0.9832775919732442|class-1>>0.9364795918367347\n",
      "Epoch: 097, Train Loss: 0.0124, Test Loss: 0.0725, Train Acc: 0.9866, Test Acc: 0.8201, Test f1: 0.8514, AUC: class-0>>0.9839046822742474|class-1>>0.9286989795918368\n",
      "Epoch: 098, Train Loss: 0.0126, Test Loss: 0.0763, Train Acc: 0.9843, Test Acc: 0.7940, Test f1: 0.8294, AUC: class-0>>0.9829640468227425|class-1>>0.9258928571428572\n",
      "Epoch: 099, Train Loss: 0.0112, Test Loss: 0.0746, Train Acc: 0.9843, Test Acc: 0.8175, Test f1: 0.8476, AUC: class-0>>0.9820234113712374|class-1>>0.9270408163265306\n",
      "Epoch: 100, Train Loss: 0.0127, Test Loss: 0.0743, Train Acc: 0.9843, Test Acc: 0.7950, Test f1: 0.8304, AUC: class-0>>0.9821279264214047|class-1>>0.9389030612244897\n",
      "Epoch: 101, Train Loss: 0.0103, Test Loss: 0.0739, Train Acc: 0.9867, Test Acc: 0.8188, Test f1: 0.8473, AUC: class-0>>0.9838001672240804|class-1>>0.9345663265306122\n",
      "Epoch: 102, Train Loss: 0.0158, Test Loss: 0.0749, Train Acc: 0.9866, Test Acc: 0.7950, Test f1: 0.8304, AUC: class-0>>0.9825459866220736|class-1>>0.9443877551020409\n",
      "Epoch: 103, Train Loss: 0.0146, Test Loss: 0.0700, Train Acc: 0.9751, Test Acc: 0.7861, Test f1: 0.8204, AUC: class-0>>0.9849498327759197|class-1>>0.9432397959183673\n",
      "Epoch: 104, Train Loss: 0.0095, Test Loss: 0.0723, Train Acc: 0.9818, Test Acc: 0.8102, Test f1: 0.8424, AUC: class-0>>0.984113712374582|class-1>>0.9308673469387756\n",
      "Epoch: 105, Train Loss: 0.0089, Test Loss: 0.0758, Train Acc: 0.9864, Test Acc: 0.8172, Test f1: 0.8488, AUC: class-0>>0.9845317725752508|class-1>>0.9266581632653061\n",
      "Epoch: 106, Train Loss: 0.0085, Test Loss: 0.0764, Train Acc: 0.9841, Test Acc: 0.8155, Test f1: 0.8462, AUC: class-0>>0.9810827759197325|class-1>>0.9192602040816326\n",
      "Epoch: 107, Train Loss: 0.0085, Test Loss: 0.0712, Train Acc: 0.9887, Test Acc: 0.8294, Test f1: 0.8577, AUC: class-0>>0.9815008361204014|class-1>>0.9316326530612244\n",
      "Epoch: 108, Train Loss: 0.0088, Test Loss: 0.0731, Train Acc: 0.9885, Test Acc: 0.8066, Test f1: 0.8376, AUC: class-0>>0.982232441471572|class-1>>0.9329081632653062\n",
      "Epoch: 109, Train Loss: 0.0086, Test Loss: 0.0751, Train Acc: 0.9887, Test Acc: 0.8172, Test f1: 0.8482, AUC: class-0>>0.9828595317725752|class-1>>0.9336734693877551\n",
      "Epoch: 110, Train Loss: 0.0083, Test Loss: 0.0714, Train Acc: 0.9885, Test Acc: 0.8102, Test f1: 0.8420, AUC: class-0>>0.9829640468227425|class-1>>0.9336734693877552\n",
      "Epoch: 111, Train Loss: 0.0082, Test Loss: 0.0779, Train Acc: 0.9908, Test Acc: 0.8165, Test f1: 0.8470, AUC: class-0>>0.9828595317725752|class-1>>0.9215561224489797\n",
      "Epoch: 112, Train Loss: 0.0072, Test Loss: 0.0715, Train Acc: 0.9885, Test Acc: 0.8182, Test f1: 0.8493, AUC: class-0>>0.9823369565217392|class-1>>0.9275510204081634\n",
      "Epoch: 113, Train Loss: 0.0069, Test Loss: 0.0731, Train Acc: 0.9908, Test Acc: 0.8066, Test f1: 0.8376, AUC: class-0>>0.9822324414715718|class-1>>0.9281887755102042\n",
      "Epoch: 114, Train Loss: 0.0066, Test Loss: 0.0729, Train Acc: 0.9933, Test Acc: 0.8205, Test f1: 0.8500, AUC: class-0>>0.9823369565217391|class-1>>0.9283163265306124\n",
      "Epoch: 115, Train Loss: 0.0063, Test Loss: 0.0764, Train Acc: 0.9933, Test Acc: 0.8135, Test f1: 0.8439, AUC: class-0>>0.980873745819398|class-1>>0.924234693877551\n",
      "Epoch: 116, Train Loss: 0.0057, Test Loss: 0.0735, Train Acc: 0.9908, Test Acc: 0.8135, Test f1: 0.8439, AUC: class-0>>0.9829640468227424|class-1>>0.93125\n",
      "Epoch: 117, Train Loss: 0.0057, Test Loss: 0.0735, Train Acc: 0.9908, Test Acc: 0.8172, Test f1: 0.8484, AUC: class-0>>0.9827550167224081|class-1>>0.9261479591836735\n",
      "Epoch: 118, Train Loss: 0.0054, Test Loss: 0.0780, Train Acc: 0.9954, Test Acc: 0.7983, Test f1: 0.8324, AUC: class-0>>0.9813963210702341|class-1>>0.9258928571428571\n",
      "Epoch: 119, Train Loss: 0.0056, Test Loss: 0.0750, Train Acc: 0.9954, Test Acc: 0.8155, Test f1: 0.8459, AUC: class-0>>0.9807692307692308|class-1>>0.9251275510204082\n",
      "Epoch: 120, Train Loss: 0.0054, Test Loss: 0.0740, Train Acc: 0.9954, Test Acc: 0.8205, Test f1: 0.8503, AUC: class-0>>0.9821279264214047|class-1>>0.9235969387755101\n",
      "Epoch: 121, Train Loss: 0.0048, Test Loss: 0.0746, Train Acc: 0.9954, Test Acc: 0.7996, Test f1: 0.8317, AUC: class-0>>0.9817098662207357|class-1>>0.9256377551020408\n",
      "Epoch: 122, Train Loss: 0.0052, Test Loss: 0.0739, Train Acc: 0.9954, Test Acc: 0.8135, Test f1: 0.8437, AUC: class-0>>0.9803511705685619|class-1>>0.9261479591836735\n",
      "Epoch: 123, Train Loss: 0.0046, Test Loss: 0.0764, Train Acc: 0.9954, Test Acc: 0.8076, Test f1: 0.8390, AUC: class-0>>0.982232441471572|class-1>>0.9207908163265306\n",
      "Epoch: 124, Train Loss: 0.0048, Test Loss: 0.0780, Train Acc: 0.9954, Test Acc: 0.8172, Test f1: 0.8487, AUC: class-0>>0.982755016722408|class-1>>0.921938775510204\n",
      "Epoch: 125, Train Loss: 0.0044, Test Loss: 0.0744, Train Acc: 0.9954, Test Acc: 0.8205, Test f1: 0.8503, AUC: class-0>>0.9839046822742474|class-1>>0.922704081632653\n",
      "Epoch: 126, Train Loss: 0.0042, Test Loss: 0.0741, Train Acc: 0.9954, Test Acc: 0.8102, Test f1: 0.8426, AUC: class-0>>0.983173076923077|class-1>>0.9241071428571429\n",
      "Epoch: 127, Train Loss: 0.0044, Test Loss: 0.0774, Train Acc: 0.9954, Test Acc: 0.8016, Test f1: 0.8342, AUC: class-0>>0.9805602006688963|class-1>>0.9201530612244898\n",
      "Epoch: 128, Train Loss: 0.0039, Test Loss: 0.0752, Train Acc: 0.9954, Test Acc: 0.8135, Test f1: 0.8439, AUC: class-0>>0.981814381270903|class-1>>0.9228316326530612\n",
      "Epoch: 129, Train Loss: 0.0037, Test Loss: 0.0758, Train Acc: 0.9954, Test Acc: 0.8172, Test f1: 0.8487, AUC: class-0>>0.9812918060200668|class-1>>0.9228316326530613\n",
      "Epoch: 130, Train Loss: 0.0042, Test Loss: 0.0754, Train Acc: 0.9977, Test Acc: 0.8033, Test f1: 0.8361, AUC: class-0>>0.9813963210702341|class-1>>0.9223214285714286\n",
      "Epoch: 131, Train Loss: 0.0035, Test Loss: 0.0743, Train Acc: 0.9977, Test Acc: 0.8135, Test f1: 0.8439, AUC: class-0>>0.9821279264214047|class-1>>0.9218112244897959\n",
      "Epoch: 132, Train Loss: 0.0035, Test Loss: 0.0756, Train Acc: 0.9977, Test Acc: 0.8172, Test f1: 0.8487, AUC: class-0>>0.9823369565217391|class-1>>0.9239795918367347\n",
      "Epoch: 133, Train Loss: 0.0035, Test Loss: 0.0746, Train Acc: 0.9977, Test Acc: 0.8205, Test f1: 0.8503, AUC: class-0>>0.9825459866220736|class-1>>0.9241071428571428\n",
      "Epoch: 134, Train Loss: 0.0033, Test Loss: 0.0750, Train Acc: 0.9977, Test Acc: 0.8135, Test f1: 0.8439, AUC: class-0>>0.9821279264214047|class-1>>0.921811224489796\n",
      "Epoch: 135, Train Loss: 0.0035, Test Loss: 0.0749, Train Acc: 0.9977, Test Acc: 0.8102, Test f1: 0.8423, AUC: class-0>>0.9815008361204013|class-1>>0.9211734693877551\n",
      "Epoch: 136, Train Loss: 0.0034, Test Loss: 0.0745, Train Acc: 0.9977, Test Acc: 0.8102, Test f1: 0.8423, AUC: class-0>>0.9821279264214047|class-1>>0.9230867346938777\n",
      "Epoch: 137, Train Loss: 0.0031, Test Loss: 0.0762, Train Acc: 0.9977, Test Acc: 0.8172, Test f1: 0.8487, AUC: class-0>>0.9819188963210702|class-1>>0.921173469387755\n",
      "Epoch: 138, Train Loss: 0.0032, Test Loss: 0.0776, Train Acc: 0.9977, Test Acc: 0.8043, Test f1: 0.8373, AUC: class-0>>0.9817098662207359|class-1>>0.9200255102040816\n",
      "Epoch: 139, Train Loss: 0.0031, Test Loss: 0.0757, Train Acc: 0.9977, Test Acc: 0.8241, Test f1: 0.8548, AUC: class-0>>0.982232441471572|class-1>>0.9218112244897959\n",
      "Epoch: 140, Train Loss: 0.0032, Test Loss: 0.0774, Train Acc: 0.9977, Test Acc: 0.8145, Test f1: 0.8454, AUC: class-0>>0.9808737458193979|class-1>>0.9214285714285715\n",
      "Epoch: 141, Train Loss: 0.0031, Test Loss: 0.0765, Train Acc: 0.9977, Test Acc: 0.8135, Test f1: 0.8439, AUC: class-0>>0.9809782608695652|class-1>>0.9201530612244898\n",
      "Epoch: 142, Train Loss: 0.0028, Test Loss: 0.0765, Train Acc: 0.9977, Test Acc: 0.8102, Test f1: 0.8423, AUC: class-0>>0.9815008361204014|class-1>>0.9204081632653062\n",
      "Epoch: 143, Train Loss: 0.0029, Test Loss: 0.0774, Train Acc: 0.9977, Test Acc: 0.8102, Test f1: 0.8426, AUC: class-0>>0.9824414715719064|class-1>>0.9198979591836736\n",
      "Epoch: 144, Train Loss: 0.0027, Test Loss: 0.0764, Train Acc: 0.9977, Test Acc: 0.8135, Test f1: 0.8439, AUC: class-0>>0.981814381270903|class-1>>0.9220663265306123\n",
      "Epoch: 145, Train Loss: 0.0028, Test Loss: 0.0779, Train Acc: 0.9977, Test Acc: 0.8145, Test f1: 0.8450, AUC: class-0>>0.981814381270903|class-1>>0.920280612244898\n",
      "Epoch: 146, Train Loss: 0.0027, Test Loss: 0.0772, Train Acc: 0.9977, Test Acc: 0.8043, Test f1: 0.8377, AUC: class-0>>0.9817098662207357|class-1>>0.9190051020408163\n",
      "Epoch: 147, Train Loss: 0.0027, Test Loss: 0.0763, Train Acc: 0.9977, Test Acc: 0.8033, Test f1: 0.8361, AUC: class-0>>0.9816053511705686|class-1>>0.9190051020408163\n",
      "Epoch: 148, Train Loss: 0.0027, Test Loss: 0.0777, Train Acc: 0.9977, Test Acc: 0.8033, Test f1: 0.8361, AUC: class-0>>0.9806647157190636|class-1>>0.9200255102040816\n",
      "Epoch: 149, Train Loss: 0.0030, Test Loss: 0.0752, Train Acc: 0.9977, Test Acc: 0.8102, Test f1: 0.8423, AUC: class-0>>0.9812918060200668|class-1>>0.9215561224489797\n",
      "Running cross-validation fold: 03\n",
      "Epoch: 001, Train Loss: 0.1647, Test Loss: 0.1570, Train Acc: 0.7463, Test Acc: 0.7927, Test f1: 0.8114, AUC: class-0>>0.9629318492071826|class-1>>0.9374597034171502\n",
      "Saved model!\n",
      "Best thresholds ===>>> class-0>>0.6169809699058533|class-1>>0.570250928401947\n",
      "Epoch: 002, Train Loss: 0.0850, Test Loss: 0.0863, Train Acc: 0.7546, Test Acc: 0.8700, Test f1: 0.8816, AUC: class-0>>0.9651370366481151|class-1>>0.9526756931012249\n",
      "Saved model!\n",
      "Best thresholds ===>>> class-0>>0.6369872093200684|class-1>>0.6474056243896484\n",
      "Epoch: 003, Train Loss: 0.0778, Test Loss: 0.0792, Train Acc: 0.7670, Test Acc: 0.8172, Test f1: 0.8321, AUC: class-0>>0.9724876614512233|class-1>>0.9549967762733719\n",
      "Epoch: 004, Train Loss: 0.0704, Test Loss: 0.0742, Train Acc: 0.7595, Test Acc: 0.8349, Test f1: 0.8519, AUC: class-0>>0.9737477685603277|class-1>>0.9542230818826563\n",
      "Epoch: 005, Train Loss: 0.0841, Test Loss: 0.0856, Train Acc: 0.8020, Test Acc: 0.8272, Test f1: 0.8494, AUC: class-0>>0.972592670376982|class-1>>0.9472598323662154\n",
      "Epoch: 006, Train Loss: 0.0824, Test Loss: 0.0910, Train Acc: 0.7795, Test Acc: 0.8555, Test f1: 0.8734, AUC: class-0>>0.9761629738527775|class-1>>0.9556415215989685\n",
      "Epoch: 007, Train Loss: 0.0668, Test Loss: 0.0739, Train Acc: 0.7772, Test Acc: 0.8114, Test f1: 0.8325, AUC: class-0>>0.9781581434421925|class-1>>0.95538362346873\n",
      "Epoch: 008, Train Loss: 0.0888, Test Loss: 0.0838, Train Acc: 0.8057, Test Acc: 0.8208, Test f1: 0.8426, AUC: class-0>>0.9759529560012601|class-1>>0.9601547388781432\n",
      "Epoch: 009, Train Loss: 0.0823, Test Loss: 0.0816, Train Acc: 0.7755, Test Acc: 0.8360, Test f1: 0.8553, AUC: class-0>>0.9757429381497427|class-1>>0.9544809800128949\n",
      "Epoch: 010, Train Loss: 0.0654, Test Loss: 0.0705, Train Acc: 0.7966, Test Acc: 0.8322, Test f1: 0.8505, AUC: class-0>>0.9780531345164339|class-1>>0.9537072856221793\n",
      "Epoch: 011, Train Loss: 0.0626, Test Loss: 0.0717, Train Acc: 0.7901, Test Acc: 0.8397, Test f1: 0.8598, AUC: class-0>>0.980363330883125|class-1>>0.9551257253384913\n",
      "Epoch: 012, Train Loss: 0.0799, Test Loss: 0.0779, Train Acc: 0.7940, Test Acc: 0.8333, Test f1: 0.8535, AUC: class-0>>0.9797332773285728|class-1>>0.958220502901354\n",
      "Epoch: 013, Train Loss: 0.0734, Test Loss: 0.0766, Train Acc: 0.8016, Test Acc: 0.8477, Test f1: 0.8632, AUC: class-0>>0.9787881969967447|class-1>>0.9577047066408769\n",
      "Epoch: 014, Train Loss: 0.0603, Test Loss: 0.0677, Train Acc: 0.8092, Test Acc: 0.8435, Test f1: 0.8644, AUC: class-0>>0.9794182505512969|class-1>>0.9617021276595744\n",
      "Epoch: 015, Train Loss: 0.0588, Test Loss: 0.0682, Train Acc: 0.8009, Test Acc: 0.8370, Test f1: 0.8581, AUC: class-0>>0.9808883755119185|class-1>>0.9589941972920696\n",
      "Epoch: 016, Train Loss: 0.0650, Test Loss: 0.0686, Train Acc: 0.8093, Test Acc: 0.8541, Test f1: 0.8692, AUC: class-0>>0.9818334558437467|class-1>>0.9605415860735009\n",
      "Epoch: 017, Train Loss: 0.0572, Test Loss: 0.0644, Train Acc: 0.8166, Test Acc: 0.8424, Test f1: 0.8608, AUC: class-0>>0.9816234379922293|class-1>>0.9584784010315925\n",
      "Epoch: 018, Train Loss: 0.0792, Test Loss: 0.0889, Train Acc: 0.7962, Test Acc: 0.8435, Test f1: 0.8641, AUC: class-0>>0.9810983933634359|class-1>>0.9565441650548033\n",
      "Epoch: 019, Train Loss: 0.0560, Test Loss: 0.0696, Train Acc: 0.8127, Test Acc: 0.8397, Test f1: 0.8598, AUC: class-0>>0.981728446917988|class-1>>0.9561573178594456\n",
      "Epoch: 020, Train Loss: 0.0548, Test Loss: 0.0682, Train Acc: 0.8232, Test Acc: 0.8424, Test f1: 0.8608, AUC: class-0>>0.981413420140712|class-1>>0.9609284332688588\n",
      "Epoch: 021, Train Loss: 0.0524, Test Loss: 0.0654, Train Acc: 0.8236, Test Acc: 0.8424, Test f1: 0.8608, AUC: class-0>>0.980678357660401|class-1>>0.9592520954223083\n",
      "Epoch: 022, Train Loss: 0.0539, Test Loss: 0.0676, Train Acc: 0.8177, Test Acc: 0.8424, Test f1: 0.8608, AUC: class-0>>0.9801533130316076|class-1>>0.9562862669245649\n",
      "Epoch: 023, Train Loss: 0.0989, Test Loss: 0.1213, Train Acc: 0.8152, Test Acc: 0.8360, Test f1: 0.8546, AUC: class-0>>0.9771080541846056|class-1>>0.9584784010315925\n",
      "Epoch: 024, Train Loss: 0.0514, Test Loss: 0.0647, Train Acc: 0.8329, Test Acc: 0.8269, Test f1: 0.8471, AUC: class-0>>0.9826735272498163|class-1>>0.9597678916827852\n",
      "Epoch: 025, Train Loss: 0.0603, Test Loss: 0.0682, Train Acc: 0.8357, Test Acc: 0.8386, Test f1: 0.8563, AUC: class-0>>0.9813084112149533|class-1>>0.9540941328175371\n",
      "Epoch: 026, Train Loss: 0.0505, Test Loss: 0.0635, Train Acc: 0.8255, Test Acc: 0.8360, Test f1: 0.8549, AUC: class-0>>0.9822534915467813|class-1>>0.958220502901354\n",
      "Epoch: 027, Train Loss: 0.0591, Test Loss: 0.0678, Train Acc: 0.8517, Test Acc: 0.8296, Test f1: 0.8487, AUC: class-0>>0.9843536700619553|class-1>>0.9583494519664733\n",
      "Epoch: 028, Train Loss: 0.0585, Test Loss: 0.0778, Train Acc: 0.8291, Test Acc: 0.8322, Test f1: 0.8498, AUC: class-0>>0.977003045258847|class-1>>0.9604126370083818\n",
      "Epoch: 029, Train Loss: 0.0666, Test Loss: 0.0779, Train Acc: 0.8326, Test Acc: 0.8269, Test f1: 0.8471, AUC: class-0>>0.9776330988133992|class-1>>0.9560283687943263\n",
      "Epoch: 030, Train Loss: 0.0506, Test Loss: 0.0663, Train Acc: 0.8411, Test Acc: 0.8424, Test f1: 0.8612, AUC: class-0>>0.9809933844376773|class-1>>0.9593810444874274\n",
      "Epoch: 031, Train Loss: 0.0565, Test Loss: 0.0760, Train Acc: 0.8331, Test Acc: 0.8167, Test f1: 0.8359, AUC: class-0>>0.9780531345164338|class-1>>0.9589941972920696\n",
      "Epoch: 032, Train Loss: 0.0497, Test Loss: 0.0691, Train Acc: 0.8566, Test Acc: 0.8322, Test f1: 0.8498, AUC: class-0>>0.9717525989709125|class-1>>0.9604126370083816\n",
      "Epoch: 033, Train Loss: 0.0464, Test Loss: 0.0681, Train Acc: 0.8518, Test Acc: 0.8386, Test f1: 0.8563, AUC: class-0>>0.9799432951800904|class-1>>0.9613152804642167\n",
      "Epoch: 034, Train Loss: 0.0468, Test Loss: 0.0625, Train Acc: 0.8574, Test Acc: 0.8205, Test f1: 0.8407, AUC: class-0>>0.9821484826210227|class-1>>0.961960025789813\n",
      "Epoch: 035, Train Loss: 0.0452, Test Loss: 0.0638, Train Acc: 0.8645, Test Acc: 0.8386, Test f1: 0.8563, AUC: class-0>>0.9799432951800904|class-1>>0.9575757575757575\n",
      "Epoch: 036, Train Loss: 0.0483, Test Loss: 0.0678, Train Acc: 0.8531, Test Acc: 0.8518, Test f1: 0.8687, AUC: class-0>>0.9794182505512968|class-1>>0.9565441650548033\n",
      "Epoch: 037, Train Loss: 0.0508, Test Loss: 0.0678, Train Acc: 0.8636, Test Acc: 0.8269, Test f1: 0.8468, AUC: class-0>>0.9789982148482621|class-1>>0.9544809800128949\n",
      "Epoch: 038, Train Loss: 0.0415, Test Loss: 0.0641, Train Acc: 0.8760, Test Acc: 0.8360, Test f1: 0.8549, AUC: class-0>>0.9787881969967447|class-1>>0.9595099935525467\n",
      "Epoch: 039, Train Loss: 0.0442, Test Loss: 0.0717, Train Acc: 0.9015, Test Acc: 0.8028, Test f1: 0.8199, AUC: class-0>>0.9653470544996324|class-1>>0.9598968407479045\n",
      "Epoch: 040, Train Loss: 0.0561, Test Loss: 0.0695, Train Acc: 0.8874, Test Acc: 0.8182, Test f1: 0.8406, AUC: class-0>>0.9822534915467815|class-1>>0.962862669245648\n",
      "Epoch: 041, Train Loss: 0.0415, Test Loss: 0.0659, Train Acc: 0.8780, Test Acc: 0.8296, Test f1: 0.8487, AUC: class-0>>0.9804683398088837|class-1>>0.9600257898130238\n",
      "Epoch: 042, Train Loss: 0.0431, Test Loss: 0.0646, Train Acc: 0.8694, Test Acc: 0.8205, Test f1: 0.8407, AUC: class-0>>0.977003045258847|class-1>>0.9562862669245648\n",
      "Epoch: 043, Train Loss: 0.0410, Test Loss: 0.0620, Train Acc: 0.8791, Test Acc: 0.8205, Test f1: 0.8407, AUC: class-0>>0.9809933844376773|class-1>>0.959638942617666\n",
      "Epoch: 044, Train Loss: 0.0466, Test Loss: 0.0634, Train Acc: 0.9031, Test Acc: 0.8337, Test f1: 0.8546, AUC: class-0>>0.9819384647695053|class-1>>0.9624758220502901\n",
      "Epoch: 045, Train Loss: 0.0443, Test Loss: 0.0734, Train Acc: 0.8617, Test Acc: 0.8360, Test f1: 0.8549, AUC: class-0>>0.9792082326997795|class-1>>0.9593810444874274\n",
      "Epoch: 046, Train Loss: 0.0405, Test Loss: 0.0650, Train Acc: 0.8787, Test Acc: 0.8296, Test f1: 0.8487, AUC: class-0>>0.9810983933634359|class-1>>0.9602836879432625\n",
      "Epoch: 047, Train Loss: 0.0410, Test Loss: 0.0669, Train Acc: 0.9046, Test Acc: 0.8299, Test f1: 0.8494, AUC: class-0>>0.9711225454163604|class-1>>0.9591231463571889\n",
      "Epoch: 048, Train Loss: 0.0392, Test Loss: 0.0645, Train Acc: 0.8944, Test Acc: 0.8269, Test f1: 0.8467, AUC: class-0>>0.9825685183240574|class-1>>0.956415215989684\n",
      "Epoch: 049, Train Loss: 0.0413, Test Loss: 0.0729, Train Acc: 0.8818, Test Acc: 0.8194, Test f1: 0.8375, AUC: class-0>>0.9764780006300536|class-1>>0.9600257898130238\n",
      "Epoch: 050, Train Loss: 0.0371, Test Loss: 0.0671, Train Acc: 0.8746, Test Acc: 0.8285, Test f1: 0.8453, AUC: class-0>>0.9726976793027406|class-1>>0.956802063185042\n",
      "Epoch: 051, Train Loss: 0.0545, Test Loss: 0.0802, Train Acc: 0.8963, Test Acc: 0.8144, Test f1: 0.8358, AUC: class-0>>0.9711225454163606|class-1>>0.959638942617666\n",
      "Epoch: 052, Train Loss: 0.0611, Test Loss: 0.0848, Train Acc: 0.8967, Test Acc: 0.8231, Test f1: 0.8426, AUC: class-0>>0.9789982148482621|class-1>>0.9568020631850419\n",
      "Epoch: 053, Train Loss: 0.0411, Test Loss: 0.0744, Train Acc: 0.8865, Test Acc: 0.8338, Test f1: 0.8483, AUC: class-0>>0.9653470544996324|class-1>>0.9571889103803998\n",
      "Epoch: 054, Train Loss: 0.0392, Test Loss: 0.0711, Train Acc: 0.8995, Test Acc: 0.8322, Test f1: 0.8501, AUC: class-0>>0.9758479470755015|class-1>>0.9592520954223083\n",
      "Epoch: 055, Train Loss: 0.0363, Test Loss: 0.0655, Train Acc: 0.8794, Test Acc: 0.8504, Test f1: 0.8639, AUC: class-0>>0.9707025097133256|class-1>>0.9574468085106383\n",
      "Epoch: 056, Train Loss: 0.0389, Test Loss: 0.0696, Train Acc: 0.8940, Test Acc: 0.8141, Test f1: 0.8344, AUC: class-0>>0.9768980363330884|class-1>>0.9607994842037395\n",
      "Epoch: 057, Train Loss: 0.0394, Test Loss: 0.0699, Train Acc: 0.9244, Test Acc: 0.8144, Test f1: 0.8356, AUC: class-0>>0.9745878399663972|class-1>>0.9631205673758865\n",
      "Epoch: 058, Train Loss: 0.0386, Test Loss: 0.0668, Train Acc: 0.9109, Test Acc: 0.8296, Test f1: 0.8487, AUC: class-0>>0.9809933844376771|class-1>>0.9614442295293358\n",
      "Epoch: 059, Train Loss: 0.0358, Test Loss: 0.0677, Train Acc: 0.9229, Test Acc: 0.8349, Test f1: 0.8515, AUC: class-0>>0.9768980363330884|class-1>>0.9626047711154093\n",
      "Epoch: 060, Train Loss: 0.0346, Test Loss: 0.0645, Train Acc: 0.9117, Test Acc: 0.8208, Test f1: 0.8422, AUC: class-0>>0.9744828310406385|class-1>>0.9601547388781431\n",
      "Epoch: 061, Train Loss: 0.0335, Test Loss: 0.0653, Train Acc: 0.9158, Test Acc: 0.8205, Test f1: 0.8407, AUC: class-0>>0.9792082326997793|class-1>>0.9606705351386202\n",
      "Epoch: 062, Train Loss: 0.0330, Test Loss: 0.0657, Train Acc: 0.9189, Test Acc: 0.8296, Test f1: 0.8487, AUC: class-0>>0.9795232594770555|class-1>>0.9614442295293358\n",
      "Epoch: 063, Train Loss: 0.0324, Test Loss: 0.0681, Train Acc: 0.9032, Test Acc: 0.8299, Test f1: 0.8494, AUC: class-0>>0.9735377507088102|class-1>>0.9610573823339781\n",
      "Epoch: 064, Train Loss: 0.0345, Test Loss: 0.0687, Train Acc: 0.9232, Test Acc: 0.8401, Test f1: 0.8601, AUC: class-0>>0.9799432951800903|class-1>>0.958736299161831\n",
      "Epoch: 065, Train Loss: 0.0402, Test Loss: 0.0723, Train Acc: 0.9456, Test Acc: 0.8221, Test f1: 0.8400, AUC: class-0>>0.9745878399663971|class-1>>0.9619600257898131\n",
      "Epoch: 066, Train Loss: 0.0340, Test Loss: 0.0644, Train Acc: 0.9423, Test Acc: 0.8208, Test f1: 0.8422, AUC: class-0>>0.9757429381497428|class-1>>0.962862669245648\n",
      "Epoch: 067, Train Loss: 0.0314, Test Loss: 0.0651, Train Acc: 0.9160, Test Acc: 0.8507, Test f1: 0.8647, AUC: class-0>>0.973957786411845|class-1>>0.9619600257898131\n",
      "Epoch: 068, Train Loss: 0.0275, Test Loss: 0.0653, Train Acc: 0.9395, Test Acc: 0.8205, Test f1: 0.8407, AUC: class-0>>0.9761629738527775|class-1>>0.9611863313990975\n",
      "Epoch: 069, Train Loss: 0.0330, Test Loss: 0.0682, Train Acc: 0.9442, Test Acc: 0.8504, Test f1: 0.8639, AUC: class-0>>0.9723826525254646|class-1>>0.9583494519664734\n",
      "Epoch: 070, Train Loss: 0.0318, Test Loss: 0.0704, Train Acc: 0.9434, Test Acc: 0.8247, Test f1: 0.8408, AUC: class-0>>0.9718576078966713|class-1>>0.9647969052224371\n",
      "Epoch: 071, Train Loss: 0.0369, Test Loss: 0.0721, Train Acc: 0.9233, Test Acc: 0.8137, Test f1: 0.8329, AUC: class-0>>0.9713325632678776|class-1>>0.9622179239200516\n",
      "Epoch: 072, Train Loss: 0.0265, Test Loss: 0.0638, Train Acc: 0.9521, Test Acc: 0.8208, Test f1: 0.8418, AUC: class-0>>0.9719626168224298|class-1>>0.9645390070921985\n",
      "Epoch: 073, Train Loss: 0.0317, Test Loss: 0.0690, Train Acc: 0.9196, Test Acc: 0.8468, Test f1: 0.8669, AUC: class-0>>0.9795232594770555|class-1>>0.9645390070921985\n",
      "Epoch: 074, Train Loss: 0.0243, Test Loss: 0.0611, Train Acc: 0.9472, Test Acc: 0.8401, Test f1: 0.8601, AUC: class-0>>0.9806783576604011|class-1>>0.9631205673758866\n",
      "Epoch: 075, Train Loss: 0.0284, Test Loss: 0.0710, Train Acc: 0.9501, Test Acc: 0.8340, Test f1: 0.8550, AUC: class-0>>0.9731177150057754|class-1>>0.9615731785944551\n",
      "Epoch: 076, Train Loss: 0.0290, Test Loss: 0.0669, Train Acc: 0.9595, Test Acc: 0.8488, Test f1: 0.8673, AUC: class-0>>0.9812034022891944|class-1>>0.9646679561573178\n",
      "Epoch: 077, Train Loss: 0.0240, Test Loss: 0.0629, Train Acc: 0.9374, Test Acc: 0.8265, Test f1: 0.8455, AUC: class-0>>0.9782631523679512|class-1>>0.9623468729851709\n",
      "Epoch: 078, Train Loss: 0.0222, Test Loss: 0.0650, Train Acc: 0.9526, Test Acc: 0.8013, Test f1: 0.8213, AUC: class-0>>0.9755329202982254|class-1>>0.9636363636363636\n",
      "Epoch: 079, Train Loss: 0.0205, Test Loss: 0.0623, Train Acc: 0.9516, Test Acc: 0.8450, Test f1: 0.8616, AUC: class-0>>0.9792082326997795|class-1>>0.9644100580270792\n",
      "Epoch: 080, Train Loss: 0.0210, Test Loss: 0.0651, Train Acc: 0.9620, Test Acc: 0.8205, Test f1: 0.8404, AUC: class-0>>0.9745878399663972|class-1>>0.9662153449387492\n",
      "Epoch: 081, Train Loss: 0.0216, Test Loss: 0.0606, Train Acc: 0.9611, Test Acc: 0.8555, Test f1: 0.8732, AUC: class-0>>0.9798382862543317|class-1>>0.9650548033526758\n",
      "Saved model!\n",
      "Best thresholds ===>>> class-0>>0.6370082497596741|class-1>>0.6488146781921387\n",
      "Epoch: 082, Train Loss: 0.0226, Test Loss: 0.0621, Train Acc: 0.9658, Test Acc: 0.8296, Test f1: 0.8484, AUC: class-0>>0.9773180720361232|class-1>>0.966602192134107\n",
      "Epoch: 083, Train Loss: 0.0198, Test Loss: 0.0637, Train Acc: 0.9632, Test Acc: 0.8374, Test f1: 0.8581, AUC: class-0>>0.977003045258847|class-1>>0.9638942617666022\n",
      "Epoch: 084, Train Loss: 0.0189, Test Loss: 0.0632, Train Acc: 0.9679, Test Acc: 0.8265, Test f1: 0.8458, AUC: class-0>>0.9782631523679512|class-1>>0.9640232108317215\n",
      "Epoch: 085, Train Loss: 0.0192, Test Loss: 0.0617, Train Acc: 0.9725, Test Acc: 0.8408, Test f1: 0.8620, AUC: class-0>>0.9784731702194686|class-1>>0.9669890393294648\n",
      "Epoch: 086, Train Loss: 0.0242, Test Loss: 0.0673, Train Acc: 0.9708, Test Acc: 0.8502, Test f1: 0.8695, AUC: class-0>>0.9745878399663972|class-1>>0.9561573178594456\n",
      "Epoch: 087, Train Loss: 0.0176, Test Loss: 0.0667, Train Acc: 0.9674, Test Acc: 0.8126, Test f1: 0.8302, AUC: class-0>>0.9702824740102909|class-1>>0.9627337201805287\n",
      "Epoch: 088, Train Loss: 0.0184, Test Loss: 0.0664, Train Acc: 0.9509, Test Acc: 0.8386, Test f1: 0.8556, AUC: class-0>>0.974587839966397|class-1>>0.9644100580270792\n",
      "Epoch: 089, Train Loss: 0.0167, Test Loss: 0.0660, Train Acc: 0.9674, Test Acc: 0.8201, Test f1: 0.8392, AUC: class-0>>0.973642759634569|class-1>>0.9644100580270792\n",
      "Epoch: 090, Train Loss: 0.0168, Test Loss: 0.0684, Train Acc: 0.9819, Test Acc: 0.8201, Test f1: 0.8392, AUC: class-0>>0.9730127060800167|class-1>>0.9579626047711154\n",
      "Epoch: 091, Train Loss: 0.0194, Test Loss: 0.0673, Train Acc: 0.9650, Test Acc: 0.8474, Test f1: 0.8620, AUC: class-0>>0.9747978578179144|class-1>>0.9629916183107673\n",
      "Epoch: 092, Train Loss: 0.0197, Test Loss: 0.0691, Train Acc: 0.9695, Test Acc: 0.8311, Test f1: 0.8472, AUC: class-0>>0.9696524204557386|class-1>>0.9640232108317215\n",
      "Epoch: 093, Train Loss: 0.0174, Test Loss: 0.0708, Train Acc: 0.9771, Test Acc: 0.8100, Test f1: 0.8287, AUC: class-0>>0.9767930274073295|class-1>>0.9588652482269503\n",
      "Epoch: 094, Train Loss: 0.0133, Test Loss: 0.0644, Train Acc: 0.9819, Test Acc: 0.8265, Test f1: 0.8455, AUC: class-0>>0.9737477685603276|class-1>>0.9654416505480334\n",
      "Epoch: 095, Train Loss: 0.0164, Test Loss: 0.0708, Train Acc: 0.9795, Test Acc: 0.8335, Test f1: 0.8467, AUC: class-0>>0.9635619027617348|class-1>>0.9627337201805286\n",
      "Epoch: 096, Train Loss: 0.0131, Test Loss: 0.0637, Train Acc: 0.9792, Test Acc: 0.8292, Test f1: 0.8471, AUC: class-0>>0.9766880184815709|class-1>>0.9605415860735009\n",
      "Epoch: 097, Train Loss: 0.0122, Test Loss: 0.0631, Train Acc: 0.9840, Test Acc: 0.8265, Test f1: 0.8458, AUC: class-0>>0.9762679827785361|class-1>>0.9623468729851707\n",
      "Epoch: 098, Train Loss: 0.0150, Test Loss: 0.0672, Train Acc: 0.9816, Test Acc: 0.8330, Test f1: 0.8520, AUC: class-0>>0.9739577864118449|class-1>>0.962862669245648\n",
      "Epoch: 099, Train Loss: 0.0116, Test Loss: 0.0629, Train Acc: 0.9863, Test Acc: 0.8330, Test f1: 0.8517, AUC: class-0>>0.9755329202982255|class-1>>0.960541586073501\n",
      "Epoch: 100, Train Loss: 0.0125, Test Loss: 0.0668, Train Acc: 0.9840, Test Acc: 0.8474, Test f1: 0.8627, AUC: class-0>>0.9707025097133255|class-1>>0.9644100580270794\n",
      "Epoch: 101, Train Loss: 0.0105, Test Loss: 0.0634, Train Acc: 0.9863, Test Acc: 0.8265, Test f1: 0.8453, AUC: class-0>>0.9756379292239841|class-1>>0.9615731785944551\n",
      "Epoch: 102, Train Loss: 0.0165, Test Loss: 0.0657, Train Acc: 0.9863, Test Acc: 0.8397, Test f1: 0.8595, AUC: class-0>>0.9776330988133992|class-1>>0.9606705351386202\n",
      "Epoch: 103, Train Loss: 0.0116, Test Loss: 0.0662, Train Acc: 0.9811, Test Acc: 0.8269, Test f1: 0.8471, AUC: class-0>>0.9725926703769819|class-1>>0.961444229529336\n",
      "Epoch: 104, Train Loss: 0.0124, Test Loss: 0.0654, Train Acc: 0.9882, Test Acc: 0.8319, Test f1: 0.8493, AUC: class-0>>0.9767930274073297|class-1>>0.9637653127014829\n",
      "Epoch: 105, Train Loss: 0.0142, Test Loss: 0.0734, Train Acc: 0.9866, Test Acc: 0.8201, Test f1: 0.8392, AUC: class-0>>0.9753229024467079|class-1>>0.9551257253384914\n",
      "Epoch: 106, Train Loss: 0.0102, Test Loss: 0.0659, Train Acc: 0.9861, Test Acc: 0.8333, Test f1: 0.8528, AUC: class-0>>0.9726976793027408|class-1>>0.960541586073501\n",
      "Epoch: 107, Train Loss: 0.0104, Test Loss: 0.0679, Train Acc: 0.9866, Test Acc: 0.8269, Test f1: 0.8466, AUC: class-0>>0.9723826525254647|class-1>>0.9557704706640877\n",
      "Epoch: 108, Train Loss: 0.0097, Test Loss: 0.0648, Train Acc: 0.9905, Test Acc: 0.8201, Test f1: 0.8392, AUC: class-0>>0.9756379292239841|class-1>>0.9627337201805287\n",
      "Epoch: 109, Train Loss: 0.0085, Test Loss: 0.0664, Train Acc: 0.9905, Test Acc: 0.8215, Test f1: 0.8441, AUC: class-0>>0.9703874829360495|class-1>>0.9642811089619601\n",
      "Epoch: 110, Train Loss: 0.0090, Test Loss: 0.0642, Train Acc: 0.9905, Test Acc: 0.8265, Test f1: 0.8455, AUC: class-0>>0.9738527774860863|class-1>>0.9618310767246938\n",
      "Epoch: 111, Train Loss: 0.0095, Test Loss: 0.0646, Train Acc: 0.9905, Test Acc: 0.8340, Test f1: 0.8551, AUC: class-0>>0.9735377507088103|class-1>>0.9618310767246937\n",
      "Epoch: 112, Train Loss: 0.0081, Test Loss: 0.0650, Train Acc: 0.9905, Test Acc: 0.8164, Test f1: 0.8351, AUC: class-0>>0.9730127060800167|class-1>>0.9619600257898131\n",
      "Epoch: 113, Train Loss: 0.0083, Test Loss: 0.0666, Train Acc: 0.9905, Test Acc: 0.8265, Test f1: 0.8455, AUC: class-0>>0.9747978578179144|class-1>>0.9610573823339781\n",
      "Epoch: 114, Train Loss: 0.0077, Test Loss: 0.0639, Train Acc: 0.9905, Test Acc: 0.8265, Test f1: 0.8455, AUC: class-0>>0.9764780006300536|class-1>>0.959638942617666\n",
      "Epoch: 115, Train Loss: 0.0070, Test Loss: 0.0648, Train Acc: 0.9905, Test Acc: 0.8330, Test f1: 0.8517, AUC: class-0>>0.974272813189121|class-1>>0.9615731785944552\n",
      "Epoch: 116, Train Loss: 0.0089, Test Loss: 0.0687, Train Acc: 0.9905, Test Acc: 0.8265, Test f1: 0.8455, AUC: class-0>>0.9738527774860863|class-1>>0.9607994842037395\n",
      "Epoch: 117, Train Loss: 0.0082, Test Loss: 0.0652, Train Acc: 0.9905, Test Acc: 0.8144, Test f1: 0.8358, AUC: class-0>>0.9743778221148797|class-1>>0.9579626047711154\n",
      "Epoch: 118, Train Loss: 0.0068, Test Loss: 0.0647, Train Acc: 0.9905, Test Acc: 0.8443, Test f1: 0.8589, AUC: class-0>>0.9755329202982254|class-1>>0.9604126370083816\n",
      "Epoch: 119, Train Loss: 0.0082, Test Loss: 0.0696, Train Acc: 0.9905, Test Acc: 0.8137, Test f1: 0.8329, AUC: class-0>>0.9740627953376038|class-1>>0.9606705351386203\n",
      "Epoch: 120, Train Loss: 0.0067, Test Loss: 0.0679, Train Acc: 0.9905, Test Acc: 0.8265, Test f1: 0.8455, AUC: class-0>>0.9732227239315342|class-1>>0.9601547388781431\n",
      "Epoch: 121, Train Loss: 0.0059, Test Loss: 0.0652, Train Acc: 0.9905, Test Acc: 0.8201, Test f1: 0.8392, AUC: class-0>>0.9754279113724666|class-1>>0.9595099935525467\n",
      "Epoch: 122, Train Loss: 0.0055, Test Loss: 0.0651, Train Acc: 0.9905, Test Acc: 0.8137, Test f1: 0.8329, AUC: class-0>>0.9740627953376035|class-1>>0.9602836879432625\n",
      "Epoch: 123, Train Loss: 0.0056, Test Loss: 0.0657, Train Acc: 0.9905, Test Acc: 0.8272, Test f1: 0.8479, AUC: class-0>>0.9730127060800167|class-1>>0.959638942617666\n",
      "Epoch: 124, Train Loss: 0.0054, Test Loss: 0.0644, Train Acc: 0.9905, Test Acc: 0.8201, Test f1: 0.8392, AUC: class-0>>0.9758479470755015|class-1>>0.9597678916827852\n",
      "Epoch: 125, Train Loss: 0.0050, Test Loss: 0.0650, Train Acc: 0.9905, Test Acc: 0.8337, Test f1: 0.8540, AUC: class-0>>0.9736427596345689|class-1>>0.9610573823339781\n",
      "Epoch: 126, Train Loss: 0.0050, Test Loss: 0.0652, Train Acc: 0.9905, Test Acc: 0.8201, Test f1: 0.8392, AUC: class-0>>0.9772130631103643|class-1>>0.9586073500967118\n",
      "Epoch: 127, Train Loss: 0.0047, Test Loss: 0.0656, Train Acc: 0.9905, Test Acc: 0.8137, Test f1: 0.8329, AUC: class-0>>0.9737477685603276|class-1>>0.9592520954223083\n",
      "Epoch: 128, Train Loss: 0.0049, Test Loss: 0.0639, Train Acc: 0.9905, Test Acc: 0.8340, Test f1: 0.8551, AUC: class-0>>0.9758479470755015|class-1>>0.9595099935525466\n",
      "Epoch: 129, Train Loss: 0.0047, Test Loss: 0.0664, Train Acc: 0.9905, Test Acc: 0.8265, Test f1: 0.8455, AUC: class-0>>0.9740627953376036|class-1>>0.9589941972920697\n",
      "Epoch: 130, Train Loss: 0.0046, Test Loss: 0.0668, Train Acc: 0.9905, Test Acc: 0.8137, Test f1: 0.8329, AUC: class-0>>0.9741678042633624|class-1>>0.9597678916827854\n",
      "Epoch: 131, Train Loss: 0.0050, Test Loss: 0.0690, Train Acc: 0.9905, Test Acc: 0.8137, Test f1: 0.8329, AUC: class-0>>0.972277643599706|class-1>>0.9597678916827853\n",
      "Epoch: 132, Train Loss: 0.0046, Test Loss: 0.0660, Train Acc: 0.9905, Test Acc: 0.8272, Test f1: 0.8481, AUC: class-0>>0.9735377507088101|class-1>>0.9571889103803998\n",
      "Epoch: 133, Train Loss: 0.0044, Test Loss: 0.0676, Train Acc: 0.9905, Test Acc: 0.8272, Test f1: 0.8481, AUC: class-0>>0.9721726346739472|class-1>>0.9589941972920697\n",
      "Epoch: 134, Train Loss: 0.0041, Test Loss: 0.0661, Train Acc: 0.9929, Test Acc: 0.8340, Test f1: 0.8551, AUC: class-0>>0.9724876614512233|class-1>>0.9595099935525466\n",
      "Epoch: 135, Train Loss: 0.0041, Test Loss: 0.0657, Train Acc: 0.9905, Test Acc: 0.8137, Test f1: 0.8329, AUC: class-0>>0.9733277328572928|class-1>>0.9607994842037395\n",
      "Epoch: 136, Train Loss: 0.0042, Test Loss: 0.0654, Train Acc: 0.9905, Test Acc: 0.8272, Test f1: 0.8481, AUC: class-0>>0.9735377507088103|class-1>>0.9595099935525468\n",
      "Epoch: 137, Train Loss: 0.0042, Test Loss: 0.0660, Train Acc: 0.9905, Test Acc: 0.8272, Test f1: 0.8481, AUC: class-0>>0.9739577864118449|class-1>>0.958220502901354\n",
      "Epoch: 138, Train Loss: 0.0040, Test Loss: 0.0671, Train Acc: 0.9905, Test Acc: 0.8337, Test f1: 0.8541, AUC: class-0>>0.9721726346739472|class-1>>0.9575757575757574\n",
      "Epoch: 139, Train Loss: 0.0038, Test Loss: 0.0660, Train Acc: 0.9905, Test Acc: 0.8272, Test f1: 0.8481, AUC: class-0>>0.9731177150057755|class-1>>0.9583494519664733\n",
      "Epoch: 140, Train Loss: 0.0037, Test Loss: 0.0665, Train Acc: 0.9905, Test Acc: 0.8272, Test f1: 0.8481, AUC: class-0>>0.9740627953376038|class-1>>0.9579626047711155\n",
      "Epoch: 141, Train Loss: 0.0038, Test Loss: 0.0665, Train Acc: 0.9955, Test Acc: 0.8137, Test f1: 0.8329, AUC: class-0>>0.9735377507088103|class-1>>0.958220502901354\n",
      "Epoch: 142, Train Loss: 0.0037, Test Loss: 0.0686, Train Acc: 0.9955, Test Acc: 0.8272, Test f1: 0.8481, AUC: class-0>>0.9715425811193952|class-1>>0.9573178594455191\n",
      "Epoch: 143, Train Loss: 0.0043, Test Loss: 0.0681, Train Acc: 0.9905, Test Acc: 0.8141, Test f1: 0.8344, AUC: class-0>>0.9725926703769819|class-1>>0.9580915538362347\n",
      "Epoch: 144, Train Loss: 0.0037, Test Loss: 0.0683, Train Acc: 0.9955, Test Acc: 0.8272, Test f1: 0.8481, AUC: class-0>>0.9723826525254646|class-1>>0.9579626047711154\n",
      "Epoch: 145, Train Loss: 0.0036, Test Loss: 0.0672, Train Acc: 0.9955, Test Acc: 0.8137, Test f1: 0.8329, AUC: class-0>>0.9728026882284995|class-1>>0.9586073500967116\n",
      "Epoch: 146, Train Loss: 0.0035, Test Loss: 0.0674, Train Acc: 0.9955, Test Acc: 0.8272, Test f1: 0.8481, AUC: class-0>>0.9725926703769819|class-1>>0.9577047066408769\n",
      "Epoch: 147, Train Loss: 0.0033, Test Loss: 0.0672, Train Acc: 0.9955, Test Acc: 0.8272, Test f1: 0.8481, AUC: class-0>>0.972592670376982|class-1>>0.9575757575757576\n",
      "Epoch: 148, Train Loss: 0.0038, Test Loss: 0.0658, Train Acc: 0.9955, Test Acc: 0.8272, Test f1: 0.8481, AUC: class-0>>0.9737477685603275|class-1>>0.9584784010315925\n",
      "Epoch: 149, Train Loss: 0.0034, Test Loss: 0.0665, Train Acc: 0.9955, Test Acc: 0.8344, Test f1: 0.8560, AUC: class-0>>0.9736427596345689|class-1>>0.9584784010315925\n"
     ]
    }
   ],
   "source": [
    "torch.save(model.state_dict(), os.path.join(weights_path, 'init_distributed.pth'))\n",
    "for i in range(num_fold):\n",
    "    model.load_state_dict(torch.load(os.path.join(weights_path, 'init_distributed.pth')))\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, num_epochs, 0.00001)\n",
    "    optimal_score = 0\n",
    "    print(f'Running cross-validation fold: {i:02d}')\n",
    "    graph_train  = []\n",
    "    graph_test = []\n",
    "    for k in range(num_fold):\n",
    "        if k != i: graph_train += chunks_data[k]\n",
    "        else: graph_test = chunks_data[k]\n",
    "    n_sample = []\n",
    "    for graph in graph_train:\n",
    "        n_sample.append(graph.y.numpy())\n",
    "    n_sample = np.asarray(n_sample)\n",
    "    counts = np.count_nonzero(n_sample, axis=0)\n",
    "    counts = counts.max() / counts\n",
    "    # criterion = torch.nn.BCEWithLogitsLoss(pos_weight=torch.from_numpy(counts)).cuda()\n",
    "    # criterion = torch.nn.BCEWithLogitsLoss().cuda()    \n",
    "    criterion = nn.MSELoss()\n",
    "    for epoch in range(1, num_epochs):\n",
    "        shuffle(graph_train)\n",
    "        train(graph_train)\n",
    "        scheduler.step()    \n",
    "        train_loss, train_acc, _, _, _ = test(graph_train)\n",
    "        test_loss, test_acc, avg_prc, aucs, thresh = test(graph_test)\n",
    "        log_msg = f'Epoch: {epoch:03d}, Train Loss: {train_loss:.4f}, Test Loss: {test_loss:.4f}, Train Acc: {train_acc:.4f}, Test Acc: {test_acc:.4f}, Test f1: {avg_prc:.4f}' + ', AUC: ' + '|'.join('class-{}>>{}'.format(*k) for k in enumerate(aucs))\n",
    "        print(log_msg)\n",
    "        sum_metrics = (avg_prc+test_acc+sum(aucs))/3\n",
    "        if sum_metrics > optimal_score:\n",
    "            optimal_score = sum_metrics\n",
    "            os.makedirs(weights_path, exist_ok=True)\n",
    "            torch.save(model.state_dict(), os.path.join(weights_path, f'{weights_name}_fold{i:02d}.pth'))\n",
    "            print('Saved model!')\n",
    "            print('Best thresholds ===>>> '+ '|'.join('class-{}>>{}'.format(*k) for k in enumerate(thresh)))\n",
    "            with open(os.path.join(weights_path, f'{weights_name}_fold{i:02d}.txt'), \"w\") as text_file:\n",
    "                print('Best thresholds ===>>> '+ '|'.join('class-{}>>{}'.format(*k) for k in enumerate(thresh))+'|||'+log_msg, file=text_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f31d9168",
   "metadata": {},
   "source": [
    "### Read result summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "10904f65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation results of GCN_TMA_Distributed: Acc 0.8627499999999999, f1 0.8836, AUC PDAC 0.9805725452685521, AUC CP 0.9557719980963282\n"
     ]
    }
   ],
   "source": [
    "log_files = glob(os.path.join(weights_path, f'{weights_name}_fold*.txt'))\n",
    "results = []\n",
    "for log_file in log_files:\n",
    "    with open(log_file) as text_file:\n",
    "        lines = text_file.readlines()[0]\n",
    "        results.append([float(i) for i in re.findall(\"\\d+\\.\\d+\", lines)[3:]])\n",
    "results = np.vstack(results).mean(0)\n",
    "print(f'Cross-validation results of {weights_name}: Acc {results[2]}, f1 {results[3]}, AUC PDAC {results[4]}, AUC CP {results[5]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "21802e70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation results of MLP_TMA_Distributed: Acc 0.8527, f1 0.87375, AUC PDAC 0.9816874910027941, AUC CP 0.9495471186392651\n"
     ]
    }
   ],
   "source": [
    "log_files = glob(os.path.join(weights_path, f'{weights_name}_fold*.txt'))\n",
    "results = []\n",
    "for log_file in log_files:\n",
    "    with open(log_file) as text_file:\n",
    "        lines = text_file.readlines()[0]\n",
    "        results.append([float(i) for i in re.findall(\"\\d+\\.\\d+\", lines)[3:]])\n",
    "results = np.vstack(results).mean(0)\n",
    "print(f'Cross-validation results of {weights_name}: Acc {results[2]}, f1 {results[3]}, AUC PDAC {results[4]}, AUC CP {results[5]}')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "de88db708fd7569f2666ff37f921dc0f88a8459546879a55f00d65d0006c4c3e"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('pt')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
