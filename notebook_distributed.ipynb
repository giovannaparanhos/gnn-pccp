{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0df9abff-b363-48fd-8cd4-873ce5a7145c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "from os.path import join, basename, splitext\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.transforms import RadiusGraph\n",
    "from skimage import io\n",
    "from tqdm import tqdm\n",
    "from skimage import img_as_float\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from torchvision import models\n",
    "import torch.nn as nn\n",
    "from random import shuffle\n",
    "from sklearn.metrics import roc_auc_score, f1_score, balanced_accuracy_score, roc_curve\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "from torch_geometric.data import Data\n",
    "from models import GraphNet, Preprocess, AttentionPool\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fdb892be-fc61-43b4-befb-80b56c5f9d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions for visualization.\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "import networkx as nx\n",
    "\n",
    "def visualize_points(pos, edge_index=None, index=None, edge_weights=None, node_weights=None, show=True, img=None, patch_size=1, fig_text=None, box_color='palegreen'):\n",
    "    fig = plt.figure(figsize=(torch.max(pos, 0)[0].numpy()[0], torch.max(pos, 0)[0].numpy()[1]))\n",
    "        # fig_w, fig_h = plt.gcf().get_size_inches()\n",
    "    if edge_index is not None:\n",
    "        if edge_weights is None:\n",
    "            for (src, dst) in edge_index.t().tolist():\n",
    "                src = pos[src].tolist()\n",
    "                dst = pos[dst].tolist()\n",
    "                plt.plot([src[0]*patch_size+int(patch_size/2), dst[0]*patch_size+int(patch_size/2)], [src[1]*patch_size+int(patch_size/2), dst[1]*patch_size+int(patch_size/2)], linewidth=3, color='royalblue')\n",
    "        else:\n",
    "            i = 0\n",
    "            for (s, d) in edge_index.t().tolist():\n",
    "                src = pos[s].tolist()\n",
    "                dst = pos[d].tolist()\n",
    "                plt.plot([src[0]*patch_size+int(patch_size/2), dst[0]*patch_size+int(patch_size/2)], [src[1]*patch_size+int(patch_size/2), dst[1]*patch_size+int(patch_size/2)], linewidth=widths[i]*3, color='royalblue')\n",
    "                i+=1\n",
    "    if index is None:\n",
    "        if node_weights is not None:\n",
    "            for p, w in zip(pos, node_weights):\n",
    "                plt.scatter(p[0]*patch_size+int(patch_size/2), p[1]*patch_size+int(patch_size/2), s=w*500, zorder=1000, color='red')\n",
    "        else:\n",
    "            plt.scatter(pos[:, 0]*patch_size+int(patch_size/2), pos[:, 1]*patch_size+int(patch_size/2), s=500, zorder=1000, color='red')\n",
    "    else:\n",
    "        mask = torch.zeros(pos.size(0), dtype=torch.bool)\n",
    "        mask[index] = True\n",
    "        plt.scatter(pos[~mask, 0], pos[~mask, 1], s=50, color='lightgray', zorder=1000)\n",
    "        plt.scatter(pos[mask, 0], pos[mask, 1], s=50, zorder=1000)\n",
    "    plt.axis('off')\n",
    "    plt.gca().invert_yaxis()\n",
    "    if img is not None:\n",
    "        im = plt.imread(img)\n",
    "        plt.imshow(im, alpha=0.5)\n",
    "    if fig_text is not None:\n",
    "        plt.figtext(0.5, 0.1, fig_text, ha=\"center\", fontsize=18, bbox={\"facecolor\":box_color, \"alpha\":0.5, \"pad\":5} )\n",
    "    if show:\n",
    "        plt.show()\n",
    "    plt.close()\n",
    "    return fig\n",
    "    \n",
    "def visualize_projection(h, color):\n",
    "    z = TSNE(n_components=2).fit_transform(h.detach().cpu().numpy())\n",
    "\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "\n",
    "    plt.scatter(z[:, 0], z[:, 1], s=70, c=color, cmap=\"Set2\")\n",
    "    plt.show()\n",
    "    \n",
    "def visualize_grid(pos, color):\n",
    "    color = color.detach().cpu().numpy()\n",
    "    pos = pos.detach().cpu().numpy()\n",
    "    # plt.figure(figsize=(10,10))\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "\n",
    "    plt.scatter(pos[:, 0], pos[:, 1], s=70, c=color, cmap=\"Set2\")\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.show()\n",
    "    \n",
    "def visualize_graph(h, color, epoch=None, loss=None):\n",
    "    plt.figure(figsize=(7,7))\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "\n",
    "    if torch.is_tensor(h):\n",
    "        h = h.detach().cpu().numpy()\n",
    "        plt.scatter(h[:, 0], h[:, 1], s=140, c=color, cmap=\"Set2\")\n",
    "        if epoch is not None and loss is not None:\n",
    "            plt.xlabel(f'Epoch: {epoch}, Loss: {loss.item():.4f}', fontsize=16)\n",
    "    else:\n",
    "        nx.draw_networkx(h, pos=nx.spring_layout(h, seed=42), with_labels=False,\n",
    "                         node_color=color, cmap=\"Set2\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "38d25aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_dataset_to_graph(csv, class_codes, num_feat=(1, 513), compute_edges=False, weight_func=cosine_similarity, radius=1.5, remove_empty=False):\n",
    "    df = pd.read_csv(csv)\n",
    "    feat_arr = np.array(df.iloc[:, num_feat[0]:num_feat[1]])\n",
    "    pos_arr = np.array(df.iloc[:, -3:-1])\n",
    "    label_arr = np.array(class_codes.get(df.iloc[0, -1]))\n",
    "\n",
    "    # if remove_empty and os.path.splitext(os.path.basename(csv))[0]:\n",
    "    #     empty_idx = np.where( np.sum(feat_arr, 1) == 0)[0]\n",
    "    #     feat_arr[empty_idx, :] = 0\n",
    "    #     if len(empty_idx)>0: pos_arr[empty_idx, :] = pos_arr[empty_idx[0], :]\n",
    "\n",
    "    data = Data(x=torch.tensor(feat_arr, dtype=torch.float), pos=torch.tensor(pos_arr, dtype=torch.long), y=torch.tensor(label_arr, dtype=torch.long))\n",
    "    radius_graph = RadiusGraph(radius, loop=True) # 1.5 or 2\n",
    "    data = radius_graph(data) \n",
    "    if compute_edges:\n",
    "        weights = []\n",
    "        for i in range(data.edge_index.shape[1]):\n",
    "            edge = data.edge_index[:, i]\n",
    "            weight = weight_func( data.x[edge[0]].view(1, -1), data.x[edge[1]].view(1, -1) )\n",
    "            weights.append(weight)\n",
    "        # weights = exposure.rescale_intensity( np.vstack(weights).squeeze(), in_range=(0.5, 1), out_range=(0, 1) )\n",
    "        weights = np.vstack(weights).squeeze()\n",
    "        data.edge_weight = torch.tensor(weights, dtype=torch.float)\n",
    "    data.name = os.path.splitext(os.path.basename(csv))[0]\n",
    "    data.label = df.iloc[0, -1]\n",
    "    return data\n",
    "\n",
    "def compute_dataset(embedder, path, class_codes, ext='png', opt_folder=False):\n",
    "    classes = glob(join(path, '*'))\n",
    "    # print(classes) \n",
    "    data_list = [] # pos, feats, node labels, node_mask, graph label\n",
    "    # return None\n",
    "    for idx, c in enumerate(classes):\n",
    "        class_name = basename(c)\n",
    "        if opt_folder:\n",
    "            regions = glob(join(path, class_name, '*', '*'))\n",
    "        else:\n",
    "            regions = glob(join(path, class_name, '*'))\n",
    "        regions = [x for x in regions if os.path.isdir(x)]\n",
    "        for region in tqdm(regions):\n",
    "            try:\n",
    "                pos_list = []\n",
    "                feat_list = []\n",
    "                node_list = []\n",
    "                mask_list = []\n",
    "                patches = glob(join(region, '*.'+ext))\n",
    "                for patch in patches:\n",
    "                    ### patch pos -> x, y\n",
    "                    patch_data = img_as_float(io.imread(patch))\n",
    "\n",
    "                    # feat = np.mean(patch_data)\n",
    "                    with torch.no_grad():\n",
    "                        patch_tensor = torch.FloatTensor(patch_data.transpose(2, 0, 1))[None, :].cuda()\n",
    "                        feat_tensor = embedder(patch_tensor)\n",
    "                        feat = feat_tensor.detach().cpu().numpy().squeeze()\n",
    "                    masked = False\n",
    "                    node_label = np.nan\n",
    "\n",
    "                    x = int(splitext(basename(patch))[0].split('_')[0])\n",
    "                    y = int(splitext(basename(patch))[0].split('_')[1])\n",
    "                    pos_list.append((x, y))\n",
    "                    feat_list.append(feat)  \n",
    "                    node_list.append(node_label)\n",
    "                    mask_list.append(masked)\n",
    "                pos_arr = np.vstack(pos_list)\n",
    "                feat_arr = np.vstack(feat_list)\n",
    "                node_arr = np.vstack(node_list)\n",
    "                mask_arr = np.vstack(mask_list)\n",
    "                graph_label = np.array(class_codes.get(class_name))\n",
    "                graph_name = region\n",
    "                data_list.append((pos_arr, feat_arr, node_arr, mask_arr, graph_label, graph_name))\n",
    "            except:\n",
    "                print(region)\n",
    "    return data_list\n",
    "\n",
    "def sample_weights(graph_train):\n",
    "    n_sample = []\n",
    "    for graph in graph_train:\n",
    "        n_sample.append(graph.y.numpy())\n",
    "    n_sample = np.asarray(n_sample)\n",
    "    _, counts = np.unique(n_sample, return_counts=True)\n",
    "    counts = counts.max() / (10e-3+counts)\n",
    "    return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd710312-a6b0-4d71-aff4-bd597820fe2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "distributed_codes = {\n",
    "    'normal': (0, 0),\n",
    "    'pancreatitis': (0, 1),\n",
    "    'pdac': (1, 1)\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a669b6",
   "metadata": {},
   "source": [
    "### Compute representations of patches of every ROI, save them as csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5851a6b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_path = 'data_bags/TMA'\n",
    "patch_ext = 'png'\n",
    "out_path = 'dataframes/TMA'\n",
    "resnet = models.resnet18(pretrained=True)\n",
    "resnet.fc = nn.Identity()\n",
    "resnet = resnet.cuda()\n",
    "resnet.eval()\n",
    "data_list = compute_dataset(resnet, bag_path, distributed_codes, ext='png')\n",
    "for data in tqdm(data_list):\n",
    "    pos = data[0]\n",
    "    feat = data[1]\n",
    "    label = data[5].split(os.sep)[-2]\n",
    "    name = os.sep.join(data[5].split(os.sep)[-2:])\n",
    "    save_name = os.path.join(out_path, name)\n",
    "    os.makedirs(os.sep.join(save_name.split(os.sep)[:-1]), exist_ok=True)\n",
    "    df = pd.DataFrame(data=feat)\n",
    "    df = df.assign(pos_x=pd.Series(pos[:, 0]).values)\n",
    "    df = df.assign(pos_y=pd.Series(pos[:, 1]).values)\n",
    "    df = df.assign(label=pd.Series([label]*feat.shape[0]).values)\n",
    "    df.to_csv(save_name+'.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e2144da",
   "metadata": {},
   "source": [
    "### Read representations of ROIs from csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3e5e2c6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 786/786 [02:43<00:00,  4.80it/s]\n"
     ]
    }
   ],
   "source": [
    "csv_path = 'dataframes/TMA'  \n",
    "num_feat = (1, 513) #512x1 vector\n",
    "csvs = glob(os.path.join(csv_path, '*', '*.csv'))\n",
    "train_graph_list = []\n",
    "fov_normal = []\n",
    "fov_pancreatitis = []\n",
    "fov_pdac = []\n",
    "for csv in tqdm(csvs):\n",
    "    data = read_dataset_to_graph(csv, distributed_codes, num_feat, compute_edges=True, weight_func=cosine_similarity, radius=1.5)\n",
    "    train_graph_list.append(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27182867",
   "metadata": {},
   "source": [
    "### Core level separation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d6a1355d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAqsAAAKaCAYAAAAZPRD5AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAABCYUlEQVR4nO3dP28c1R+o8Wc2yzoxSEihcyRuFIWGpKBIT5OGN0CBKKJINJT3BYBukQ6RF4BQXCAK3gANDT0STVL8FBRZEUlHpBReO/Z6zy1mQzBedmfnz87Zr5+PtEKRvId57BP7m/HsbJFSQpIkScrRoO8DkCRJkv6Lw6okSZKy5bAqSZKkbDmsSpIkKVsOq5IkScqWw6okSZKy5bAqSZKkbDmsSpIkKVsOq5IkScqWw6okSZKy5bAqSZKkbDmsSpIkKVsOq5IkScqWw6okSZKy5bAqSZKkbDmsSpIkKVsOq5IkScqWw6okSZKy5bAqSZKkbDmsSpIkKVsOq5IkScqWw6okSZKy5bAqSZKkbA37PgBJNRXFELgKXAQOgT1SmvR6THVE6QBbchSlQzrHipRS38cgqaqieA+4C9wBrgNHwJTytyRbwGNgF/ielF70c5AVROkAW3IUpUMS4LAqbYaiGAFfA/+X8ofu9oKPHlP+UP4W+H+kdNT9AVYUpQNsybElSoekUxxWpdwVxfvAL8AVFv/w/bcx8Ay4TUpPuzi0lUTpAFtKebVE6ZB0hsOqlLPyB/BvwGXgQo0VToAXwK1efxBH6QBbTsujJUqHpLkcVqVclb/SfAhco94P4NdOgCfADVI6buPQVhKlA2yZz/0lqVPeukrK19eUv9Js8gOY2fN3gK8aH1E9UTrAlnn6bonSIek/eGZVylH5auY/KW+305ZD4MpaX/0cpQNsWc79JakTnlmV8nSX8tXMbZrO1l2nKB1gyzLuL0md8MyqlKOieAR82MHKj0jpZgfrzhelA2ypxv0lqXUOq1Juynfc2QdGHax+BLy9lnfwidIBtlTn/pLUOodVKTdFcR34HXin7aUPhtt88enPPH/3attLn7Hzco/vfvqES5Nx62uvswNsqSrK/qIcgj8ipT+6WFzSarxmVcrPRdq/Dg+AaTFgNHnVxdJnjCavmBbdfItZZwfYUlWU/UV5G6s2X7QlqQGHVSk/h3T0d3OQphwNt7pY+oyj4RaD1MnMvdYOsKWqKPuL8jZWh10tLmk1XgYg5SbKtXhROsCW6txfklrnmVUpN+UPyK6ulXu8th/AUTrAlurcX5Ja57Aq5WkXaPuVI2PgQctrLrNLjA6wZRn3l6ROeBmAlKOiuAw8Y9PfmSdKB9iynPtLUic8syrlqPxB+S3tnTUaA9+s/QdwlA6wZTH3l6TOeGZVylVRjICHwDXKVyfXdQI8AW6Q0nEbh7aSKB1gy3zuL0md8syqlKuUjoDbwAvKH6R1nMyef7u3H8BROsCWs/pvidIh6T85rEo5S+kpcIvyjM+qv+rcnz3v1myd/kTpAFveyKclSoekuRxWpdyVP0BvAvcpX/yx7IfxePZx9yl/pZnHD+AoHWBLji1ROiSd4TWr0iYpX/18F7gDfHAw3B5NiwGDNOXSZHwEPKa87c6DrF8kEqUDbMlRlA5JgMOqtLmKYvj5Z78ejyavOBpu8cOPH7+1kTcyj9IBtuQoSod0jg37PgBJNaU0ef7l01N/7u9gGojSAbbkKEqHdI55zaokSZKy5bAqSZKkbDmsSpIkKVsOq5IkScqWw6okSZKy5bAqSZKkbDmsSpIkKVsOq5IkScqWw6okSZKy5bAqSZKkbDmsSpIkKVsOq5IkScqWw6okSZKy5bAqSZKkbDmsSpIkKVsOq5IkScqWw6okSZKy5bAqSZKkbDmsSpIkKVsOq5IkScqWw6okSZKy5bAqSZKkbDmsSpIkKVsOq5IkScrWsO8D0AYoiiFwFbgIHAJ7pDTp9ZjqCtay89mvjCavOBpuQfF/hhvZEqUDbMlRlA4I9/2LKC3qXJFS6vsYlKOieA+4C9wBrgNHwJTybPwW8BjYBb4npRf9HGRFgVsOhtujaTFgkKZcmoyP2ZSWKB1gS46idEDo719scovWK6Xkw8ebB4wS3EtwkGA/QVrw2J993L0Eo96P3Zb8W6J02JJnS5QOW/Jt8dHLwzOreqMo3gd+Aa4A2ys8cww8A26T0tMuDm1ltkBuLVE6wJZSXi1ROsCWUn4t6o3DqkrlN5TfgMvAhRornAAvgFu9f2Ox5Z/yaInSAbaclkdLlA6w5bR8WtQrh1VBUYyAh8A16n1Dee0EeALcIKXjNg5tZbbM029LlA6wZT73V1tsmaf/FvXOW1cJ4GvKX9E0+YbC7Pk7wFeNj6g+W87quyVKB9gyT98tUTrAlnlyaFHPPLN63pWvzvyT8vYhbTkErrDuV3Passz6W6J0gC3Lub+asGWZflqUBc+s6i7lrUPaNJ2tu262LNZHS5QOsGUZ91cztizWV4sy4JnV864oHgEfdrDyI1K62cG6/82WKtbbEqUDbKnG/VWXLVWsv0VZcFg9z8p3ENkHRh2sfgS8zbrekcSWqtbXEqUDbKnO/VWHLVWtt0XZcFg9z4riOvA78E7bSx8Mt/ni0595/u7Vtpeea+flHt/99AmXJuPW17alnigdYEtV7q96bKlsH/iIlP7oYnHly2tWz7eLtH9dEQDTYsBo8qqLpecaTV4xLbrZzrbUE6UDbKnK/VWPLZWd0O6LtrQhHFbPt0M62gODNOVouNXF0nMdDbcYpE7mbltqitIBtlTl/qrHlsouUP7c0jnjZQDnWaRri2ypymsK67ClKvdXHbZU5TWr55RnVs+z8i98V9f+PF7rNxRbqlpfS5QOsKU691cdtlS13hZlw2FVu0DbV8KPgQctr1nFLrYs0kfLLjE6wJZl3F/N7GLLIn21KANeBnDeFcVl4BkR3mnElmX6eIehGB1gy3LuryZsWcZ3sDrHPLN63pV/8b+lvX8Fj4FvevmGYssi/bRE6QBbFnN/NWXLIv21KAueWRUUxQh4CFyjfLVlXSfAE+AGKR23cWgrs2WefluidIAt87m/2mLLPP23qHeeWRWkdATcBl5QfmOo42T2/Nu9fkOx5d/6b4nSAbac1X9LlA6w5aw8WtQ7h1WVUnoK3KL8F+yqv7rZnz3v1mydftnyWj4tUTrAljfyaYnSAba8kVeLeuWwqjfKbwg3gfuUF7Mv++Yynn3cfcpf0eTzDcWW/FqidIAtObZE6QBbcm1Rb7xmVfOVr+a8C9wBPjgYbo+mxYBBmnJpMj4CHlPeRuRB9he925KfKB1gS46idIAtEg6rqqIohp9/9uvxaPKKo+EWP/z48Vsbe2NmW/ITpQNsyVGUDrBF59aw7wPQBkhp8vzLp6f+3N/BNGRLfqJ0gC05itIBtujc8ppVSZIkZcthVZIkSdlyWJUkSVK2HFYlSZKULYdVSZIkZcthVZIkSdlyWJUkSVK2HFYlSZKULYdVSZIkZcthVZIkSdlyWJUkSVK2HFYlSZKULYdVSZIkZcthVZIkSdlyWJUkSVK2HFYlSZKULYdVSZIkZcthVZIkSdlyWJUkSVK2HFYlSZKULYdVSZIkZcthVZIkSdlyWJUkSVK2HFa7UhRDiuI6RXFz9t9h34dUW1EMd17ucfWv/7Hzcg9bMhGlJUoH2JKjKB1gS64i/bzPVJFS6vsY4iiK94C7wB3gOnAETCn/UbAFPAZ2ge9J6UU/B1nRv1oOhtujaTFgkKZcmoyPsaUfUVqidIAtOYrSAbbkKtLP+02QUvLR9AGjBPcSHCTYT5AWPPZnH3cvwaj3Y7fFFjtsidISpcMWW3ycenhmtamieB/4BbgCbK/wzDHwDLhNSk+7OLSV2QK2dCdKB9hSyqslSgfYUrJFf3NYbaLcuL8Bl4ELNVY4AV4At3rfwLb8ky1ti9IBtpyWR0uUDrDlNFsEOKzWVxQj4CFwjXob97UT4Alwg5SO2zi0ldkyjy1tidIBtszn/mqLLfPYIu8G0MDXlL8KaLJxmT1/B/iq8RHVZ8tZtrQnSgfYMk/fLVE6wJZ5bJFnVmspXwX4J3CxxVUPgSus+1WDtixjSxNROsCW5dxfTdiyjC3nmGdW67lLeYuKNk1n666bLYvZ0kyUDrBlGfdXM7YsZss55pnVOoriEfBhBys/IqWbHaz732ypwpa6onSALdW4v+qypQpbzimH1VWV70yxD4w6WP0IeJuUJh2sfZYtVdlSR5QOsKU691cdtlRlyznlsLqqorgO/A680/bSB8Ntvvj0Z56/e7XtpefaebnHdz99wqXJuPW1bakvSkuUDrClKvdXPbZUE6mFcgj+iJT+6GLxaLxmdXUXaf/6FQCmxYDR5FUXS881mrxiWnSzBWypL0pLlA6wpSr3Vz22VBOphfI2Vm2+aCs0h9XVHdLR522QphwNt7pYeq6j4RaD1MncbUsDUVqidIAtVbm/6rGlmkgtlLexOuxq8Wi8DGBVka5hsaUqW+qI0gG2VOf+qsOWqmw5pzyzuqpyY3V1jcnjtW5cW6qypY4oHWBLde6vOmypypZzymG1nl2g7Suux8CDltesYhdbFrGlmV1idIAty7i/mtnFlkVsOce8DKCOorgMPCPCO1rYsowtTUTpAFuWc381Ycsytpxjnlmto9xg39Lev7bGwDe9bFxbFrGlqSgdYMti7q+mbFnElnPOM6t1FcUIeAhco3xVX10nwBPgBikdt3FoK7NlHlvaEqUDbJnP/dUWW+axRZ5ZrS2lI+A28IJyA9ZxMnv+7V43ri3/ZkubonSALWf13xKlA2w5yxYBDqvNpPQUuEX5L6VVf0WwP3verdk6/bLlNVu6EKUDbHkjn5YoHWDLG7bobw6rTZUb7yZwn/Ki6WWbeDz7uPuUvwrIZ+PaYkuXonSALTm2ROkAW2zRv3jNapvKVw3eBe4AHxwMt0fTYsAgTbk0GR8BjylvV/Eg+4urbclTlJYoHWBLjqJ0gC25itSyARxWu1IUw88/+/V4NHnF0XCLH378+K2NvQGwLXmK0hKlA2zJUZQOsCVXkVoyNez7AMJKafL8y6en/tzfwTRkS56itETpAFtyFKUDbMlVpJZMec2qJEmSsuWwKkmSpGw5rEqSJClbDquSJEnKlsOqJEmSsuWwKkmSpGw5rEqSJClbDquSJEnKlsOqJEmSsuWwKkmSpGw5rEqSJClbDquSJEnKlsOqJEmSsuWwKkmSpGw5rEqSJClbDquSJEnKlsOqJEmSsuWwKkmSpGw5rEqSJClbDquSJEnKlsOqJEmSsuWwKkmSpGw5rEqSJClbDqtdKYrhzss9rv71P3Ze7kFRDPs+pNpsyVOUligdYEuOonSALbmK1JKpIqXU9zHEURTvAXeBO8D1g+H2aFoMGKQplybjY+AxsAt8T0ov+jvQCmzJU5SWKB1gS46idIAtuYrUsglSSj6aPmCU4F6CgwT7CdKCx/7s4+4lGPV+7LbYYoctUVqidNhii49TD8+sNlUU7wO/AFeA7RWeOQaeAbdJ6WkXh7YyW8CW7kTpAFtKebVE6QBbSrbobw6rTZQb9zfgMnChxgonwAvgVu8b2JZ/sqVtUTrAltPyaInSAbacZosAh9X6imIEPASuUW/jvnYCPAFukNJxG4e2MlvmsaUtUTrAlvncX22xZR5b5N0AGvia8lcBTTYus+fvAF81PqL6bDnLlvZE6QBb5um7JUoH2DKPLfLMai3lqwD/BC62uOohcIV1v2rQlmVsaSJKB9iynPurCVuWseUc88xqPXeBactrTmfrrpsti9nSTJQOsGUZ91cztixmyznmmdU6iuIR8GEHKz8ipZsdrPvfbKnClrqidIAt1bi/6rKlClvOKYfVVZXvTLEPjDpY/Qh4m5QmHax9li1V2VJHlA6wpTr3Vx22VGXLOeWwuqqiuA78DrzT9tIHw22++PRnnr97te2l59p5ucd3P33Cpcm49bVtqS9KS5QOsKUq91c9tlQTqYVyCP6IlP7oYvFovGZ1dRdp//oVAKbFgNHkVRdLzzWavGJadLMFbKkvSkuUDrClKvdXPbZUE6mF8jZWbb5oKzSH1dUd0tHnbZCmHA23ulh6rqPhFoPUydxtSwNRWqJ0gC1Vub/qsaWaSC2Ut7E67GrxaLwMYFWRrmGxpSpb6ojSAbZU5/6qw5aqbDmnPLO6qnJjdXWNyeO1blxbqrKljigdYEt17q86bKnKlnPKYbWeXaDtK67HwIOW16xiF1sWsaWZXWJ0gC3LuL+a2cWWRWw5x7wMoI6iuAw8I8I7WtiyjC1NROkAW5ZzfzVhyzK2nGOeWa2j3GDf0t6/tsbAN71sXFsWsaWpKB1gy2Lur6ZsWcSWc84zq3UVxQh4CFyjfFVfXSfAE+AGKR23cWgrs2UeW9oSpQNsmc/91RZb5rFFnlmtLaUj4DbwgnID1nEye/7tXjeuLf9mS5uidIAtZ/XfEqUDbDnLFgEOq82k9BS4RfkvpVV/RbA/e96t2Tr9suU1W7oQpQNseSOfligdYMsbtuhvDqtNlRvvJnCf8qLpZZt4PPu4+5S/Cshn49piS5eidIAtObZE6QBbbNG/eM1qm8pXDd4F7gAfHAy3R9NiwCBNuTQZHwGPKW9X8SD7i6ttyVOUligdYEuOonSALbmK1LIBHFa7UhTDzz/79Xg0ecXRcIsffvz4rY29AbAteYrSEqUDbMlRlA6wJVeRWjI17PsAwkpp8vzLp6f+3N/BNGRLnqK0ROkAW3IUpQNsyVWklkx5zaokSZKy5bAqSZKkbDmsSpIkKVsOq5IkScqWw6okSZKy5bAqSZKkbDmsSpIkKVsOq5IkScqWw6okSZKy5bAqSZKkbDmsSpIkKVsOq5IkScqWw6okSZKy5bAqSZKkbDmsSpIkKVsOq5IkScqWw6okSZKy5bAqSZKkbDmsSpIkKVsOq5IkScqWw6okSZKy5bAqSZKkbDmsSpIkKVsOq5IkScqWw2pXimK483KPq3/9j52Xe1AUw74PqTZb8hSlJUoH2JKjKB1gS66itBTFkKK4TlHcnP03m44ipdT3McRRFO8Bd4E7wPWD4fZoWgwYpCmXJuNj4DGwC3xPSi/6O9AKbMlTlJYoHWBLjqJ0gC25itLyrw7gCJhSnszcIpeOlJKPpg8YJbiX4CDBfoK04LE/+7h7CUa9H7sttthhS5SWKB222GLHqYdnVpsqiveBX4ArwPYKzxwDz4DbpPS0i0NbmS1gS3eidIAtpbxaonSALSVburKBHQ6rTZRf8N+Ay8CFGiucAC+AW71vYFv+yZa2RekAW07LoyVKB9hymi1t29AOh9W6imIEPASuUe8L/toJ8AS4QUrHbRzaymyZx5a2ROkAW+Zzf7XFlnlsacsGd3g3gPq+pjyF3uQLzuz5O8BXjY+oPlvOsqU9UTrAlnn6bonSAbbMY0t7NrbDM6t1lK+e+xO42OKqh8AV1v1qO1uWsaWJKB1gy3LuryZsWcaWJja8wzOr9dylvLVDm6azddfNlsVsaSZKB9iyjPurGVsWs6WZje7wzGodRfEI+LCDlR+R0s0O1v1vtlRhS11ROsCWatxfddlShS11bXiHw+qqynd02AdGHax+BLxNSpMO1j7LlqpsqSNKB9hSnfurDluqsqWOAB0Oq6sqiuvA78A7bS99MNzmi09/5vm7V9teeq6dl3t899MnXJqMW1/blvqitETpAFuqcn/VY0s1ttTTZQflEPwRKf3RxeKvec3q6i7S/nUfAEyLAaPJqy6Wnms0ecW06GYL2FJflJYoHWBLVe6vemypxpZ6uuygvI1Vmy/amsthdXWHdPR5G6QpR8OtLpae62i4xSB1Mnfb0kCUligdYEtV7q96bKnGlnq67KC8jdVhV4u/5mUAqwpw7cffbKnKljqidIAt1bm/6rClKlvqCNDhmdVVlV+Qrq7NeLy2v4RgS3W21BGlA2ypzv1Vhy1V2VJHgA6H1Xp2gbavVB4DD1pes4pdbFnElmZ2idEBtizj/mpmF1sWsaWZXTa4w8sA6iiKy8AzNvSdIE6xZRlbmojSAbYs5/5qwpZlbGliwzs8s1pH+YX5lvb+lTIGvln7X0KwZTFbmorSAbYs5v5qypZFbGlqwzs8s1pXUYyAh8A1ylfD1XUCPAFukNJxG4e2MlvmsaUtUTrAlvncX22xZR5b2rLBHZ5ZrSulI+A28ILyC1fHyez5t3v7Swi2nGVLm6J0gC1n9d8SpQNsOcuWNm1wh8NqEyk9BW5R/gtj1VPr+7Pn3Zqt0y9bXrOlC1E6wJY38mmJ0gG2vGFLFza0w2G1qfILdhO4T3mx8bIv/nj2cfcpT6H3/5fwNVts6VKUDrAlx5YoHWCLLd3awA6vWW1T+Wq7u8Ad4IOD4fZoWgwYpCmXJuMj4DHlbR4e9HKh+CpsyVOUligdYEuOonSALbmK0rIhHQ6rXSmK4eef/Xo8mrziaLjFDz9+/BbrvJlxm2zJU5SWKB1gS46idIAtuYrSknHHsO8DCCulyfMvn576c38H05AteYrSEqUDbMlRlA6wJVdRWjLu8JpVSZIkZcthVZIkSdlyWJUkSVK2HFYlSZKULYdVSZIkZcthVZIkSdlyWJUkSVK2HFYlSZKULYdVSZIkZcthVZIkSdlyWJUkSVK2HFYlSZKULYdVSZIkZcthVZIkSdlyWJUkSVK2HFYlSZKULYdVSZIkZcthVZIkSdlyWJUkSVK2HFYlSZKULYdVSZIkZcthVZIkSdlyWJUkSVK28htWi2JIUVynKG7O/jvs+5BqKYrhzss9rv71P3Ze7rGxHWBLrqK0ROkAW3IUpQNsyVWUlow7ipRS38cARfEecBe4A1wHjoAp5TC9BTwGdoHvSelFPwdZwb86Dobbo2kxYJCmXJqMj9mUDrAlV1FaonSALTmK0gG25CpKy6Z0pJT6e8Aowb0EBwn2E6QFj/3Zx91LMOr1uKN22GKLHbZsckuUDltssePUo78zq0XxPvALcAXYXuGZY+AZcJuUnnZxaCuJ0gG2lGzpSpQOsKWUV0uUDrClZEtXNrCjn2G1/ET9BlwGLtRY4QR4Adzq9QsfpQNsOc2WtkXpAFtOy6MlSgfYcpotbdvQjvUPq0UxAh4C16j3iXrtBHgC3CCl4zYObSVROsCW+WxpS5QOsGU+91dbbJnHlrZscEcfdwP4mvLUc5NPFLPn7wBfNT6ieqJ0gC3z2NKeKB1gyzx9t0TpAFvmsaU9G9ux3jOr5avO/gQutrjqIXCFdb5KLUoH2LKcLU1E6QBblnN/NWHLMrY0seEd6z6zepfyllRtms7WXacoHWDLMrY0E6UDbFnG/dWMLYvZ0sxGd6z7zOoj4MMOVn5ESjc7WHe+KB1gSzW21BWlA2ypxv1Vly1V2FLXhnesb1gt3wlhHxh1sPoR8DYpTTpY+7QoHWBLdbbUEaUDbKnO/VWHLVXZUkeAjnUOq9eB34F32l76YLjNF5/+zPN3r7a99Bk7L/f47qdPuDQZt772OjvAlqpsqSdKB9hSlfurHluqsaWeLjsoh+CPSOmPLhZ/bZ3XrF6k/eslAJgWA0aTV10sfcZo8opp0c2nbZ0dYEtVttQTpQNsqcr9VY8t1dhST5cdlLexavNFW3Otc1g97Or/N0hTjoZbXSx9xtFwi0HqZOZeawfYUpUt9UTpAFuqcn/VY0s1ttTTZQflbawOu1r8Na9ZXVWUDrClOlvqiNIBtlTn/qrDlqpsqSNAx/rOrJYhXV3T8HhtmzdKB9hSnS11ROkAW6pzf9VhS1W21BGgY933Wd0F2r7Cdww8aHnNZXaJ0QG2LGNLM7vE6ABblnF/NbOLLYvY0swuG9yx7vusXgaesaHvoPC3KB1gy3K2NBGlA2xZzv3VhC3L2NLEhnes98xqGfQt7U33Y+CbtW/eKB1gy2K2NBWlA2xZzP3VlC2L2NLUhnes98wqQFGMgIfANcpXkdV1AjwBbpDScRuHtpIoHWDLfLa0JUoH2DKf+6sttsxjS1s2uGPd16xCSkfAbeAFZXAdJ7Pn3+5t80bpAFvOsqVNUTrAlrP6b4nSAbacZUubNrhj/cMqQEpPgVuUk/mqp6T3Z8+7NVunP1E6wJY3bOlClA6w5Y18WqJ0gC1v2NKFDe3oZ1iF15+wm8B9yot0l33SxrOPu0956rn/zQtxOsAWW7oVpQNsybElSgfYYku3NrBj/deszj2K4jJwF7gDfHAw3B5NiwGDNOXSZHwEPKa8PcKDXi6wripKB9iSqygtUTrAlhxF6QBbchWlZUM68hhW/6kohp9/9uvxaPKKo+EWP/z48Vus8ybAbYnSAbbkKkpLlA6wJUdROsCWXEVpybhj2PcBnJHS5PmXT0/9ub+DaSBKB9iSqygtUTrAlhxF6QBbchWlJeOO/q5ZlSRJkpZwWJUkSVK2HFYlSZKULYdVSZIkZcthVZIkSdlyWJUkSVK2HFYlSZKULYdVSZIkZcthVZIkSdlyWJUkSVK2HFYlSZKULYdVSZIkZcthVZIkSdlyWJUkSVK2HFYlSZKULYdVSZIkZcthVZIkSdlyWJUkSVK2HFYlSZKULYdVSZIkZcthVZIkSdlyWJUkSVK2HFYlSZKULYdVSZIkZSu/YbUohjsv97j61//YebkHRTHs+5BqidIBtuQqSkuUDrAlR1E6wJZcRWnJuKNIKfV9DFAU7wF3gTvA9YPh9mhaDBikKZcm42PgMbALfE9KL/o70CWidIAtuYrSEqUDbMlRlA6wJVdRWjalI6XU3wNGCe4lOEiwnyAteOzPPu5eglGvxx21wxZb7LBlk1uidNhiix2nHv2dWS2K94FfgCvA9grPHAPPgNuk9LSLQ1tJlA6wpWRLV6J0gC2lvFqidIAtJVu6soEd/Qyr5SfqN+AycKHGCifAC+BWr1/4KB1gy2m2tC1KB9hyWh4tUTrAltNsaduGdqx/WC2KEfAQuEa9T9RrJ8AT4AYpHbdxaCuJ0gG2zGdLW6J0gC3zub/aYss8trRlgzv6uBvA15Snnpt8opg9fwf4qvER1ROlA2yZx5b2ROkAW+bpuyVKB9gyjy3t2diO9Z5ZLV919idwscVVD4ErrPNValE6wJblbGkiSgfYspz7qwlblrGliQ3vWPeZ1bvAtOU1p7N11ylKB9iyjC3NROkAW5ZxfzVjy2K2NLPRHes+s/oI+LCDlR+R0s0O1p0vSgfYUo0tdUXpAFuqcX/VZUsVttS14R3rG1bLd0LYB0YdrH4EvE1Kkw7WPi1KB9hSnS11ROkAW6pzf9VhS1W21BGgY53D6nXgd+Cdtpc+GG7zxac/8/zdq20vfcbOyz2+++kTLk3Gra+9zg6wpSpb6onSAbZU5f6qx5ZqbKmnyw7KIfgjUvqji8VfW+c1qxdp/3oJAKbFgNHkVRdLnzGavGJadPNpW2cH2FKVLfVE6QBbqnJ/1WNLNbbU02UH5W2s2nzR1lzrHFYPu/r/DdKUo+FWF0ufcTTcYpA6mbnX2gG2VGVLPVE6wJaq3F/12FKNLfV02UF5G6vDrhZ/zWtWVxWlA2ypzpY6onSALdW5v+qwpSpb6gjQsb4zq2VIV9c0PF7b5o3SAbZUZ0sdUTrAlurcX3XYUpUtdQToWPd9VneBtq/wHQMPWl5zmV1idIAty9jSzC4xOsCWZdxfzexiyyK2NLPLBnes+z6rl4FnbOg7KPwtSgfYspwtTUTpAFuWc381YcsytjSx4R3rPbNaBn1Le9P9GPhm7Zs3SgfYspgtTUXpAFsWc381ZcsitjS14R3rPbMKUBQj4CFwjfJVZHWdAE+AG6R03MahrSRKB9gyny1tidIBtszn/mqLLfPY0pYN7lj3NauQ0hFwG3hBGVzHyez5t3vbvFE6wJazbGlTlA6w5az+W6J0gC1n2dKmDe5Y/7AKkNJT4BblZL7qKen92fNuzdbpT5QOsOUNW7oQpQNseSOfligdYMsbtnRhQzv6GVbh9SfsJnCf8iLdZZ+08ezj7lOeeu5/80KcDrDFlm5F6QBbcmyJ0gG22NKtDexY/zWrc4+iuAzcBe4AHxwMt0fTYsAgTbk0GR8Bjylvj/Cglwusq4rSAbbkKkpLlA6wJUdROsCWXEVp2ZCOPIbVfyqK4eef/Xo8mrziaLjFDz9+/BbrvAlwW6J0gC25itISpQNsyVGUDrAlV1FaMu4Y9n0AZ6Q0ef7l01N/7u9gGojSAbbkKkpLlA6wJUdROsCWXEVpybijv2tWJUmSpCUcViVJkpQth1VJkiRly2FVkiRJ2XJYlSRJUrYcViVJkpQth1VJkiRly2FVkiRJ2XJYlSRJUrYcViVJkpQth1VJkiRly2FVkiRJ2XJYlSRJUrYcViVJkpQth1VJkiRly2FVkiRJ2XJYlSRJUrYcViVJkpQth1VJkiRly2FVkiRJ2XJYlSRJUrYcViVJkpQth1VJkiRlK79htSiGOy/3uPrX/9h5uQdFMez7kGqJ0gG25CpKS5QOsCVHUTrAllxFacm4o0gp9X0MUBTvAXeBO8D1g+H2aFoMGKQplybjY+AxsAt8T0ov+jvQJaJ0gC25itISpQNsyVGUDrAlV1FaNqUjpdTfA0YJ7iU4SLCfIC147M8+7l6CUa/HHbXDFlvssGWTW6J02GKLHace/Z1ZLYr3gV+AK8D2Cs8cA8+A26T0tItDW0mUDrClZEtXonSALaW8WqJ0gC0lW7qygR39DKvlJ+o34DJwocYKJ8AL4FavX/goHWDLaba0LUoH2HJaHi1ROsCW02xp24Z2rH9YLYoR8BC4Rr1P1GsnwBPgBikdt3FoK4nSAbbMZ0tbonSALfO5v9piyzy2tGWDO/q4G8DXlKeem3yimD1/B/iq8RHVE6UDbJnHlvZE6QBb5um7JUoH2DKPLe3Z2I71nlktX3X2J3CxxVUPgSus81VqUTrAluVsaSJKB9iynPurCVuWsaWJDe9Y95nVu8C05TWns3XXKUoH2LKMLc1E6QBblnF/NWPLYrY0s9Ed6z6z+gj4sIOVH5HSzQ7WnS9KB9hSjS11RekAW6pxf9VlSxW21LXhHesbVst3QtgHRh2sfgS8TUqTDtY+LUoH2FKdLXVE6QBbqnN/1WFLVbbUEaBjncPqdeB34J22lz4YbvPFpz/z/N2rbS99xs7LPb776RMuTcatr73ODrClKlvqidIBtlTl/qrHlmpsqafLDsoh+CNS+qOLxV9b5zWrF2n/egkApsWA0eRVF0ufMZq8Ylp082lbZwfYUpUt9UTpAFuqcn/VY0s1ttTTZQflbazafNHWXOscVg+7+v8N0pSj4VYXS59xNNxikDqZudfaAbZUZUs9UTrAlqrcX/XYUo0t9XTZQXkbq8OuFn/Na1ZXFaUDbKnOljqidIAt1bm/6rClKlvqCNCxvjOrZUhX1zQ8XtvmjdIBtlRnSx1ROsCW6txfddhSlS11BOhY931Wd4G2r/AdAw9aXnOZXWJ0gC3L2NLMLjE6wJZl3F/N7GLLIrY0s8sGd6z7PquXgWds6Dso/C1KB9iynC1NROkAW5ZzfzVhyzK2NLHhHes9s1oGfUt70/0Y+GbtmzdKB9iymC1NRekAWxZzfzVlyyK2NLXhHes9swpQFCPgIXCN8lVkdZ0AT4AbpHTcxqGtJEoH2DKfLW2J0gG2zOf+aost89jSlg3uWPc1q5DSEXAbeEEZXMfJ7Pm3e9u8UTrAlrNsaVOUDrDlrP5bonSALWfZ0qYN7lj/sAqQ0lPgFuVkvuop6f3Z827N1ulPlA6w5Q1buhClA2x5I5+WKB1gyxu2dGFDO/oZVuH1J+wmcJ/yIt1ln7Tx7OPuU5567n/zQpwOsMWWbkXpAFtybInSAbbY0q0N7Fj/Natzj6K4DNwF7gAfAMeUp5ovAG8Bjylvj/Cglwusq/pXx8FwezQtBgzSlEuT8RGb0gG25CpKS5QOsCVHUTrAllxFadmQjjyG1X8q32nhKuXtFQ6BPdZ5E+C2FMXw889+PR5NXnE03OKHHz9+ayM7wJZcRWmJ0gG25ChKB9iSqygtGXcM+z6AM7p9p4X1SWny/Munp/7c38E0ZEueorRE6QBbchSlA2zJVZSWjDv6u2ZVkiRJWsJhVZIkSdlyWJUkSVK2HFYlSZKULYdVSZIkZcthVZIkSdlyWJUkSVK2HFYlSZKULYdVSZIkZcthVZIkSdlyWJUkSVK2HFYlSZKULYdVSZIkZcthVZIkSdlyWJUkSVK2HFYlSZKULYdVSZIkZcthVZIkSdlyWJUkSVK2HFYlSZKULYdVSZIkZcthVZIkSdlyWJUkSVK2HFa7UhTDnZd7XP3rf+y83IOiGPZ9SLXZkqcoLVE6wJYcRekAW3IVpSXjjiKl1PcxxFEU7wF3gTvA9YPh9mhaDBikKZcm42PgMbALfE9KL/o70ApsyVOUligdYEuOonSALbmK0rIpHSklH00fMEpwL8FBgv0EacFjf/Zx9xKMej92W2yxw5YoLVE6bLHFjlMPz6w2VRTvA78AV4DtFZ45Bp4Bt0npaReHtjJbwJbuROkAW0p5tUTpAFtKtnRlAzscVpsov+C/AZeBCzVWOAFeALd638C2/JMtbYvSAbaclkdLlA6w5TRb2rahHQ6rdRXFCHgIXKPeF/y1E+AJcIOUjts4tJXZMo8tbYnSAbbM5/5qiy3z2NKWDe7wbgD1fU15Cr3JF5zZ83eArxofUX22nGVLe6J0gC3z9N0SpQNsmceW9mxsh2dW6yhfPfcncLHFVQ+BK6z71Xa2LGNLE1E6wJbl3F9N2LKMLU1seIdnVuu5C0xbXnM6W3fdbFnMlmaidIAty7i/mrFlMVua2egOz6zWURSPgA87WPkRKd3sYN3/ZksVttQVpQNsqcb9VZctVdhS14Z3OKyuqnxHh31g1MHqR8DbpDTpYO2zbKnKljqidIAt1bm/6rClKlvqCNDhsLqqorgO/A680/bSB8Ntvvj0Z56/e7XtpefaebnHdz99wqXJuPW1bakvSkuUDrClKvdXPbZUY0s9XXZQDsEfkdIfXSz+mtesru4i7V/3AcC0GDCavOpi6blGk1dMi262gC31RWmJ0gG2VOX+qseWamypp8sOyttYtfmirbkcVld3SEeft0GacjTc6mLpuY6GWwxSJ3O3LQ1EaYnSAbZU5f6qx5ZqbKmnyw7K21gddrX4a14GsKoA1378zZaqbKkjSgfYUp37qw5bqrKljgAdnlldVfkF6erajMdr+0sItlRnSx1ROsCW6txfddhSlS11BOhwWK1nF2j7SuUx8KDlNavYxZZFbGlmlxgdYMsy7q9mdrFlEVua2WWDO7wMoI6iuAw8Y0PfCeIUW5axpYkoHWDLcu6vJmxZxpYmNrzDM6t1lF+Yb2nvXylj4Ju1/yUEWxazpakoHWDLYu6vpmxZxJamNrzDM6t1FcUIeAhco3w1XF0nwBPgBikdt3FoK7NlHlvaEqUDbJnP/dUWW+axpS0b3OGZ1bpSOgJuAy8ov3B1nMyef7u3v4Rgy1m2tClKB9hyVv8tUTrAlrNsadMGdzisNpHSU+AW5b8wVj21vj973q3ZOv2y5TVbuhClA2x5I5+WKB1gyxu2dGFDOxxWmyq/YDeB+5QXGy/74o9nH3ef8hR6/38JX7PFli5F6QBbcmyJ0gG22NKtDezwmtU2la+2uwvcAT44GG6PpsWAQZpyaTI+Ah5T3ubhQS8Xiq/CljxFaYnSAbbkKEoH2JKrKC0b0uGw2pWiGH7+2a/Ho8krjoZb/PDjx2+xzpsZt8mWPEVpidIBtuQoSgfYkqsoLRl3DPs+gLBSmjz/8umpP/d3MA3ZkqcoLVE6wJYcRekAW3IVpSXjDq9ZlSRJUrYcViVJkpQth1VJkiRly2FVkiRJ2XJYlSRJUrYcViVJkpQth1VJkiRly2FVkiRJ2XJYlSRJUrYcViVJkpQth1VJkiRly2FVkiRJ2XJYlSRJUrYcViVJkpQth1VJkiRly2FVkiRJ2XJYlSRJUrYcViVJkpQth1VJkiRly2FVkiRJ2XJYlSRJUrYcViVJkpQth1VJkiRly2FVkiRJ2XJY7UpRDHde7nH1r/+x83IPimLY9yHVZkueorRE6QBbchSlA2zJVZSWjDuKlFLfxxBHUbwH3AXuANcPhtujaTFgkKZcmoyPgcfALvA9Kb3o70ArsCVPUVqidIAtOYrSAbbkKkrLpnSklHw0fcAowb0EBwn2E6QFj/3Zx91LMOr92G2xxQ5borRE6bDFFjtOPTyz2lRRvA/8AlwBtld45hh4BtwmpaddHNrKbAFbuhOlA2wp5dUSpQNsKdnSlQ3scFhtovyC/wZcBi7UWOEEeAHc6n0D2/JPtrQtSgfYcloeLVE6wJbTbGnbhnY4rNZVFCPgIXCNel/w106AJ8ANUjpu49BWZss8trQlSgfYMp/7qy22zGNLWza4w7sB1Pc15Sn0Jl9wZs/fAb5qfET12XKWLe2J0gG2zNN3S5QOsGUeW9qzsR2eWa2jfPXcn8DFFlc9BK6w7lfb2bKMLU1E6QBblnN/NWHLMrY0seEdnlmt5y4wbXnN6WzddbNlMVuaidIBtizj/mrGlsVsaWajOzyzWkdRPAI+7GDlR6R0s4N1/5stVdhSV5QOsKUa91ddtlRhS10b3uGwuqryHR32gVEHqx8Bb5PSpIO1z7KlKlvqiNIBtlTn/qrDlqpsqSNAh8PqqoriOvA78E7bSx8Mt/ni0595/u7Vtpeea+flHt/99AmXJuPW17alvigtUTrAlqrcX/XYUo0t9XTZQTkEf0RKf3Sx+Gtes7q6i7R/3QcA02LAaPKqi6XnGk1eMS262QK21BelJUoH2FKV+6seW6qxpZ4uOyhvY9Xmi7bmclhd3SEdfd4GacrRcKuLpec6Gm4xSJ3M3bY0EKUlSgfYUpX7qx5bqrGlni47KG9jddjV4q95GcCqAlz78TdbqrKljigdYEt17q86bKnKljoCdHhmdVXlF6SrazMer+0vIdhSnS11ROkAW6pzf9VhS1W21BGgw2G1nl2g7SuVx8CDltesYhdbFrGlmV1idIAty7i/mtnFlkVsaWaXDe7wMoA6iuIy8IwNfSeIU2xZxpYmonSALcu5v5qwZRlbmtjwDs+s1lF+Yb6lvX+ljIFv1v6XEGxZzJamonSALYu5v5qyZRFbmtrwDs+s1lUUI+AhcI3y1XB1nQBPgBukdNzGoa3MlnlsaUuUDrBlPvdXW2yZx5a2bHCHZ1brSukIuA28oPzC1XEye/7t3v4Sgi1n2dKmKB1gy1n9t0TpAFvOsqVNG9zhsNpESk+BW5T/wlj11Pr+7Hm3Zuv0y5bXbOlClA6w5Y18WqJ0gC1v2NKFDe1wWG2q/ILdBO5TXmy87Is/nn3cfcpT6P3/JXzNFlu6FKUDbMmxJUoH2GJLtzaww2tW21S+2u4ucAf44GC4PZoWAwZpyqXJ+Ah4THmbhwe9XCi+ClvyFKUlSgfYkqMoHWBLrqK0/KsDOKb8Vf8F4C0y6XBY7UpRDD//7Nfj0eQVR8Mtfvjx47dY582M22RLnqK0ROkAW3IUpQNsyVWUlvKdrq5S3t7qENjLpWPY9wGEldLk+ZdPT/25v4NpyJY8RWmJ0gG25ChKB9iSqygt3b7TVSNesypJkqRsOaxKkiQpWw6rkiRJypbDqiRJkrLlsCpJkqRsOaxKkiQpWw6rkiRJypbDqiRJkrLlsCpJkqRsOaxKkiQpWw6rkiRJypbDqiRJkrLlsCpJkqRsOaxKkiQpWw6rkiRJypbDqiRJkrLlsCpJkqRsOaxKkiQpWw6rkiRJypbDqiRJkrLlsCpJkqRsOaxKkiQpWw6rkiRJypbDaleKYrjzco+rf/2PnZd7UBTDvg+pNlvyFKUlSgfYkqMoHWBLriK1ZKpIKfV9DHEUxXvAXeAOcP1guD2aFgMGacqlyfgYeAzsAt+T0ov+DrQCW/IUpSVKB9iSoygdYEuuIrVsgpSSj6YPGCW4l+AgwX6CtOCxP/u4ewlGvR+7LbbYYUuUligdttji49TDM6tNFcX7wC/AFWB7hWeOgWfAbVJ62sWhrcwWsKU7UTrAllJeLVE6wJaSLfqbw2oT5cb9DbgMXKixwgnwArjV+wa25Z9saVuUDrDltDxaonSALafZIsBhtb6iGAEPgWvU27ivnQBPgBukdNzGoa3MlnlsaUuUDrBlPvdXW2yZxxZ5N4AGvqb8VUCTjcvs+TvAV42PqD5bzrKlPVE6wJZ5+m6J0gG2zGOLPLNaS/kqwD+Biy2ueghcYd2vGrRlGVuaiNIBtizn/mrClmVsOcc8s1rPXWDa8prT2brrZstitjQTpQNsWcb91Ywti9lyjnlmtY6ieAR82MHKj0jpZgfr/jdbqrClrigdYEs17q+6bKnClnPKYXVV5TtT7AOjDlY/At4mpUkHa59lS1W21BGlA2ypzv1Vhy1V2XJOOayuqiiuA78D77S99MFwmy8+/Znn715te+m5dl7u8d1Pn3BpMm59bVvqi9ISpQNsqcr9VY8t1URqoRyCPyKlP7pYPBqvWV3dRdq/fgWAaTFgNHnVxdJzjSavmBbdbAFb6ovSEqUDbKnK/VWPLdVEaqG8jVWbL9oKzWF1dYd09HkbpClHw60ulp7raLjFIHUyd9vSQJSWKB1gS1Xur3psqSZSC+VtrA67WjwaLwNYVaRrWGypypY6onSALdW5v+qwpSpbzinPrK6q3FhdXWPyeK0b15aqbKkjSgfYUp37qw5bqrLlnHJYrWcXaPuK6zHwoOU1q9jFlkVsaWaXGB1gyzLur2Z2sWURW84xLwOooyguA8+I8I4WtixjSxNROsCW5dxfTdiyjC3nmGdW6yg32Le096+tMfBNLxvXlkVsaSpKB9iymPurKVsWseWc88xqXUUxAh4C1yhf1VfXCfAEuEFKx20c2spsmceWtkTpAFvmc3+1xZZ5bJFnVmtL6Qi4Dbyg3IB1nMyef7vXjWvLv9nSpigdYMtZ/bdE6QBbzrJFgMNqMyk9BW5R/ktp1V8R7M+ed2u2Tr9sec2WLkTpAFveyKclSgfY8oYt+pvDalPlxrsJ3Ke8aHrZJh7PPu4+5a8C8tm4ttjSpSgdYEuOLVE6wBZb9C9es9qm8lWDd4E7wAfAMeWp/wvAW8BjyttVPMj+4up/tRwMt0fTYsAgTbk0GR9hSz+itETpAFtyFKUDbMlVpJ/3G8BhtSvlO19cpbzdxSGwt7E3AC6K4eef/Xo8mrziaLjFDz9+/JYtGYjSEqUDbMlRlA6wJVeRft5natj3AYTV7TtfrFdKk+dfPj315/4OpiFb8hOlA2zJUZQOsCVXkX7eZ8prViVJkpQth1VJkiRly2FVkiRJ2XJYlSRJUrYcViVJkpQth1VJkiRly2FVkiRJ2XJYlSRJUrYcViVJkpQth1VJkiRly2FVkiRJ2XJYlSRJUrYcViVJkpQth1VJkiRly2FVkiRJ2XJYlSRJUrYcViVJkpQth1VJkiRly2FVkiRJ2XJYlSRJUrYcViVJkpQth1VJkiRly2FVkiRJ2XJYlSRJUrYcVrVcUQx3Xu5x9a//sfNyD4pi2Pch1WZLfqJ0gC05itIBtujcKlJKfR+DclQU7wF3gTvA9YPh9mhaDBikKZcm42PgMbALfE9KL/o70ApsyU+UDrAlR1E6wBYJIKXkw8ebB4wS3EtwkGA/QVrw2J993L0Eo96P3Zb8W6J02JJnS5QOW/Jt8dHLwzOreqMo3gd+Aa4A2ys8cww8A26T0tMuDm1ltkBuLVE6wJZSXi1ROsCWUn4t6o3DqkrlN5TfgMvAhRornAAvgFu9f2Ox5Z/yaInSAbaclkdLlA6w5bR8WtQrh1VBUYyAh8A16n1Dee0EeALcIKXjNg5tZbbM029LlA6wZT73V1tsmaf/FvXOuwEI4GvKX9E0+YbC7Pk7wFeNj6g+W87quyVKB9gyT98tUTrAlnlyaFHPPLN63pWvzvwTuNjiqofAFdb9ak5blll/S5QOsGU591cTtizTT4uy4JlV3QWmLa85na27brYs1kdLlA6wZRn3VzO2LNZXizLgmdXzrigeAR92sPIjUrrZwbr/zZYq1tsSpQNsqcb9VZctVay/RVlwWD3PyncM2QdGHax+BLxNSpMO1j7LlqrW1xKlA2ypzv1Vhy1VrbdF2XBYPc+K4jrwO/BO20sfDLf54tOfef7u1baXnmvn5R7f/fQJlybj1te2pZ4oHWBLVe6vemypbB/4iJT+6GJx5ctrVs+3i7R/XREA02LAaPKqi6XnGk1eMS262c621BOlA2ypyv1Vjy2VndDui7a0IRxWz7dDOtoDgzTlaLjVxdJzHQ23GKRO5m5baorSAbZU5f6qx5bKLlD+3NI542UA51mka4tsqcprCuuwpSr3Vx22VOU1q+eUZ1bPs/IvfFfX/jxe6zcUW6paX0uUDrClOvdXHbZUtd4WZcNhVbtA21fCj4EHLa9ZxS62LNJHyy4xOsCWZdxfzexiyyJ9tSgDXgZw3hXFZeAZEd5pxJZl+niHoRgdYMty7q8mbFnGd7A6xzyzet6Vf/G/pb1/BY+Bb3r5hmLLIv20ROkAWxZzfzVlyyL9tSgLnlkVFMUIeAhco3y1ZV0nwBPgBikdt3FoK7Nlnn5bonSALfO5v9piyzz9t6h3nlkVpHQE3AZeUH5jqONk9vzbvX5DseXf+m+J0gG2nNV/S5QOsOWsPFrUO4dVlVJ6Ctyi/Bfsqr+62Z8979ZsnX7Z8lo+LVE6wJY38mmJ0gG2vJFXi3rlsKo3ym8IN4H7lBezL/vmMp593H3KX9Hk8w3FlvxaonSALTm2ROkAW3JtUW+8ZlXzla/mvAvcAT4Ajil/JXMBeAt4THkbkQfZX/T+r5aD4fZoWgwYpCmXJuMjbFm/KB1gS46idEDoFjb554rWymFVy5XvSHKV8jYkh8Dext6YuSiGn3/26/Fo8oqj4RY//PjxW7b0LEoH2JKjKB0QroUoP1fUuWHfB6AN0O07kqxXSpPnXz499ef+DqahKC1ROsCWHEXpgHAtRPm5os55zaokSZKy5bAqSZKkbDmsSpIkKVsOq5IkScqWw6okSZKy5bAqSZKkbDmsSpIkKVsOq5IkScqWw6okSZKy5bAqSZKkbDmsSpIkKVsOq5IkScqWw6okSZKy5bAqSZKkbDmsSpIkKVsOq5IkScqWw6okSZKy5bAqSZKkbDmsSpIkKVsOq5IkScqWw6okSZKy5bAqSZKkbDmsSpIkKVsOqzpfimK483KPq3/9j52Xe1AUw74PqbYoLVE6wJYcRemAWC3SCoqUUt/HIHWrKN4D7gJ3gOsHw+3RtBgwSFMuTcbHwGNgF/ielF70d6AVRGmJ0gG25ChKB8RqkWpyWFVcRTECvgb+LzAFthd89JjyNw3fAv+PlI66P8AVRGmJ0gG25NgSpQNitUgNOawqpqJ4H/gFuMLib/L/NgaeAbdJ6WkXh7ayKC1ROsCWUl4tUTogVovUAodVxVN+o/8NuAxcqLHCCfACuNX7N/woLVE6wJbT8miJ0gGxWqSWOKwqlvJXZw+Ba9T7Rv/aCfAEuEFKx20c2sqitETpAFvmc3+1JVKL1CLvBqBovqb81VmTb/TMnr8DfNX4iOqL0hKlA2yZp++WKB0Qq0VqjWdWFUf5qtk/gYstrnoIXFn7q2yjtETpAFuWc381EalFaplnVhXJXcpXzbZpOlt33aK0ROkAW5ZxfzUTqUVqlWdWFUdRPAI+7GDlR6R0s4N1/1uUligdYEs17q+6IrVILXNYVQzlO7nsA6MOVj8C3ialSQdrnxWlJUoH2FKd+6uOSC1SBxxWFUNRXAd+B95pe+mD4TZffPozz9+92vbSc+283OO7nz7h0mTc+trrbInSAbZU5f6qp8sWyiH4I1L6o4vFpXXwmlVFcZH2r/cCYFoMGE1edbH0XKPJK6ZFN38119kSpQNsqcr9VU+XLZS3sWrzRVvS2jmsKopDOtrPgzTlaLjVxdJzHQ23GKRO5u61tkTpAFuqcn/V02UL5W2sDrtaXFoHLwNQDJGu+YrSEqUDbKnO/VVHpBapA55ZVQzlN+Kursl6vNZv9FFaonSALdW5v+qI1CJ1wGFVkewCbb9CYQw8aHnNKnaJ0bJLjA6wZRn3VzO7xGmRWuVlAIqjKC4Dz4jwDjBRWqJ0gC3Lub+aiNQitcwzq4qj/Ib8Le2dnRgD3/TyjT5KS5QOsGUx91dTkVqklnlmVbEUxQh4CFyjfBVsXSfAE+AGKR23cWgri9ISpQNsmc/91ZZILVKLPLOqWFI6Am4DLyi/YddxMnv+7V6/0UdpidIBtpzVf0uUDojVIrXIYVXxpPQUuEV5ZmHVX6ntz553a7ZOv6K0ROkAW97IpyVKB8RqkVrisKqYym/UN4H7lC8yWPZNfzz7uPuUvzrL5xt9lJYoHWBLji1ROiBWi9QCr1lVfOWrbO8Cd4APgGPKX5VdAN4CHlPe3uVB9i9GiNISpQNsyVGUDojVItXksKrzpXynmKuUt4c5BPY29obZUVqidIAtOYrSAbFapBU4rEqSJClbXrMqSZKkbDmsSpIkKVsOq5IkScqWw6okSZKy5bAqSZKkbDmsSpIkKVsOq5IkScqWw6okSZKy5bAqSZKkbDmsSpIkKVsOq5IkScqWw6okSZKy5bAqSZKkbDmsSpIkKVsOq5IkScqWw6okSZKy5bAqSZKkbDmsSpIkKVsOq5IkScqWw6okSZKy5bAqSZKkbDmsSpIkKVsOq5IkScrW/wfRRSlKu9Va7gAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 864x864 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_fold = 4\n",
    "while(True):\n",
    "    weight_mean = max(sample_weights(train_graph_list))\n",
    "    shuffle(train_graph_list)\n",
    "    cores_per_chunk = int(len(train_graph_list)/num_fold)\n",
    "    chunks_data = [train_graph_list[i*cores_per_chunk:(i+1)*cores_per_chunk] for i in range(num_fold)]\n",
    "    weights = []\n",
    "    for chunk_data in chunks_data:\n",
    "        weights.append(sample_weights(chunk_data))\n",
    "    max_weight = max([max(i) for i in weights])\n",
    "    if max_weight<weight_mean*1.2: break\n",
    "fig = visualize_points(train_graph_list[0].pos, edge_index=train_graph_list[0].edge_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d00191a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_label_roc(labels, predictions, pos_label=1):\n",
    "    num_class = labels.shape[1]\n",
    "    fprs = []\n",
    "    tprs = []\n",
    "    thresholds = []\n",
    "    thresholds_optimal = []\n",
    "    aucs = []\n",
    "    if len(predictions.shape)==1:\n",
    "        predictions = predictions[:, None]\n",
    "    for c in range(0, num_class):\n",
    "        label = labels[:, c]\n",
    "        prediction = predictions[:, c]\n",
    "        fpr, tpr, threshold = roc_curve(label, prediction, pos_label=pos_label)\n",
    "        fpr_optimal, tpr_optimal, threshold_optimal = optimal_thresh(fpr, tpr, threshold)\n",
    "        c_auc = roc_auc_score(label, prediction)\n",
    "        aucs.append(c_auc)\n",
    "        thresholds.append(threshold)\n",
    "        thresholds_optimal.append(threshold_optimal)\n",
    "    return aucs, thresholds, thresholds_optimal\n",
    "\n",
    "def optimal_thresh(fpr, tpr, thresholds, p=0):\n",
    "    loss = (fpr - tpr) - p * tpr / (fpr + tpr + 1)\n",
    "    idx = np.argmin(loss, axis=0)\n",
    "    return fpr[idx], tpr[idx], thresholds[idx]\n",
    "\n",
    "def accuracy_from_thresh(labels, predictions, thresh, priority_func=None): # numpy array, 2D, 2D, 1D\n",
    "    labels = copy.deepcopy(labels)\n",
    "    predictions = copy.deepcopy(predictions)\n",
    "    num_class = labels.shape[1]\n",
    "    if num_class==1:\n",
    "        class_prediction_bag = copy.deepcopy(predictions)\n",
    "        class_prediction_bag[predictions>=thresh[0]] = 1\n",
    "        class_prediction_bag[predictions<thresh[0]] = 0\n",
    "        predictions = class_prediction_bag\n",
    "    else:        \n",
    "        for i in range(num_class):\n",
    "            class_prediction_bag = copy.deepcopy(predictions[:, i])\n",
    "            class_prediction_bag[predictions[:, i]>=thresh[i]] = 1\n",
    "            class_prediction_bag[predictions[:, i]<thresh[i]] = 0\n",
    "            predictions[:, i] = class_prediction_bag\n",
    "    if priority_func is not None:\n",
    "        predictions = priority_func(predictions)\n",
    "        labels = priority_func(labels)\n",
    "        # print(predictions)\n",
    "        # print(labels)\n",
    "    # zeros = np.zeros((labels.shape[0], 1))\n",
    "    # labels = np.argmax(np.concatenate((zeros, labels), 1), 1)\n",
    "    # predictions = np.argmax(np.concatenate((zeros, predictions), 1), 1)\n",
    "    acc = balanced_accuracy_score(labels, predictions)\n",
    "    f1 = f1_score(labels, predictions, average='weighted')\n",
    "    return acc, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d2db9d53-7fce-43b2-adb2-48ca6a1295fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(loader):\n",
    "    model.train()\n",
    "    loss_avg = 0\n",
    "    count = 0\n",
    "    for data in loader:  \n",
    "        optimizer.zero_grad()  \n",
    "        out = model(\n",
    "            data.x.float().cuda(), \n",
    "            edge_index=data.edge_index.cuda(), \n",
    "            batch=torch.LongTensor(np.zeros(data.x.shape[0])).cuda(), \n",
    "            edge_weight=data.edge_weight.squeeze().cuda() if data.edge_weight is not None else None,\n",
    "            )\n",
    "        loss = criterion(out, data.y.float().cuda().view(1, -1)) \n",
    "        loss.backward()  \n",
    "        optimizer.step() \n",
    "        loss_avg += loss.item()\n",
    "        count += 1\n",
    "    return loss_avg / count\n",
    "\n",
    "def test(loader):\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        labels = []\n",
    "        preds = []\n",
    "        loss_avg = 0\n",
    "        for data in loader:   \n",
    "            out = model(\n",
    "                data.x.float().cuda(), \n",
    "                edge_index=data.edge_index.long().cuda(), \n",
    "                batch=torch.LongTensor(np.zeros(data.x.shape[0])).cuda(),\n",
    "                edge_weight=data.edge_weight.squeeze().cuda() if data.edge_weight is not None else None,\n",
    "                )  \n",
    "            loss = criterion(out, data.y.float().cuda().view(1, -1)) \n",
    "            preds.append(torch.sigmoid(out).cpu().numpy().squeeze())\n",
    "            labels.append(data.y.numpy().squeeze())\n",
    "            loss_avg += loss.item()\n",
    "        preds = np.array(preds)\n",
    "        labels = np.array(labels)\n",
    "        aucs, thresholds, thresholds_optimal = multi_label_roc(labels, preds)\n",
    "        def priority_func(pred):\n",
    "            codes = []\n",
    "            for x in pred:\n",
    "                if x[0]==1: code = 2\n",
    "                if x[0]==0 and x[1]==0: code = 0\n",
    "                if x[0]==0 and x[1]==1: code = 1\n",
    "                codes.append(code)\n",
    "            return codes\n",
    "        # priority_func = None\n",
    "        acc, f1 = accuracy_from_thresh(labels, preds, thresholds_optimal, priority_func)\n",
    "    return loss_avg / len(loader), acc, f1, aucs, thresholds_optimal "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1a2bfcff-147b-49dc-9a57-05963a3cca25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Preprocess(\n",
       "  (lin1): Sequential(\n",
       "    (0): Linear(in_features=512, out_features=1024, bias=True)\n",
       "    (1): PReLU(num_parameters=1)\n",
       "  )\n",
       "  (layers): Sequential(\n",
       "    (0): Sequential(\n",
       "      (0): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      (1): PReLU(num_parameters=1)\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      (1): PReLU(num_parameters=1)\n",
       "    )\n",
       "    (2): Sequential(\n",
       "      (0): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      (1): PReLU(num_parameters=1)\n",
       "    )\n",
       "  )\n",
       "  (lin2): Sequential(\n",
       "    (0): Linear(in_features=1024, out_features=512, bias=True)\n",
       "    (1): PReLU(num_parameters=1)\n",
       "  )\n",
       "  (classifier): Linear(in_features=512, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_epochs = 150\n",
    "weight_decay = 0.0005\n",
    "weights_name = 'MLP_TMA_Distributed'\n",
    "weights_path = 'weights_cv'\n",
    "os.makedirs('weights_cv', exist_ok=True)\n",
    "model = GraphNet(\n",
    "        in_channels=512, \n",
    "        out_channels=2, \n",
    "        hidden_channels=512, \n",
    "        gcn_layer='GCN', \n",
    "        graph_pool='mean', \n",
    "        drop_p0=0.0,\n",
    "        drop_p1=0.25, \n",
    "        preprocess=Preprocess(in_channels=512, hidden_channels=512, out_channels=512, n_layers=0),\n",
    "        )\n",
    "model = Preprocess(in_channels=512, hidden_channels=1024, out_channels=512, out_class=2, n_layers=3, head=True)\n",
    "lr = 0.0005\n",
    "model = model.cuda()\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "af793c0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running cross-validation fold: 00\n",
      "Epoch: 001, Train Loss: 0.1361, Test Loss: 0.1212, Train Acc: 0.7317, Test Acc: 0.7982, Test f1: 0.8613, AUC: class-0>>0.956626254180602|class-1>>0.9616306954436451\n",
      "Saved model!\n",
      "Best thresholds ===>>> class-0>>0.6852997541427612|class-1>>0.66994309425354\n",
      "Epoch: 002, Train Loss: 0.1165, Test Loss: 0.0978, Train Acc: 0.7366, Test Acc: 0.8330, Test f1: 0.8745, AUC: class-0>>0.9518185618729097|class-1>>0.9604947620850688\n",
      "Saved model!\n",
      "Best thresholds ===>>> class-0>>0.6210781335830688|class-1>>0.5656075477600098\n",
      "Epoch: 003, Train Loss: 0.0916, Test Loss: 0.0786, Train Acc: 0.7507, Test Acc: 0.8267, Test f1: 0.8735, AUC: class-0>>0.9555811036789297|class-1>>0.9599899028145904\n",
      "Epoch: 004, Train Loss: 0.0984, Test Loss: 0.0878, Train Acc: 0.7580, Test Acc: 0.7664, Test f1: 0.8371, AUC: class-0>>0.9623745819397993|class-1>>0.9574656064621987\n",
      "Epoch: 005, Train Loss: 0.0887, Test Loss: 0.0730, Train Acc: 0.7745, Test Acc: 0.8620, Test f1: 0.8802, AUC: class-0>>0.9603887959866221|class-1>>0.9618831250788843\n",
      "Saved model!\n",
      "Best thresholds ===>>> class-0>>0.6994276642799377|class-1>>0.616722822189331\n",
      "Epoch: 006, Train Loss: 0.1048, Test Loss: 0.0907, Train Acc: 0.7618, Test Acc: 0.7887, Test f1: 0.8549, AUC: class-0>>0.9620610367892977|class-1>>0.9588539694560142\n",
      "Epoch: 007, Train Loss: 0.0808, Test Loss: 0.0769, Train Acc: 0.8009, Test Acc: 0.7701, Test f1: 0.8395, AUC: class-0>>0.9659280936454849|class-1>>0.9488829988640667\n",
      "Epoch: 008, Train Loss: 0.0745, Test Loss: 0.0669, Train Acc: 0.7795, Test Acc: 0.8207, Test f1: 0.8621, AUC: class-0>>0.9680183946488294|class-1>>0.9583491101855358\n",
      "Epoch: 009, Train Loss: 0.0789, Test Loss: 0.0681, Train Acc: 0.7811, Test Acc: 0.8297, Test f1: 0.8630, AUC: class-0>>0.9638377926421404|class-1>>0.96011611763221\n",
      "Epoch: 010, Train Loss: 0.0751, Test Loss: 0.0694, Train Acc: 0.7751, Test Acc: 0.8493, Test f1: 0.8789, AUC: class-0>>0.9687500000000001|class-1>>0.9596112583617317\n",
      "Epoch: 011, Train Loss: 0.0702, Test Loss: 0.0598, Train Acc: 0.7924, Test Acc: 0.8303, Test f1: 0.8676, AUC: class-0>>0.9683319397993311|class-1>>0.959485043544112\n",
      "Epoch: 012, Train Loss: 0.0793, Test Loss: 0.0727, Train Acc: 0.7917, Test Acc: 0.8404, Test f1: 0.8775, AUC: class-0>>0.9695861204013378|class-1>>0.9599899028145904\n",
      "Epoch: 013, Train Loss: 0.0718, Test Loss: 0.0661, Train Acc: 0.7844, Test Acc: 0.8462, Test f1: 0.8827, AUC: class-0>>0.9705267558528429|class-1>>0.9598636879969709\n",
      "Epoch: 014, Train Loss: 0.0777, Test Loss: 0.0693, Train Acc: 0.7844, Test Acc: 0.8399, Test f1: 0.8810, AUC: class-0>>0.9709448160535118|class-1>>0.9598636879969709\n",
      "Epoch: 015, Train Loss: 0.0642, Test Loss: 0.0589, Train Acc: 0.7984, Test Acc: 0.8525, Test f1: 0.8835, AUC: class-0>>0.9698996655518395|class-1>>0.9599899028145904\n",
      "Saved model!\n",
      "Best thresholds ===>>> class-0>>0.6740160584449768|class-1>>0.6642098426818848\n",
      "Epoch: 016, Train Loss: 0.0688, Test Loss: 0.0677, Train Acc: 0.8041, Test Acc: 0.8367, Test f1: 0.8768, AUC: class-0>>0.9692725752508362|class-1>>0.9583491101855358\n",
      "Epoch: 017, Train Loss: 0.0666, Test Loss: 0.0600, Train Acc: 0.8044, Test Acc: 0.8726, Test f1: 0.8947, AUC: class-0>>0.9718854515050167|class-1>>0.9616306954436451\n",
      "Saved model!\n",
      "Best thresholds ===>>> class-0>>0.6359559297561646|class-1>>0.6129637956619263\n",
      "Epoch: 018, Train Loss: 0.0671, Test Loss: 0.0596, Train Acc: 0.7972, Test Acc: 0.8664, Test f1: 0.9020, AUC: class-0>>0.97063127090301|class-1>>0.9587277546383945\n",
      "Epoch: 019, Train Loss: 0.0694, Test Loss: 0.0590, Train Acc: 0.8066, Test Acc: 0.8462, Test f1: 0.8823, AUC: class-0>>0.97063127090301|class-1>>0.9592326139088729\n",
      "Epoch: 020, Train Loss: 0.0783, Test Loss: 0.0688, Train Acc: 0.8033, Test Acc: 0.8953, Test f1: 0.9058, AUC: class-0>>0.9714673913043479|class-1>>0.9608734065379276\n",
      "Saved model!\n",
      "Best thresholds ===>>> class-0>>0.7086533904075623|class-1>>0.6043012738227844\n",
      "Epoch: 021, Train Loss: 0.0688, Test Loss: 0.0688, Train Acc: 0.8248, Test Acc: 0.8393, Test f1: 0.8770, AUC: class-0>>0.9696906354515051|class-1>>0.9584753250031555\n",
      "Epoch: 022, Train Loss: 0.0793, Test Loss: 0.0782, Train Acc: 0.8115, Test Acc: 0.8394, Test f1: 0.8849, AUC: class-0>>0.9724080267558528|class-1>>0.9454751987883377\n",
      "Epoch: 023, Train Loss: 0.0632, Test Loss: 0.0578, Train Acc: 0.8109, Test Acc: 0.8304, Test f1: 0.8753, AUC: class-0>>0.9696906354515051|class-1>>0.9577180360974378\n",
      "Epoch: 024, Train Loss: 0.0700, Test Loss: 0.0675, Train Acc: 0.8089, Test Acc: 0.8462, Test f1: 0.8823, AUC: class-0>>0.9694816053511706|class-1>>0.9596112583617317\n",
      "Epoch: 025, Train Loss: 0.0652, Test Loss: 0.0610, Train Acc: 0.8151, Test Acc: 0.8430, Test f1: 0.8777, AUC: class-0>>0.9685409698996655|class-1>>0.9589801842736336\n",
      "Epoch: 026, Train Loss: 0.0593, Test Loss: 0.0549, Train Acc: 0.8080, Test Acc: 0.8400, Test f1: 0.8890, AUC: class-0>>0.9705267558528428|class-1>>0.9589801842736337\n",
      "Epoch: 027, Train Loss: 0.0908, Test Loss: 0.0902, Train Acc: 0.8211, Test Acc: 0.8461, Test f1: 0.8744, AUC: class-0>>0.9701086956521738|class-1>>0.9597374731793512\n",
      "Epoch: 028, Train Loss: 0.0743, Test Loss: 0.0766, Train Acc: 0.8269, Test Acc: 0.8330, Test f1: 0.8756, AUC: class-0>>0.9717809364548495|class-1>>0.9607471917203079\n",
      "Epoch: 029, Train Loss: 0.0589, Test Loss: 0.0541, Train Acc: 0.8198, Test Acc: 0.8849, Test f1: 0.9087, AUC: class-0>>0.9730351170568562|class-1>>0.9623879843493626\n",
      "Epoch: 030, Train Loss: 0.0655, Test Loss: 0.0562, Train Acc: 0.8100, Test Acc: 0.9007, Test f1: 0.9146, AUC: class-0>>0.9732441471571907|class-1>>0.9630190584374605\n",
      "Saved model!\n",
      "Best thresholds ===>>> class-0>>0.6596062779426575|class-1>>0.5925947427749634\n",
      "Epoch: 031, Train Loss: 0.0685, Test Loss: 0.0601, Train Acc: 0.8008, Test Acc: 0.8912, Test f1: 0.9093, AUC: class-0>>0.9713628762541806|class-1>>0.9601161176322099\n",
      "Epoch: 032, Train Loss: 0.0633, Test Loss: 0.0579, Train Acc: 0.7972, Test Acc: 0.8849, Test f1: 0.9086, AUC: class-0>>0.9723035117056857|class-1>>0.9606209769026884\n",
      "Epoch: 033, Train Loss: 0.0590, Test Loss: 0.0579, Train Acc: 0.8302, Test Acc: 0.8462, Test f1: 0.8827, AUC: class-0>>0.9728260869565217|class-1>>0.9604947620850688\n",
      "Epoch: 034, Train Loss: 0.0624, Test Loss: 0.0651, Train Acc: 0.8557, Test Acc: 0.8267, Test f1: 0.8745, AUC: class-0>>0.9721989966555185|class-1>>0.9580966805502966\n",
      "Epoch: 035, Train Loss: 0.0603, Test Loss: 0.0620, Train Acc: 0.8281, Test Acc: 0.8267, Test f1: 0.8745, AUC: class-0>>0.9720944816053512|class-1>>0.9603685472674491\n",
      "Epoch: 036, Train Loss: 0.0667, Test Loss: 0.0689, Train Acc: 0.8231, Test Acc: 0.8394, Test f1: 0.8849, AUC: class-0>>0.9734531772575252|class-1>>0.958601539820775\n",
      "Epoch: 037, Train Loss: 0.0557, Test Loss: 0.0542, Train Acc: 0.8222, Test Acc: 0.8463, Test f1: 0.8903, AUC: class-0>>0.9726170568561873|class-1>>0.9599899028145904\n",
      "Epoch: 038, Train Loss: 0.0737, Test Loss: 0.0650, Train Acc: 0.8233, Test Acc: 0.8305, Test f1: 0.8828, AUC: class-0>>0.97063127090301|class-1>>0.9613782658084058\n",
      "Epoch: 039, Train Loss: 0.0658, Test Loss: 0.0643, Train Acc: 0.8273, Test Acc: 0.8588, Test f1: 0.8843, AUC: class-0>>0.9720944816053512|class-1>>0.9613782658084058\n",
      "Epoch: 040, Train Loss: 0.0683, Test Loss: 0.0676, Train Acc: 0.8227, Test Acc: 0.8557, Test f1: 0.8878, AUC: class-0>>0.9719899665551839|class-1>>0.9611258361731667\n",
      "Epoch: 041, Train Loss: 0.0584, Test Loss: 0.0559, Train Acc: 0.8261, Test Acc: 0.8785, Test f1: 0.9080, AUC: class-0>>0.9726170568561874|class-1>>0.9601161176322099\n",
      "Epoch: 042, Train Loss: 0.0579, Test Loss: 0.0589, Train Acc: 0.8365, Test Acc: 0.8525, Test f1: 0.8835, AUC: class-0>>0.9726170568561874|class-1>>0.9654171399722328\n",
      "Epoch: 043, Train Loss: 0.0706, Test Loss: 0.0606, Train Acc: 0.8128, Test Acc: 0.8685, Test f1: 0.8978, AUC: class-0>>0.9717809364548495|class-1>>0.9616306954436451\n",
      "Epoch: 044, Train Loss: 0.0540, Test Loss: 0.0546, Train Acc: 0.8292, Test Acc: 0.8817, Test f1: 0.9126, AUC: class-0>>0.9728260869565216|class-1>>0.9608734065379276\n",
      "Epoch: 045, Train Loss: 0.0561, Test Loss: 0.0583, Train Acc: 0.8398, Test Acc: 0.8589, Test f1: 0.8925, AUC: class-0>>0.9734531772575251|class-1>>0.9617569102612646\n",
      "Epoch: 046, Train Loss: 0.0790, Test Loss: 0.0831, Train Acc: 0.8426, Test Acc: 0.8235, Test f1: 0.8699, AUC: class-0>>0.9698996655518395|class-1>>0.9582228953679162\n",
      "Epoch: 047, Train Loss: 0.0588, Test Loss: 0.0591, Train Acc: 0.8354, Test Acc: 0.8268, Test f1: 0.8818, AUC: class-0>>0.9731396321070235|class-1>>0.9593588287264925\n",
      "Epoch: 048, Train Loss: 0.0552, Test Loss: 0.0553, Train Acc: 0.8379, Test Acc: 0.8621, Test f1: 0.8972, AUC: class-0>>0.9727215719063546|class-1>>0.9599899028145904\n",
      "Epoch: 049, Train Loss: 0.0537, Test Loss: 0.0563, Train Acc: 0.8351, Test Acc: 0.8727, Test f1: 0.9031, AUC: class-0>>0.973871237458194|class-1>>0.9627666288022214\n",
      "Epoch: 050, Train Loss: 0.0590, Test Loss: 0.0619, Train Acc: 0.8398, Test Acc: 0.8336, Test f1: 0.8797, AUC: class-0>>0.9721989966555183|class-1>>0.970591947494636\n",
      "Epoch: 051, Train Loss: 0.0535, Test Loss: 0.0555, Train Acc: 0.8421, Test Acc: 0.8436, Test f1: 0.8821, AUC: class-0>>0.9721989966555185|class-1>>0.9615044806260254\n",
      "Epoch: 052, Train Loss: 0.0571, Test Loss: 0.0551, Train Acc: 0.8298, Test Acc: 0.8526, Test f1: 0.8913, AUC: class-0>>0.9737667224080266|class-1>>0.9602423324498296\n",
      "Epoch: 053, Train Loss: 0.0573, Test Loss: 0.0607, Train Acc: 0.8408, Test Acc: 0.8489, Test f1: 0.8906, AUC: class-0>>0.9732441471571907|class-1>>0.9613782658084059\n",
      "Epoch: 054, Train Loss: 0.0610, Test Loss: 0.0642, Train Acc: 0.8396, Test Acc: 0.8621, Test f1: 0.8972, AUC: class-0>>0.9735576923076923|class-1>>0.9583491101855358\n",
      "Epoch: 055, Train Loss: 0.0517, Test Loss: 0.0543, Train Acc: 0.8514, Test Acc: 0.8589, Test f1: 0.8925, AUC: class-0>>0.9731396321070235|class-1>>0.9695822289536792\n",
      "Epoch: 056, Train Loss: 0.0545, Test Loss: 0.0555, Train Acc: 0.8618, Test Acc: 0.8748, Test f1: 0.8987, AUC: class-0>>0.9734531772575251|class-1>>0.9695822289536791\n",
      "Epoch: 057, Train Loss: 0.0506, Test Loss: 0.0538, Train Acc: 0.8653, Test Acc: 0.8653, Test f1: 0.8933, AUC: class-0>>0.9743938127090301|class-1>>0.965921999242711\n",
      "Epoch: 058, Train Loss: 0.0529, Test Loss: 0.0543, Train Acc: 0.8378, Test Acc: 0.8780, Test f1: 0.9034, AUC: class-0>>0.972930602006689|class-1>>0.9673103622365266\n",
      "Epoch: 059, Train Loss: 0.0577, Test Loss: 0.0634, Train Acc: 0.8568, Test Acc: 0.8585, Test f1: 0.8960, AUC: class-0>>0.9739757525083612|class-1>>0.9615044806260256\n",
      "Epoch: 060, Train Loss: 0.0496, Test Loss: 0.0541, Train Acc: 0.8476, Test Acc: 0.8621, Test f1: 0.8972, AUC: class-0>>0.9741847826086956|class-1>>0.9700870882241575\n",
      "Epoch: 061, Train Loss: 0.0505, Test Loss: 0.0536, Train Acc: 0.8559, Test Acc: 0.8653, Test f1: 0.9016, AUC: class-0>>0.9736622073578596|class-1>>0.9699608734065379\n",
      "Epoch: 062, Train Loss: 0.0519, Test Loss: 0.0582, Train Acc: 0.8623, Test Acc: 0.8617, Test f1: 0.9008, AUC: class-0>>0.9749163879598662|class-1>>0.9625141991669822\n",
      "Epoch: 063, Train Loss: 0.0512, Test Loss: 0.0526, Train Acc: 0.8572, Test Acc: 0.8589, Test f1: 0.8925, AUC: class-0>>0.9740802675585285|class-1>>0.964407421431276\n",
      "Epoch: 064, Train Loss: 0.0490, Test Loss: 0.0524, Train Acc: 0.8517, Test Acc: 0.8590, Test f1: 0.9005, AUC: class-0>>0.9742892976588629|class-1>>0.9686987252303421\n",
      "Epoch: 065, Train Loss: 0.0481, Test Loss: 0.0523, Train Acc: 0.8622, Test Acc: 0.8685, Test f1: 0.8980, AUC: class-0>>0.9735576923076923|class-1>>0.9686987252303421\n",
      "Epoch: 066, Train Loss: 0.0514, Test Loss: 0.0586, Train Acc: 0.8605, Test Acc: 0.8621, Test f1: 0.8972, AUC: class-0>>0.975752508361204|class-1>>0.9611258361731667\n",
      "Epoch: 067, Train Loss: 0.0626, Test Loss: 0.0645, Train Acc: 0.8550, Test Acc: 0.8594, Test f1: 0.8887, AUC: class-0>>0.9735576923076923|class-1>>0.9611258361731667\n",
      "Epoch: 068, Train Loss: 0.0533, Test Loss: 0.0589, Train Acc: 0.8706, Test Acc: 0.8653, Test f1: 0.8933, AUC: class-0>>0.9725125418060201|class-1>>0.96932979931844\n",
      "Epoch: 069, Train Loss: 0.0670, Test Loss: 0.0692, Train Acc: 0.8461, Test Acc: 0.8463, Test f1: 0.8901, AUC: class-0>>0.9729306020066889|class-1>>0.9603685472674492\n",
      "Epoch: 070, Train Loss: 0.0465, Test Loss: 0.0536, Train Acc: 0.8677, Test Acc: 0.8653, Test f1: 0.8933, AUC: class-0>>0.9732441471571907|class-1>>0.9650384955193739\n",
      "Epoch: 071, Train Loss: 0.0513, Test Loss: 0.0526, Train Acc: 0.8466, Test Acc: 0.8749, Test f1: 0.9074, AUC: class-0>>0.9743938127090301|class-1>>0.9702133030417771\n",
      "Epoch: 072, Train Loss: 0.0517, Test Loss: 0.0597, Train Acc: 0.8659, Test Acc: 0.8425, Test f1: 0.8813, AUC: class-0>>0.9740802675585285|class-1>>0.9690773696832007\n",
      "Epoch: 073, Train Loss: 0.0472, Test Loss: 0.0557, Train Acc: 0.8671, Test Acc: 0.8557, Test f1: 0.8878, AUC: class-0>>0.9742892976588629|class-1>>0.9671841474189069\n",
      "Epoch: 074, Train Loss: 0.0474, Test Loss: 0.0539, Train Acc: 0.8710, Test Acc: 0.8685, Test f1: 0.8980, AUC: class-0>>0.9741847826086956|class-1>>0.9676890066893853\n",
      "Epoch: 075, Train Loss: 0.0487, Test Loss: 0.0577, Train Acc: 0.8677, Test Acc: 0.8589, Test f1: 0.8925, AUC: class-0>>0.9735576923076924|class-1>>0.9666792881484286\n",
      "Epoch: 076, Train Loss: 0.1042, Test Loss: 0.1168, Train Acc: 0.8488, Test Acc: 0.8494, Test f1: 0.8870, AUC: class-0>>0.9726170568561873|class-1>>0.9573393916445789\n",
      "Epoch: 077, Train Loss: 0.0457, Test Loss: 0.0531, Train Acc: 0.8645, Test Acc: 0.8525, Test f1: 0.8832, AUC: class-0>>0.9738712374581939|class-1>>0.9688249400479616\n",
      "Epoch: 078, Train Loss: 0.0448, Test Loss: 0.0513, Train Acc: 0.8689, Test Acc: 0.8621, Test f1: 0.8972, AUC: class-0>>0.9744983277591972|class-1>>0.9705919474946358\n",
      "Epoch: 079, Train Loss: 0.0521, Test Loss: 0.0602, Train Acc: 0.8692, Test Acc: 0.8394, Test f1: 0.8849, AUC: class-0>>0.9746028428093646|class-1>>0.9594850435441121\n",
      "Epoch: 080, Train Loss: 0.0464, Test Loss: 0.0525, Train Acc: 0.8782, Test Acc: 0.8780, Test f1: 0.9034, AUC: class-0>>0.9744983277591973|class-1>>0.9690773696832009\n",
      "Epoch: 081, Train Loss: 0.0476, Test Loss: 0.0570, Train Acc: 0.8696, Test Acc: 0.8299, Test f1: 0.8791, AUC: class-0>>0.9746028428093645|class-1>>0.9639025621607977\n",
      "Epoch: 082, Train Loss: 0.0484, Test Loss: 0.0521, Train Acc: 0.8802, Test Acc: 0.8939, Test f1: 0.9182, AUC: class-0>>0.9733486622073578|class-1>>0.9692035845008204\n",
      "Saved model!\n",
      "Best thresholds ===>>> class-0>>0.64512038230896|class-1>>0.6141788363456726\n",
      "Epoch: 083, Train Loss: 0.0527, Test Loss: 0.0654, Train Acc: 0.8804, Test Acc: 0.8500, Test f1: 0.8913, AUC: class-0>>0.9749163879598661|class-1>>0.9683200807774833\n",
      "Epoch: 084, Train Loss: 0.0464, Test Loss: 0.0536, Train Acc: 0.8705, Test Acc: 0.8621, Test f1: 0.8972, AUC: class-0>>0.9754389632107023|class-1>>0.9722327401236905\n",
      "Epoch: 085, Train Loss: 0.0464, Test Loss: 0.0559, Train Acc: 0.8678, Test Acc: 0.8526, Test f1: 0.8916, AUC: class-0>>0.9738712374581939|class-1>>0.9683200807774832\n",
      "Epoch: 086, Train Loss: 0.0429, Test Loss: 0.0542, Train Acc: 0.8900, Test Acc: 0.8431, Test f1: 0.8854, AUC: class-0>>0.9752299331103679|class-1>>0.970844377129875\n",
      "Epoch: 087, Train Loss: 0.0439, Test Loss: 0.0522, Train Acc: 0.8929, Test Acc: 0.8589, Test f1: 0.8925, AUC: class-0>>0.9731396321070235|class-1>>0.9694560141360595\n",
      "Epoch: 088, Train Loss: 0.0456, Test Loss: 0.0578, Train Acc: 0.8856, Test Acc: 0.8531, Test f1: 0.8879, AUC: class-0>>0.9751254180602006|class-1>>0.9649122807017544\n",
      "Epoch: 089, Train Loss: 0.0442, Test Loss: 0.0512, Train Acc: 0.8803, Test Acc: 0.8849, Test f1: 0.9087, AUC: class-0>>0.973453177257525|class-1>>0.9723589549413101\n",
      "Epoch: 090, Train Loss: 0.0432, Test Loss: 0.0542, Train Acc: 0.8927, Test Acc: 0.8589, Test f1: 0.8925, AUC: class-0>>0.9747073578595318|class-1>>0.970844377129875\n",
      "Epoch: 091, Train Loss: 0.0485, Test Loss: 0.0593, Train Acc: 0.8939, Test Acc: 0.8526, Test f1: 0.8913, AUC: class-0>>0.9735576923076923|class-1>>0.9698346585889183\n",
      "Epoch: 092, Train Loss: 0.0460, Test Loss: 0.0528, Train Acc: 0.8944, Test Acc: 0.8812, Test f1: 0.9081, AUC: class-0>>0.9743938127090301|class-1>>0.970844377129875\n",
      "Epoch: 093, Train Loss: 0.0439, Test Loss: 0.0571, Train Acc: 0.9002, Test Acc: 0.8362, Test f1: 0.8798, AUC: class-0>>0.9740802675585284|class-1>>0.96932979931844\n",
      "Epoch: 094, Train Loss: 0.0468, Test Loss: 0.0564, Train Acc: 0.8923, Test Acc: 0.8621, Test f1: 0.8972, AUC: class-0>>0.9743938127090301|class-1>>0.9710968067651142\n",
      "Epoch: 095, Train Loss: 0.0457, Test Loss: 0.0601, Train Acc: 0.8997, Test Acc: 0.8362, Test f1: 0.8798, AUC: class-0>>0.9747073578595318|class-1>>0.9683200807774833\n",
      "Epoch: 096, Train Loss: 0.0404, Test Loss: 0.0536, Train Acc: 0.8901, Test Acc: 0.8526, Test f1: 0.8913, AUC: class-0>>0.9751254180602007|class-1>>0.9703395178593968\n",
      "Epoch: 097, Train Loss: 0.0411, Test Loss: 0.0544, Train Acc: 0.8981, Test Acc: 0.8462, Test f1: 0.8823, AUC: class-0>>0.9754389632107023|class-1>>0.9689511548655811\n",
      "Epoch: 098, Train Loss: 0.0431, Test Loss: 0.0547, Train Acc: 0.8960, Test Acc: 0.8494, Test f1: 0.8867, AUC: class-0>>0.9751254180602007|class-1>>0.9694560141360595\n",
      "Epoch: 099, Train Loss: 0.0409, Test Loss: 0.0530, Train Acc: 0.8919, Test Acc: 0.8494, Test f1: 0.8870, AUC: class-0>>0.9743938127090301|class-1>>0.9695822289536792\n",
      "Epoch: 100, Train Loss: 0.0402, Test Loss: 0.0515, Train Acc: 0.8948, Test Acc: 0.8621, Test f1: 0.8972, AUC: class-0>>0.9752299331103679|class-1>>0.9705919474946358\n",
      "Epoch: 101, Train Loss: 0.0447, Test Loss: 0.0537, Train Acc: 0.8938, Test Acc: 0.8621, Test f1: 0.8972, AUC: class-0>>0.9733486622073578|class-1>>0.971475451217973\n",
      "Epoch: 102, Train Loss: 0.0385, Test Loss: 0.0536, Train Acc: 0.9061, Test Acc: 0.8468, Test f1: 0.8867, AUC: class-0>>0.9756479933110367|class-1>>0.96932979931844\n",
      "Epoch: 103, Train Loss: 0.0393, Test Loss: 0.0531, Train Acc: 0.9027, Test Acc: 0.8558, Test f1: 0.8960, AUC: class-0>>0.9736622073578596|class-1>>0.9707181623122554\n",
      "Epoch: 104, Train Loss: 0.0385, Test Loss: 0.0531, Train Acc: 0.9069, Test Acc: 0.8526, Test f1: 0.8913, AUC: class-0>>0.9756479933110367|class-1>>0.970844377129875\n",
      "Epoch: 105, Train Loss: 0.0387, Test Loss: 0.0534, Train Acc: 0.9050, Test Acc: 0.8557, Test f1: 0.8878, AUC: class-0>>0.9754389632107023|class-1>>0.9703395178593968\n",
      "Epoch: 106, Train Loss: 0.0385, Test Loss: 0.0527, Train Acc: 0.9050, Test Acc: 0.8557, Test f1: 0.8878, AUC: class-0>>0.9755434782608695|class-1>>0.9708443771298749\n",
      "Epoch: 107, Train Loss: 0.0371, Test Loss: 0.0525, Train Acc: 0.9070, Test Acc: 0.8457, Test f1: 0.8858, AUC: class-0>>0.9752299331103679|class-1>>0.9697084437712987\n",
      "Epoch: 108, Train Loss: 0.0374, Test Loss: 0.0528, Train Acc: 0.9018, Test Acc: 0.8457, Test f1: 0.8858, AUC: class-0>>0.9746028428093645|class-1>>0.9707181623122555\n",
      "Epoch: 109, Train Loss: 0.0374, Test Loss: 0.0540, Train Acc: 0.9072, Test Acc: 0.8457, Test f1: 0.8858, AUC: class-0>>0.975334448160535|class-1>>0.9704657326770163\n",
      "Epoch: 110, Train Loss: 0.0373, Test Loss: 0.0529, Train Acc: 0.9009, Test Acc: 0.8626, Test f1: 0.8933, AUC: class-0>>0.9752299331103679|class-1>>0.9699608734065379\n",
      "Epoch: 111, Train Loss: 0.0366, Test Loss: 0.0532, Train Acc: 0.9033, Test Acc: 0.8268, Test f1: 0.8815, AUC: class-0>>0.97439381270903|class-1>>0.9686987252303421\n",
      "Epoch: 112, Train Loss: 0.0369, Test Loss: 0.0532, Train Acc: 0.9061, Test Acc: 0.8457, Test f1: 0.8858, AUC: class-0>>0.9742892976588629|class-1>>0.9704657326770163\n",
      "Epoch: 113, Train Loss: 0.0372, Test Loss: 0.0529, Train Acc: 0.9040, Test Acc: 0.8457, Test f1: 0.8858, AUC: class-0>>0.9751254180602007|class-1>>0.9697084437712986\n",
      "Epoch: 114, Train Loss: 0.0413, Test Loss: 0.0607, Train Acc: 0.9148, Test Acc: 0.8436, Test f1: 0.8821, AUC: class-0>>0.9748118729096991|class-1>>0.967815221507005\n",
      "Epoch: 115, Train Loss: 0.0375, Test Loss: 0.0540, Train Acc: 0.9031, Test Acc: 0.8331, Test f1: 0.8830, AUC: class-0>>0.975229933110368|class-1>>0.9690773696832009\n",
      "Epoch: 116, Train Loss: 0.0361, Test Loss: 0.0547, Train Acc: 0.9097, Test Acc: 0.8394, Test f1: 0.8845, AUC: class-0>>0.9753344481605352|class-1>>0.9689511548655813\n",
      "Epoch: 117, Train Loss: 0.0404, Test Loss: 0.0569, Train Acc: 0.9115, Test Acc: 0.8436, Test f1: 0.8821, AUC: class-0>>0.9747073578595318|class-1>>0.9683200807774832\n",
      "Epoch: 118, Train Loss: 0.0355, Test Loss: 0.0541, Train Acc: 0.9161, Test Acc: 0.8499, Test f1: 0.8833, AUC: class-0>>0.9746028428093646|class-1>>0.9692035845008203\n",
      "Epoch: 119, Train Loss: 0.0359, Test Loss: 0.0548, Train Acc: 0.9131, Test Acc: 0.8436, Test f1: 0.8821, AUC: class-0>>0.9746028428093645|class-1>>0.9686987252303421\n",
      "Epoch: 120, Train Loss: 0.0352, Test Loss: 0.0542, Train Acc: 0.9140, Test Acc: 0.8425, Test f1: 0.8811, AUC: class-0>>0.9740802675585284|class-1>>0.9698346585889184\n",
      "Epoch: 121, Train Loss: 0.0358, Test Loss: 0.0544, Train Acc: 0.9152, Test Acc: 0.8362, Test f1: 0.8798, AUC: class-0>>0.9743938127090301|class-1>>0.968698725230342\n",
      "Epoch: 122, Train Loss: 0.0352, Test Loss: 0.0546, Train Acc: 0.9151, Test Acc: 0.8499, Test f1: 0.8833, AUC: class-0>>0.9747073578595318|class-1>>0.9686987252303421\n",
      "Epoch: 123, Train Loss: 0.0358, Test Loss: 0.0556, Train Acc: 0.9131, Test Acc: 0.8394, Test f1: 0.8845, AUC: class-0>>0.9739757525083612|class-1>>0.9704657326770163\n",
      "Epoch: 124, Train Loss: 0.0349, Test Loss: 0.0535, Train Acc: 0.9173, Test Acc: 0.8300, Test f1: 0.8862, AUC: class-0>>0.9740802675585285|class-1>>0.9695822289536791\n",
      "Epoch: 125, Train Loss: 0.0354, Test Loss: 0.0539, Train Acc: 0.9152, Test Acc: 0.8499, Test f1: 0.8833, AUC: class-0>>0.9743938127090301|class-1>>0.9690773696832008\n",
      "Epoch: 126, Train Loss: 0.0348, Test Loss: 0.0530, Train Acc: 0.9173, Test Acc: 0.8241, Test f1: 0.8812, AUC: class-0>>0.9743938127090301|class-1>>0.9699608734065379\n",
      "Epoch: 127, Train Loss: 0.0344, Test Loss: 0.0533, Train Acc: 0.9173, Test Acc: 0.8362, Test f1: 0.8798, AUC: class-0>>0.97439381270903|class-1>>0.9698346585889183\n",
      "Epoch: 128, Train Loss: 0.0353, Test Loss: 0.0564, Train Acc: 0.9173, Test Acc: 0.8436, Test f1: 0.8821, AUC: class-0>>0.9740802675585284|class-1>>0.9694560141360596\n",
      "Epoch: 129, Train Loss: 0.0343, Test Loss: 0.0531, Train Acc: 0.9194, Test Acc: 0.8268, Test f1: 0.8815, AUC: class-0>>0.9748118729096991|class-1>>0.9699608734065379\n",
      "Epoch: 130, Train Loss: 0.0341, Test Loss: 0.0540, Train Acc: 0.9194, Test Acc: 0.8241, Test f1: 0.8812, AUC: class-0>>0.9743938127090301|class-1>>0.96932979931844\n",
      "Epoch: 131, Train Loss: 0.0341, Test Loss: 0.0531, Train Acc: 0.9173, Test Acc: 0.8362, Test f1: 0.8798, AUC: class-0>>0.9735576923076923|class-1>>0.970213303041777\n",
      "Epoch: 132, Train Loss: 0.0340, Test Loss: 0.0541, Train Acc: 0.9194, Test Acc: 0.8241, Test f1: 0.8812, AUC: class-0>>0.9737667224080268|class-1>>0.9698346585889184\n",
      "Epoch: 133, Train Loss: 0.0338, Test Loss: 0.0535, Train Acc: 0.9194, Test Acc: 0.8241, Test f1: 0.8812, AUC: class-0>>0.9748118729096991|class-1>>0.9700870882241575\n",
      "Epoch: 134, Train Loss: 0.0338, Test Loss: 0.0543, Train Acc: 0.9256, Test Acc: 0.8373, Test f1: 0.8883, AUC: class-0>>0.9741847826086956|class-1>>0.9689511548655813\n",
      "Epoch: 135, Train Loss: 0.0337, Test Loss: 0.0540, Train Acc: 0.9173, Test Acc: 0.8241, Test f1: 0.8812, AUC: class-0>>0.9744983277591974|class-1>>0.9702133030417771\n",
      "Epoch: 136, Train Loss: 0.0338, Test Loss: 0.0544, Train Acc: 0.9215, Test Acc: 0.8373, Test f1: 0.8883, AUC: class-0>>0.9736622073578596|class-1>>0.9695822289536792\n",
      "Epoch: 137, Train Loss: 0.0337, Test Loss: 0.0539, Train Acc: 0.9173, Test Acc: 0.8268, Test f1: 0.8815, AUC: class-0>>0.9739757525083612|class-1>>0.9700870882241575\n",
      "Epoch: 138, Train Loss: 0.0340, Test Loss: 0.0541, Train Acc: 0.9194, Test Acc: 0.8268, Test f1: 0.8815, AUC: class-0>>0.9737667224080269|class-1>>0.9704657326770163\n",
      "Epoch: 139, Train Loss: 0.0335, Test Loss: 0.0538, Train Acc: 0.9194, Test Acc: 0.8373, Test f1: 0.8883, AUC: class-0>>0.973871237458194|class-1>>0.96932979931844\n",
      "Epoch: 140, Train Loss: 0.0334, Test Loss: 0.0536, Train Acc: 0.9194, Test Acc: 0.8241, Test f1: 0.8812, AUC: class-0>>0.9741847826086957|class-1>>0.9699608734065379\n",
      "Epoch: 141, Train Loss: 0.0337, Test Loss: 0.0538, Train Acc: 0.9235, Test Acc: 0.8373, Test f1: 0.8883, AUC: class-0>>0.974289297658863|class-1>>0.96932979931844\n",
      "Epoch: 142, Train Loss: 0.0335, Test Loss: 0.0536, Train Acc: 0.9194, Test Acc: 0.8268, Test f1: 0.8815, AUC: class-0>>0.9737667224080269|class-1>>0.9703395178593968\n",
      "Epoch: 143, Train Loss: 0.0334, Test Loss: 0.0541, Train Acc: 0.9256, Test Acc: 0.8241, Test f1: 0.8812, AUC: class-0>>0.9738712374581939|class-1>>0.9700870882241575\n",
      "Epoch: 144, Train Loss: 0.0334, Test Loss: 0.0543, Train Acc: 0.9256, Test Acc: 0.8362, Test f1: 0.8798, AUC: class-0>>0.9735576923076923|class-1>>0.9700870882241576\n",
      "Epoch: 145, Train Loss: 0.0334, Test Loss: 0.0534, Train Acc: 0.9235, Test Acc: 0.8362, Test f1: 0.8798, AUC: class-0>>0.9737667224080269|class-1>>0.970213303041777\n",
      "Epoch: 146, Train Loss: 0.0335, Test Loss: 0.0544, Train Acc: 0.9256, Test Acc: 0.8268, Test f1: 0.8815, AUC: class-0>>0.9735576923076923|class-1>>0.9700870882241575\n",
      "Epoch: 147, Train Loss: 0.0335, Test Loss: 0.0548, Train Acc: 0.9256, Test Acc: 0.8341, Test f1: 0.8840, AUC: class-0>>0.9737667224080268|class-1>>0.9697084437712987\n",
      "Epoch: 148, Train Loss: 0.0332, Test Loss: 0.0541, Train Acc: 0.9256, Test Acc: 0.8362, Test f1: 0.8798, AUC: class-0>>0.9736622073578595|class-1>>0.9702133030417771\n",
      "Epoch: 149, Train Loss: 0.0331, Test Loss: 0.0539, Train Acc: 0.9256, Test Acc: 0.8373, Test f1: 0.8883, AUC: class-0>>0.9735576923076923|class-1>>0.9698346585889184\n",
      "Running cross-validation fold: 01\n",
      "Epoch: 001, Train Loss: 0.1520, Test Loss: 0.1551, Train Acc: 0.7671, Test Acc: 0.6985, Test f1: 0.7171, AUC: class-0>>0.9567389455782314|class-1>>0.9268175252768416\n",
      "Saved model!\n",
      "Best thresholds ===>>> class-0>>0.5784044861793518|class-1>>0.5808583498001099\n",
      "Epoch: 002, Train Loss: 0.0984, Test Loss: 0.1154, Train Acc: 0.7620, Test Acc: 0.6546, Test f1: 0.6343, AUC: class-0>>0.9622661564625851|class-1>>0.9093644679826673\n",
      "Epoch: 003, Train Loss: 0.1099, Test Loss: 0.1242, Train Acc: 0.7724, Test Acc: 0.7093, Test f1: 0.7358, AUC: class-0>>0.9664115646258503|class-1>>0.9270582571015888\n",
      "Saved model!\n",
      "Best thresholds ===>>> class-0>>0.6526367664337158|class-1>>0.6947576999664307\n",
      "Epoch: 004, Train Loss: 0.0805, Test Loss: 0.0945, Train Acc: 0.7962, Test Acc: 0.6999, Test f1: 0.7125, AUC: class-0>>0.9657738095238095|class-1>>0.9406596051998074\n",
      "Epoch: 005, Train Loss: 0.0833, Test Loss: 0.0964, Train Acc: 0.8040, Test Acc: 0.6984, Test f1: 0.7217, AUC: class-0>>0.9686437074829932|class-1>>0.9310303322099182\n",
      "Epoch: 006, Train Loss: 0.0712, Test Loss: 0.0863, Train Acc: 0.8080, Test Acc: 0.7357, Test f1: 0.7635, AUC: class-0>>0.9715136054421769|class-1>>0.9409003370245547\n",
      "Saved model!\n",
      "Best thresholds ===>>> class-0>>0.6391561627388|class-1>>0.6461546421051025\n",
      "Epoch: 007, Train Loss: 0.0789, Test Loss: 0.0925, Train Acc: 0.8145, Test Acc: 0.7371, Test f1: 0.7652, AUC: class-0>>0.9753401360544218|class-1>>0.930789600385171\n",
      "Epoch: 008, Train Loss: 0.0708, Test Loss: 0.0854, Train Acc: 0.7763, Test Acc: 0.7027, Test f1: 0.7141, AUC: class-0>>0.9735331632653061|class-1>>0.943307655272027\n",
      "Epoch: 009, Train Loss: 0.0920, Test Loss: 0.1022, Train Acc: 0.8136, Test Acc: 0.7267, Test f1: 0.7469, AUC: class-0>>0.9732142857142856|class-1>>0.9353635050553682\n",
      "Epoch: 010, Train Loss: 0.0717, Test Loss: 0.0859, Train Acc: 0.7773, Test Acc: 0.7584, Test f1: 0.7853, AUC: class-0>>0.9734268707482994|class-1>>0.9457149735194993\n",
      "Saved model!\n",
      "Best thresholds ===>>> class-0>>0.6330136060714722|class-1>>0.6407173871994019\n",
      "Epoch: 011, Train Loss: 0.0739, Test Loss: 0.0923, Train Acc: 0.8156, Test Acc: 0.7027, Test f1: 0.7154, AUC: class-0>>0.9772534013605443|class-1>>0.9310303322099182\n",
      "Epoch: 012, Train Loss: 0.0999, Test Loss: 0.1154, Train Acc: 0.8336, Test Acc: 0.7093, Test f1: 0.7296, AUC: class-0>>0.9584396258503403|class-1>>0.9321136254212806\n",
      "Epoch: 013, Train Loss: 0.0604, Test Loss: 0.0788, Train Acc: 0.8162, Test Acc: 0.7813, Test f1: 0.8054, AUC: class-0>>0.9774659863945578|class-1>>0.9399374097255656\n",
      "Saved model!\n",
      "Best thresholds ===>>> class-0>>0.6333082318305969|class-1>>0.6649540662765503\n",
      "Epoch: 014, Train Loss: 0.0859, Test Loss: 0.0993, Train Acc: 0.8462, Test Acc: 0.7480, Test f1: 0.7722, AUC: class-0>>0.9757653061224489|class-1>>0.9340394800192585\n",
      "Epoch: 015, Train Loss: 0.0619, Test Loss: 0.0806, Train Acc: 0.8089, Test Acc: 0.7944, Test f1: 0.8194, AUC: class-0>>0.9770408163265305|class-1>>0.934520943668753\n",
      "Saved model!\n",
      "Best thresholds ===>>> class-0>>0.6071924567222595|class-1>>0.6663201451301575\n",
      "Epoch: 016, Train Loss: 0.0627, Test Loss: 0.0816, Train Acc: 0.8028, Test Acc: 0.7905, Test f1: 0.8150, AUC: class-0>>0.97906037414966|class-1>>0.9465575349061146\n",
      "Saved model!\n",
      "Best thresholds ===>>> class-0>>0.6586939692497253|class-1>>0.6624289155006409\n",
      "Epoch: 017, Train Loss: 0.0646, Test Loss: 0.0837, Train Acc: 0.8126, Test Acc: 0.7892, Test f1: 0.8131, AUC: class-0>>0.9799107142857143|class-1>>0.9306692344727973\n",
      "Epoch: 018, Train Loss: 0.0617, Test Loss: 0.0804, Train Acc: 0.8402, Test Acc: 0.7665, Test f1: 0.7936, AUC: class-0>>0.9789540816326532|class-1>>0.9404188733750601\n",
      "Epoch: 019, Train Loss: 0.0614, Test Loss: 0.0855, Train Acc: 0.8217, Test Acc: 0.7892, Test f1: 0.8138, AUC: class-0>>0.9796981292517007|class-1>>0.925132402503611\n",
      "Epoch: 020, Train Loss: 0.0580, Test Loss: 0.0782, Train Acc: 0.8387, Test Acc: 0.7533, Test f1: 0.7789, AUC: class-0>>0.979591836734694|class-1>>0.933317284545017\n",
      "Epoch: 021, Train Loss: 0.0581, Test Loss: 0.0798, Train Acc: 0.8480, Test Acc: 0.7798, Test f1: 0.8060, AUC: class-0>>0.9791666666666667|class-1>>0.9390948483389504\n",
      "Epoch: 022, Train Loss: 0.0632, Test Loss: 0.0834, Train Acc: 0.8396, Test Acc: 0.7878, Test f1: 0.8132, AUC: class-0>>0.9790603741496599|class-1>>0.9396966779008186\n",
      "Epoch: 023, Train Loss: 0.0584, Test Loss: 0.0791, Train Acc: 0.8345, Test Acc: 0.7732, Test f1: 0.7999, AUC: class-0>>0.9807610544217686|class-1>>0.9324747231584015\n",
      "Epoch: 024, Train Loss: 0.0816, Test Loss: 0.1024, Train Acc: 0.8312, Test Acc: 0.7705, Test f1: 0.7962, AUC: class-0>>0.9753401360544218|class-1>>0.9271786230139625\n",
      "Epoch: 025, Train Loss: 0.0575, Test Loss: 0.0784, Train Acc: 0.8484, Test Acc: 0.7572, Test f1: 0.7828, AUC: class-0>>0.9792729591836734|class-1>>0.9457149735194993\n",
      "Epoch: 026, Train Loss: 0.0556, Test Loss: 0.0783, Train Acc: 0.8335, Test Acc: 0.7426, Test f1: 0.7664, AUC: class-0>>0.9792729591836735|class-1>>0.9419836302359172\n",
      "Epoch: 027, Train Loss: 0.0545, Test Loss: 0.0776, Train Acc: 0.8450, Test Acc: 0.7719, Test f1: 0.7980, AUC: class-0>>0.9803358843537415|class-1>>0.9182715454983149\n",
      "Epoch: 028, Train Loss: 0.0630, Test Loss: 0.0844, Train Acc: 0.8347, Test Acc: 0.7386, Test f1: 0.7598, AUC: class-0>>0.9809736394557824|class-1>>0.935483870967742\n",
      "Epoch: 029, Train Loss: 0.0592, Test Loss: 0.0829, Train Acc: 0.8596, Test Acc: 0.7280, Test f1: 0.7486, AUC: class-0>>0.9805484693877551|class-1>>0.93608570052961\n",
      "Epoch: 030, Train Loss: 0.0558, Test Loss: 0.0794, Train Acc: 0.8475, Test Acc: 0.7772, Test f1: 0.8030, AUC: class-0>>0.9804421768707483|class-1>>0.929224843524314\n",
      "Epoch: 031, Train Loss: 0.0549, Test Loss: 0.0791, Train Acc: 0.8511, Test Acc: 0.7719, Test f1: 0.7976, AUC: class-0>>0.9785289115646258|class-1>>0.9359653346172364\n",
      "Epoch: 032, Train Loss: 0.0532, Test Loss: 0.0771, Train Acc: 0.8639, Test Acc: 0.7825, Test f1: 0.8085, AUC: class-0>>0.9798044217687074|class-1>>0.9263360616273472\n",
      "Epoch: 033, Train Loss: 0.0523, Test Loss: 0.0776, Train Acc: 0.8592, Test Acc: 0.8104, Test f1: 0.8340, AUC: class-0>>0.9804421768707483|class-1>>0.929826673086182\n",
      "Saved model!\n",
      "Best thresholds ===>>> class-0>>0.6247166991233826|class-1>>0.6205406188964844\n",
      "Epoch: 034, Train Loss: 0.0514, Test Loss: 0.0756, Train Acc: 0.8628, Test Acc: 0.8037, Test f1: 0.8280, AUC: class-0>>0.9809736394557824|class-1>>0.9334376504573905\n",
      "Epoch: 035, Train Loss: 0.0622, Test Loss: 0.0867, Train Acc: 0.8405, Test Acc: 0.7705, Test f1: 0.7966, AUC: class-0>>0.9796981292517006|class-1>>0.9382522869523351\n",
      "Epoch: 036, Train Loss: 0.0582, Test Loss: 0.0834, Train Acc: 0.8584, Test Acc: 0.7853, Test f1: 0.8097, AUC: class-0>>0.9804421768707483|class-1>>0.9189937409725566\n",
      "Epoch: 037, Train Loss: 0.0563, Test Loss: 0.0805, Train Acc: 0.8701, Test Acc: 0.7839, Test f1: 0.8093, AUC: class-0>>0.9813988095238095|class-1>>0.9233269138180068\n",
      "Epoch: 038, Train Loss: 0.0598, Test Loss: 0.0843, Train Acc: 0.8510, Test Acc: 0.7705, Test f1: 0.7980, AUC: class-0>>0.9819302721088435|class-1>>0.9307896003851709\n",
      "Epoch: 039, Train Loss: 0.0526, Test Loss: 0.0770, Train Acc: 0.8652, Test Acc: 0.7812, Test f1: 0.8066, AUC: class-0>>0.9800170068027211|class-1>>0.9336783822821376\n",
      "Epoch: 040, Train Loss: 0.0848, Test Loss: 0.1140, Train Acc: 0.8690, Test Acc: 0.7917, Test f1: 0.8173, AUC: class-0>>0.9805484693877551|class-1>>0.8426817525276841\n",
      "Epoch: 041, Train Loss: 0.0622, Test Loss: 0.0879, Train Acc: 0.8664, Test Acc: 0.7891, Test f1: 0.8132, AUC: class-0>>0.9806547619047619|class-1>>0.9410207029369282\n",
      "Epoch: 042, Train Loss: 0.0505, Test Loss: 0.0776, Train Acc: 0.8579, Test Acc: 0.7493, Test f1: 0.7736, AUC: class-0>>0.9809736394557823|class-1>>0.9209195955705345\n",
      "Epoch: 043, Train Loss: 0.0579, Test Loss: 0.0829, Train Acc: 0.8664, Test Acc: 0.7997, Test f1: 0.8236, AUC: class-0>>0.9815051020408164|class-1>>0.9406596051998075\n",
      "Epoch: 044, Train Loss: 0.0537, Test Loss: 0.0791, Train Acc: 0.8698, Test Acc: 0.7852, Test f1: 0.8115, AUC: class-0>>0.9792729591836735|class-1>>0.9323543572460279\n",
      "Epoch: 045, Train Loss: 0.0492, Test Loss: 0.0744, Train Acc: 0.8748, Test Acc: 0.7906, Test f1: 0.8156, AUC: class-0>>0.9820365646258504|class-1>>0.918873375060183\n",
      "Epoch: 046, Train Loss: 0.0482, Test Loss: 0.0749, Train Acc: 0.8661, Test Acc: 0.7679, Test f1: 0.7941, AUC: class-0>>0.980654761904762|class-1>>0.9353635050553682\n",
      "Epoch: 047, Train Loss: 0.0506, Test Loss: 0.0774, Train Acc: 0.8733, Test Acc: 0.7587, Test f1: 0.7834, AUC: class-0>>0.9808673469387754|class-1>>0.9357246027924891\n",
      "Epoch: 048, Train Loss: 0.0509, Test Loss: 0.0770, Train Acc: 0.8791, Test Acc: 0.8064, Test f1: 0.8291, AUC: class-0>>0.9808673469387756|class-1>>0.9336783822821377\n",
      "Epoch: 049, Train Loss: 0.0542, Test Loss: 0.0829, Train Acc: 0.8546, Test Acc: 0.7452, Test f1: 0.7719, AUC: class-0>>0.9801232993197279|class-1>>0.9334376504573906\n",
      "Epoch: 050, Train Loss: 0.0509, Test Loss: 0.0804, Train Acc: 0.8567, Test Acc: 0.7372, Test f1: 0.7612, AUC: class-0>>0.9803358843537415|class-1>>0.9217621569571498\n",
      "Epoch: 051, Train Loss: 0.0538, Test Loss: 0.0811, Train Acc: 0.8677, Test Acc: 0.7891, Test f1: 0.8132, AUC: class-0>>0.9798044217687074|class-1>>0.9412614347616755\n",
      "Epoch: 052, Train Loss: 0.0506, Test Loss: 0.0741, Train Acc: 0.8710, Test Acc: 0.7333, Test f1: 0.7553, AUC: class-0>>0.9818239795918368|class-1>>0.9445113143957631\n",
      "Epoch: 053, Train Loss: 0.0547, Test Loss: 0.0821, Train Acc: 0.8743, Test Acc: 0.8156, Test f1: 0.8365, AUC: class-0>>0.9792729591836734|class-1>>0.930789600385171\n",
      "Saved model!\n",
      "Best thresholds ===>>> class-0>>0.5975241661071777|class-1>>0.6107787489891052\n",
      "Epoch: 054, Train Loss: 0.0588, Test Loss: 0.0894, Train Acc: 0.8888, Test Acc: 0.7667, Test f1: 0.7905, AUC: class-0>>0.9805484693877551|class-1>>0.9174289841116995\n",
      "Epoch: 055, Train Loss: 0.0491, Test Loss: 0.0778, Train Acc: 0.8778, Test Acc: 0.7970, Test f1: 0.8223, AUC: class-0>>0.97906037414966|class-1>>0.9256138661531054\n",
      "Epoch: 056, Train Loss: 0.0462, Test Loss: 0.0750, Train Acc: 0.8766, Test Acc: 0.7812, Test f1: 0.8070, AUC: class-0>>0.980017006802721|class-1>>0.9279008184882042\n",
      "Epoch: 057, Train Loss: 0.0541, Test Loss: 0.0825, Train Acc: 0.8636, Test Acc: 0.7904, Test f1: 0.8162, AUC: class-0>>0.9813988095238095|class-1>>0.9411410688493019\n",
      "Epoch: 058, Train Loss: 0.0465, Test Loss: 0.0743, Train Acc: 0.8897, Test Acc: 0.7706, Test f1: 0.7964, AUC: class-0>>0.980654761904762|class-1>>0.9356042368801155\n",
      "Epoch: 059, Train Loss: 0.0545, Test Loss: 0.0841, Train Acc: 0.8709, Test Acc: 0.7679, Test f1: 0.7944, AUC: class-0>>0.9762967687074829|class-1>>0.9140587385652383\n",
      "Epoch: 060, Train Loss: 0.0486, Test Loss: 0.0782, Train Acc: 0.8874, Test Acc: 0.7507, Test f1: 0.7752, AUC: class-0>>0.9817176870748299|class-1>>0.929826673086182\n",
      "Epoch: 061, Train Loss: 0.0465, Test Loss: 0.0755, Train Acc: 0.8704, Test Acc: 0.7759, Test f1: 0.8025, AUC: class-0>>0.9816113945578231|class-1>>0.922002888781897\n",
      "Epoch: 062, Train Loss: 0.0484, Test Loss: 0.0770, Train Acc: 0.8858, Test Acc: 0.7851, Test f1: 0.8106, AUC: class-0>>0.9816113945578231|class-1>>0.9313914299470389\n",
      "Epoch: 063, Train Loss: 0.0448, Test Loss: 0.0753, Train Acc: 0.8905, Test Acc: 0.7891, Test f1: 0.8148, AUC: class-0>>0.981186224489796|class-1>>0.9305488685604237\n",
      "Epoch: 064, Train Loss: 0.0567, Test Loss: 0.0852, Train Acc: 0.8790, Test Acc: 0.7943, Test f1: 0.8200, AUC: class-0>>0.9829931972789115|class-1>>0.937048627828599\n",
      "Epoch: 065, Train Loss: 0.0514, Test Loss: 0.0825, Train Acc: 0.8944, Test Acc: 0.7491, Test f1: 0.7762, AUC: class-0>>0.9817176870748299|class-1>>0.9175493500240731\n",
      "Epoch: 066, Train Loss: 0.0452, Test Loss: 0.0742, Train Acc: 0.8966, Test Acc: 0.7480, Test f1: 0.7729, AUC: class-0>>0.9798044217687074|class-1>>0.9366875300914781\n",
      "Epoch: 067, Train Loss: 0.0458, Test Loss: 0.0761, Train Acc: 0.8846, Test Acc: 0.8037, Test f1: 0.8283, AUC: class-0>>0.9801232993197279|class-1>>0.9306692344727973\n",
      "Epoch: 068, Train Loss: 0.0443, Test Loss: 0.0745, Train Acc: 0.8930, Test Acc: 0.8024, Test f1: 0.8253, AUC: class-0>>0.981079931972789|class-1>>0.9340394800192586\n",
      "Epoch: 069, Train Loss: 0.0542, Test Loss: 0.0833, Train Acc: 0.8697, Test Acc: 0.7159, Test f1: 0.7301, AUC: class-0>>0.980123299319728|class-1>>0.9425854597977852\n",
      "Epoch: 070, Train Loss: 0.0508, Test Loss: 0.0825, Train Acc: 0.8821, Test Acc: 0.7904, Test f1: 0.8162, AUC: class-0>>0.9809736394557823|class-1>>0.9305488685604237\n",
      "Epoch: 071, Train Loss: 0.0480, Test Loss: 0.0793, Train Acc: 0.8916, Test Acc: 0.7891, Test f1: 0.8161, AUC: class-0>>0.9803358843537414|class-1>>0.9328358208955223\n",
      "Epoch: 072, Train Loss: 0.0478, Test Loss: 0.0804, Train Acc: 0.9034, Test Acc: 0.7786, Test f1: 0.8046, AUC: class-0>>0.9793792517006803|class-1>>0.9164660568127106\n",
      "Epoch: 073, Train Loss: 0.0445, Test Loss: 0.0761, Train Acc: 0.9092, Test Acc: 0.7680, Test f1: 0.7929, AUC: class-0>>0.980654761904762|class-1>>0.9250120365912373\n",
      "Epoch: 074, Train Loss: 0.0532, Test Loss: 0.0855, Train Acc: 0.8859, Test Acc: 0.7704, Test f1: 0.7979, AUC: class-0>>0.9815051020408163|class-1>>0.9382522869523351\n",
      "Epoch: 075, Train Loss: 0.0453, Test Loss: 0.0776, Train Acc: 0.8940, Test Acc: 0.7743, Test f1: 0.8022, AUC: class-0>>0.9828869047619048|class-1>>0.9347616754935002\n",
      "Epoch: 076, Train Loss: 0.0421, Test Loss: 0.0730, Train Acc: 0.9045, Test Acc: 0.7811, Test f1: 0.8077, AUC: class-0>>0.9821428571428572|class-1>>0.9365671641791045\n",
      "Epoch: 077, Train Loss: 0.0471, Test Loss: 0.0791, Train Acc: 0.9012, Test Acc: 0.7770, Test f1: 0.8042, AUC: class-0>>0.9819302721088435|class-1>>0.939215214251324\n",
      "Epoch: 078, Train Loss: 0.0724, Test Loss: 0.1015, Train Acc: 0.9015, Test Acc: 0.7971, Test f1: 0.8215, AUC: class-0>>0.9821428571428571|class-1>>0.9044294655753491\n",
      "Epoch: 079, Train Loss: 0.0512, Test Loss: 0.0833, Train Acc: 0.8975, Test Acc: 0.7824, Test f1: 0.8103, AUC: class-0>>0.9783163265306123|class-1>>0.9333172845450168\n",
      "Epoch: 080, Train Loss: 0.0838, Test Loss: 0.1139, Train Acc: 0.9000, Test Acc: 0.7827, Test f1: 0.8065, AUC: class-0>>0.983312074829932|class-1>>0.9049109292248436\n",
      "Epoch: 081, Train Loss: 0.0406, Test Loss: 0.0729, Train Acc: 0.9067, Test Acc: 0.7720, Test f1: 0.7974, AUC: class-0>>0.9819302721088435|class-1>>0.93608570052961\n",
      "Epoch: 082, Train Loss: 0.0418, Test Loss: 0.0750, Train Acc: 0.9021, Test Acc: 0.7878, Test f1: 0.8144, AUC: class-0>>0.9830994897959183|class-1>>0.9201974000962927\n",
      "Epoch: 083, Train Loss: 0.0397, Test Loss: 0.0724, Train Acc: 0.9113, Test Acc: 0.7878, Test f1: 0.8126, AUC: class-0>>0.9819302721088435|class-1>>0.937048627828599\n",
      "Epoch: 084, Train Loss: 0.0426, Test Loss: 0.0763, Train Acc: 0.9114, Test Acc: 0.7905, Test f1: 0.8143, AUC: class-0>>0.9825680272108844|class-1>>0.9342802118440058\n",
      "Epoch: 085, Train Loss: 0.0459, Test Loss: 0.0805, Train Acc: 0.9092, Test Acc: 0.7957, Test f1: 0.8222, AUC: class-0>>0.9783163265306123|class-1>>0.9312710640346653\n",
      "Epoch: 086, Train Loss: 0.0449, Test Loss: 0.0780, Train Acc: 0.9091, Test Acc: 0.7837, Test f1: 0.8101, AUC: class-0>>0.9819302721088434|class-1>>0.9336783822821376\n",
      "Epoch: 087, Train Loss: 0.0398, Test Loss: 0.0741, Train Acc: 0.9139, Test Acc: 0.7945, Test f1: 0.8187, AUC: class-0>>0.982249149659864|class-1>>0.9353635050553684\n",
      "Epoch: 088, Train Loss: 0.0407, Test Loss: 0.0752, Train Acc: 0.9055, Test Acc: 0.7891, Test f1: 0.8161, AUC: class-0>>0.980123299319728|class-1>>0.9352431391429947\n",
      "Epoch: 089, Train Loss: 0.0438, Test Loss: 0.0782, Train Acc: 0.9114, Test Acc: 0.7653, Test f1: 0.7907, AUC: class-0>>0.9821428571428572|class-1>>0.9359653346172364\n",
      "Epoch: 090, Train Loss: 0.0416, Test Loss: 0.0761, Train Acc: 0.9054, Test Acc: 0.7984, Test f1: 0.8228, AUC: class-0>>0.9841624149659864|class-1>>0.929826673086182\n",
      "Epoch: 091, Train Loss: 0.0537, Test Loss: 0.0878, Train Acc: 0.9057, Test Acc: 0.7983, Test f1: 0.8249, AUC: class-0>>0.9760841836734694|class-1>>0.9358449687048629\n",
      "Epoch: 092, Train Loss: 0.0463, Test Loss: 0.0829, Train Acc: 0.9020, Test Acc: 0.8103, Test f1: 0.8356, AUC: class-0>>0.9786352040816326|class-1>>0.9287433798748195\n",
      "Epoch: 093, Train Loss: 0.0375, Test Loss: 0.0734, Train Acc: 0.9183, Test Acc: 0.7903, Test f1: 0.8172, AUC: class-0>>0.9830994897959184|class-1>>0.9321136254212806\n",
      "Epoch: 094, Train Loss: 0.0391, Test Loss: 0.0753, Train Acc: 0.9077, Test Acc: 0.8051, Test f1: 0.8289, AUC: class-0>>0.9841624149659864|class-1>>0.9245305729417429\n",
      "Epoch: 095, Train Loss: 0.0414, Test Loss: 0.0767, Train Acc: 0.9220, Test Acc: 0.7879, Test f1: 0.8119, AUC: class-0>>0.9823554421768708|class-1>>0.9253731343283582\n",
      "Epoch: 096, Train Loss: 0.0392, Test Loss: 0.0755, Train Acc: 0.9163, Test Acc: 0.8038, Test f1: 0.8258, AUC: class-0>>0.9818239795918366|class-1>>0.9381319210399615\n",
      "Epoch: 097, Train Loss: 0.0384, Test Loss: 0.0743, Train Acc: 0.9208, Test Acc: 0.7824, Test f1: 0.8091, AUC: class-0>>0.9828869047619049|class-1>>0.9368078960038517\n",
      "Epoch: 098, Train Loss: 0.0400, Test Loss: 0.0749, Train Acc: 0.9125, Test Acc: 0.7891, Test f1: 0.8150, AUC: class-0>>0.9830994897959184|class-1>>0.9383726528647087\n",
      "Epoch: 099, Train Loss: 0.0474, Test Loss: 0.0862, Train Acc: 0.9175, Test Acc: 0.7891, Test f1: 0.8168, AUC: class-0>>0.9793792517006802|class-1>>0.906355320173327\n",
      "Epoch: 100, Train Loss: 0.0378, Test Loss: 0.0757, Train Acc: 0.9113, Test Acc: 0.8011, Test f1: 0.8257, AUC: class-0>>0.9826743197278911|class-1>>0.9199566682715455\n",
      "Epoch: 101, Train Loss: 0.0366, Test Loss: 0.0735, Train Acc: 0.9207, Test Acc: 0.7743, Test f1: 0.8023, AUC: class-0>>0.9824617346938775|class-1>>0.9318728935965335\n",
      "Epoch: 102, Train Loss: 0.0355, Test Loss: 0.0736, Train Acc: 0.9278, Test Acc: 0.7892, Test f1: 0.8136, AUC: class-0>>0.9829931972789115|class-1>>0.9301877708233028\n",
      "Epoch: 103, Train Loss: 0.0427, Test Loss: 0.0787, Train Acc: 0.9184, Test Acc: 0.7743, Test f1: 0.8023, AUC: class-0>>0.9834183673469388|class-1>>0.9359653346172364\n",
      "Epoch: 104, Train Loss: 0.0381, Test Loss: 0.0766, Train Acc: 0.9290, Test Acc: 0.8024, Test f1: 0.8273, AUC: class-0>>0.9833120748299319|class-1>>0.9312710640346653\n",
      "Epoch: 105, Train Loss: 0.0380, Test Loss: 0.0749, Train Acc: 0.9289, Test Acc: 0.7716, Test f1: 0.8004, AUC: class-0>>0.9826743197278912|class-1>>0.9400577756379394\n",
      "Epoch: 106, Train Loss: 0.0398, Test Loss: 0.0790, Train Acc: 0.9278, Test Acc: 0.8051, Test f1: 0.8285, AUC: class-0>>0.9821428571428572|class-1>>0.9287433798748195\n",
      "Epoch: 107, Train Loss: 0.0356, Test Loss: 0.0739, Train Acc: 0.9301, Test Acc: 0.7651, Test f1: 0.7927, AUC: class-0>>0.9821428571428572|class-1>>0.9335580163697641\n",
      "Epoch: 108, Train Loss: 0.0347, Test Loss: 0.0736, Train Acc: 0.9312, Test Acc: 0.7864, Test f1: 0.8127, AUC: class-0>>0.9827806122448979|class-1>>0.9309099662975446\n",
      "Epoch: 109, Train Loss: 0.0354, Test Loss: 0.0754, Train Acc: 0.9310, Test Acc: 0.7878, Test f1: 0.8139, AUC: class-0>>0.9830994897959184|class-1>>0.9318728935965335\n",
      "Epoch: 110, Train Loss: 0.0336, Test Loss: 0.0739, Train Acc: 0.9312, Test Acc: 0.7744, Test f1: 0.8015, AUC: class-0>>0.9835246598639455|class-1>>0.9323543572460279\n",
      "Epoch: 111, Train Loss: 0.0368, Test Loss: 0.0765, Train Acc: 0.9290, Test Acc: 0.7797, Test f1: 0.8071, AUC: class-0>>0.9823554421768708|class-1>>0.9356042368801155\n",
      "Epoch: 112, Train Loss: 0.0399, Test Loss: 0.0802, Train Acc: 0.9288, Test Acc: 0.7984, Test f1: 0.8228, AUC: class-0>>0.9834183673469388|class-1>>0.933919114106885\n",
      "Epoch: 113, Train Loss: 0.0374, Test Loss: 0.0773, Train Acc: 0.9301, Test Acc: 0.7823, Test f1: 0.8096, AUC: class-0>>0.9824617346938775|class-1>>0.9390948483389504\n",
      "Epoch: 114, Train Loss: 0.0337, Test Loss: 0.0741, Train Acc: 0.9311, Test Acc: 0.7970, Test f1: 0.8233, AUC: class-0>>0.983312074829932|class-1>>0.9307896003851709\n",
      "Epoch: 115, Train Loss: 0.0337, Test Loss: 0.0745, Train Acc: 0.9311, Test Acc: 0.7930, Test f1: 0.8195, AUC: class-0>>0.9826743197278911|class-1>>0.931030332209918\n",
      "Epoch: 116, Train Loss: 0.0328, Test Loss: 0.0738, Train Acc: 0.9324, Test Acc: 0.7903, Test f1: 0.8175, AUC: class-0>>0.983312074829932|class-1>>0.9292248435243139\n",
      "Epoch: 117, Train Loss: 0.0343, Test Loss: 0.0751, Train Acc: 0.9300, Test Acc: 0.7930, Test f1: 0.8195, AUC: class-0>>0.9818239795918366|class-1>>0.934520943668753\n",
      "Epoch: 118, Train Loss: 0.0329, Test Loss: 0.0744, Train Acc: 0.9346, Test Acc: 0.8051, Test f1: 0.8289, AUC: class-0>>0.982780612244898|class-1>>0.9318728935965335\n",
      "Epoch: 119, Train Loss: 0.0324, Test Loss: 0.0734, Train Acc: 0.9322, Test Acc: 0.7930, Test f1: 0.8195, AUC: class-0>>0.9826743197278911|class-1>>0.9318728935965335\n",
      "Epoch: 120, Train Loss: 0.0355, Test Loss: 0.0770, Train Acc: 0.9347, Test Acc: 0.7970, Test f1: 0.8223, AUC: class-0>>0.983312074829932|class-1>>0.9195955705344246\n",
      "Epoch: 121, Train Loss: 0.0365, Test Loss: 0.0780, Train Acc: 0.9370, Test Acc: 0.7997, Test f1: 0.8254, AUC: class-0>>0.9813988095238095|class-1>>0.9281415503129513\n",
      "Epoch: 122, Train Loss: 0.0410, Test Loss: 0.0826, Train Acc: 0.9310, Test Acc: 0.7997, Test f1: 0.8256, AUC: class-0>>0.982249149659864|class-1>>0.9305488685604237\n",
      "Epoch: 123, Train Loss: 0.0344, Test Loss: 0.0766, Train Acc: 0.9381, Test Acc: 0.8037, Test f1: 0.8294, AUC: class-0>>0.9824617346938775|class-1>>0.9288637457871931\n",
      "Epoch: 124, Train Loss: 0.0328, Test Loss: 0.0748, Train Acc: 0.9322, Test Acc: 0.7943, Test f1: 0.8206, AUC: class-0>>0.9821428571428571|class-1>>0.9305488685604236\n",
      "Epoch: 125, Train Loss: 0.0318, Test Loss: 0.0742, Train Acc: 0.9347, Test Acc: 0.7864, Test f1: 0.8130, AUC: class-0>>0.9823554421768708|class-1>>0.9291044776119403\n",
      "Epoch: 126, Train Loss: 0.0332, Test Loss: 0.0752, Train Acc: 0.9346, Test Acc: 0.7757, Test f1: 0.8033, AUC: class-0>>0.9821428571428572|class-1>>0.932956186807896\n",
      "Epoch: 127, Train Loss: 0.0321, Test Loss: 0.0743, Train Acc: 0.9334, Test Acc: 0.7784, Test f1: 0.8059, AUC: class-0>>0.9823554421768708|class-1>>0.9339191141068849\n",
      "Epoch: 128, Train Loss: 0.0315, Test Loss: 0.0739, Train Acc: 0.9370, Test Acc: 0.7891, Test f1: 0.8150, AUC: class-0>>0.9823554421768708|class-1>>0.9304285026480501\n",
      "Epoch: 129, Train Loss: 0.0315, Test Loss: 0.0745, Train Acc: 0.9346, Test Acc: 0.7744, Test f1: 0.8015, AUC: class-0>>0.982249149659864|class-1>>0.9315117958594126\n",
      "Epoch: 130, Train Loss: 0.0316, Test Loss: 0.0744, Train Acc: 0.9334, Test Acc: 0.7824, Test f1: 0.8099, AUC: class-0>>0.9824617346938775|class-1>>0.9319932595089071\n",
      "Epoch: 131, Train Loss: 0.0314, Test Loss: 0.0745, Train Acc: 0.9347, Test Acc: 0.7970, Test f1: 0.8235, AUC: class-0>>0.9825680272108843|class-1>>0.9293452094366875\n",
      "Epoch: 132, Train Loss: 0.0316, Test Loss: 0.0748, Train Acc: 0.9346, Test Acc: 0.7864, Test f1: 0.8135, AUC: class-0>>0.9823554421768708|class-1>>0.9316321617717862\n",
      "Epoch: 133, Train Loss: 0.0315, Test Loss: 0.0746, Train Acc: 0.9380, Test Acc: 0.7824, Test f1: 0.8099, AUC: class-0>>0.9825680272108843|class-1>>0.9309099662975445\n",
      "Epoch: 134, Train Loss: 0.0312, Test Loss: 0.0747, Train Acc: 0.9359, Test Acc: 0.7784, Test f1: 0.8059, AUC: class-0>>0.9821428571428572|class-1>>0.929465575349061\n",
      "Epoch: 135, Train Loss: 0.0308, Test Loss: 0.0744, Train Acc: 0.9359, Test Acc: 0.7930, Test f1: 0.8195, AUC: class-0>>0.9825680272108843|class-1>>0.929465575349061\n",
      "Epoch: 136, Train Loss: 0.0308, Test Loss: 0.0743, Train Acc: 0.9369, Test Acc: 0.7930, Test f1: 0.8195, AUC: class-0>>0.982249149659864|class-1>>0.9300674049109292\n",
      "Epoch: 137, Train Loss: 0.0313, Test Loss: 0.0747, Train Acc: 0.9380, Test Acc: 0.7970, Test f1: 0.8235, AUC: class-0>>0.9823554421768707|class-1>>0.9301877708233028\n",
      "Epoch: 138, Train Loss: 0.0311, Test Loss: 0.0746, Train Acc: 0.9381, Test Acc: 0.7970, Test f1: 0.8235, AUC: class-0>>0.9825680272108843|class-1>>0.9291044776119404\n",
      "Epoch: 139, Train Loss: 0.0310, Test Loss: 0.0747, Train Acc: 0.9346, Test Acc: 0.7824, Test f1: 0.8103, AUC: class-0>>0.9824617346938775|class-1>>0.9322339913336543\n",
      "Epoch: 140, Train Loss: 0.0306, Test Loss: 0.0745, Train Acc: 0.9370, Test Acc: 0.7930, Test f1: 0.8195, AUC: class-0>>0.9820365646258504|class-1>>0.9294655753490612\n",
      "Epoch: 141, Train Loss: 0.0306, Test Loss: 0.0747, Train Acc: 0.9369, Test Acc: 0.7930, Test f1: 0.8195, AUC: class-0>>0.982249149659864|class-1>>0.9289841116995666\n",
      "Epoch: 142, Train Loss: 0.0311, Test Loss: 0.0749, Train Acc: 0.9392, Test Acc: 0.7970, Test f1: 0.8235, AUC: class-0>>0.9824617346938775|class-1>>0.9293452094366876\n",
      "Epoch: 143, Train Loss: 0.0306, Test Loss: 0.0746, Train Acc: 0.9393, Test Acc: 0.7903, Test f1: 0.8172, AUC: class-0>>0.9823554421768708|class-1>>0.9280211844005778\n",
      "Epoch: 144, Train Loss: 0.0306, Test Loss: 0.0746, Train Acc: 0.9392, Test Acc: 0.7970, Test f1: 0.8235, AUC: class-0>>0.9825680272108843|class-1>>0.9299470389985556\n",
      "Epoch: 145, Train Loss: 0.0309, Test Loss: 0.0752, Train Acc: 0.9392, Test Acc: 0.7784, Test f1: 0.8059, AUC: class-0>>0.98171768707483|class-1>>0.9306692344727973\n",
      "Epoch: 146, Train Loss: 0.0305, Test Loss: 0.0747, Train Acc: 0.9392, Test Acc: 0.7784, Test f1: 0.8059, AUC: class-0>>0.9821428571428571|class-1>>0.9297063071738083\n",
      "Epoch: 147, Train Loss: 0.0306, Test Loss: 0.0749, Train Acc: 0.9392, Test Acc: 0.7784, Test f1: 0.8059, AUC: class-0>>0.9819302721088435|class-1>>0.9291044776119403\n",
      "Epoch: 148, Train Loss: 0.0303, Test Loss: 0.0745, Train Acc: 0.9392, Test Acc: 0.7930, Test f1: 0.8195, AUC: class-0>>0.9820365646258503|class-1>>0.9293452094366875\n",
      "Epoch: 149, Train Loss: 0.0303, Test Loss: 0.0746, Train Acc: 0.9392, Test Acc: 0.7930, Test f1: 0.8195, AUC: class-0>>0.9819302721088435|class-1>>0.9289841116995666\n",
      "Running cross-validation fold: 02\n",
      "Epoch: 001, Train Loss: 0.2502, Test Loss: 0.2621, Train Acc: 0.6852, Test Acc: 0.7936, Test f1: 0.8210, AUC: class-0>>0.9639837143751958|class-1>>0.9355797101449276\n",
      "Saved model!\n",
      "Best thresholds ===>>> class-0>>0.5714393854141235|class-1>>0.5710734128952026\n",
      "Epoch: 002, Train Loss: 0.0970, Test Loss: 0.0822, Train Acc: 0.7228, Test Acc: 0.8381, Test f1: 0.8663, AUC: class-0>>0.9722309218081219|class-1>>0.961231884057971\n",
      "Saved model!\n",
      "Best thresholds ===>>> class-0>>0.6535552740097046|class-1>>0.620881199836731\n",
      "Epoch: 003, Train Loss: 0.1378, Test Loss: 0.1162, Train Acc: 0.7184, Test Acc: 0.8372, Test f1: 0.8715, AUC: class-0>>0.9756759578244075|class-1>>0.9671739130434782\n",
      "Saved model!\n",
      "Best thresholds ===>>> class-0>>0.6848602294921875|class-1>>0.7124695181846619\n",
      "Epoch: 004, Train Loss: 0.0896, Test Loss: 0.0787, Train Acc: 0.7771, Test Acc: 0.8480, Test f1: 0.8786, AUC: class-0>>0.9758847478860007|class-1>>0.9618115942028986\n",
      "Saved model!\n",
      "Best thresholds ===>>> class-0>>0.6275126338005066|class-1>>0.6174363493919373\n",
      "Epoch: 005, Train Loss: 0.0818, Test Loss: 0.0692, Train Acc: 0.7590, Test Acc: 0.8303, Test f1: 0.8644, AUC: class-0>>0.9795385739638792|class-1>>0.9641304347826087\n",
      "Epoch: 006, Train Loss: 0.0849, Test Loss: 0.0727, Train Acc: 0.7490, Test Acc: 0.8377, Test f1: 0.8688, AUC: class-0>>0.9792253888714897|class-1>>0.9664492753623188\n",
      "Epoch: 007, Train Loss: 0.0831, Test Loss: 0.0761, Train Acc: 0.7840, Test Acc: 0.8025, Test f1: 0.8354, AUC: class-0>>0.9803737342102516|class-1>>0.9641304347826086\n",
      "Epoch: 008, Train Loss: 0.0814, Test Loss: 0.0714, Train Acc: 0.7625, Test Acc: 0.8162, Test f1: 0.8502, AUC: class-0>>0.9822528447645892|class-1>>0.9665942028985507\n",
      "Epoch: 009, Train Loss: 0.1033, Test Loss: 0.0917, Train Acc: 0.7109, Test Acc: 0.8107, Test f1: 0.8369, AUC: class-0>>0.9701430211921913|class-1>>0.9605072463768115\n",
      "Epoch: 010, Train Loss: 0.1213, Test Loss: 0.1001, Train Acc: 0.7260, Test Acc: 0.7888, Test f1: 0.8230, AUC: class-0>>0.9736924522392735|class-1>>0.9583333333333333\n",
      "Epoch: 011, Train Loss: 0.0977, Test Loss: 0.0841, Train Acc: 0.7142, Test Acc: 0.8130, Test f1: 0.8487, AUC: class-0>>0.9746320075164422|class-1>>0.9516666666666667\n",
      "Epoch: 012, Train Loss: 0.0976, Test Loss: 0.0866, Train Acc: 0.7509, Test Acc: 0.8586, Test f1: 0.8871, AUC: class-0>>0.9777638584403382|class-1>>0.9583333333333333\n",
      "Saved model!\n",
      "Best thresholds ===>>> class-0>>0.6176744699478149|class-1>>0.6021955609321594\n",
      "Epoch: 013, Train Loss: 0.0839, Test Loss: 0.0738, Train Acc: 0.7795, Test Acc: 0.8213, Test f1: 0.8463, AUC: class-0>>0.9783902286251175|class-1>>0.9657246376811595\n",
      "Epoch: 014, Train Loss: 0.0844, Test Loss: 0.0786, Train Acc: 0.7737, Test Acc: 0.8207, Test f1: 0.8498, AUC: class-0>>0.979851759056269|class-1>>0.964855072463768\n",
      "Epoch: 015, Train Loss: 0.0739, Test Loss: 0.0658, Train Acc: 0.7867, Test Acc: 0.7916, Test f1: 0.8264, AUC: class-0>>0.982774819918572|class-1>>0.9650724637681158\n",
      "Epoch: 016, Train Loss: 0.0790, Test Loss: 0.0702, Train Acc: 0.7683, Test Acc: 0.8315, Test f1: 0.8581, AUC: class-0>>0.9823572397953858|class-1>>0.9647101449275363\n",
      "Epoch: 017, Train Loss: 0.0806, Test Loss: 0.0693, Train Acc: 0.7784, Test Acc: 0.8596, Test f1: 0.8819, AUC: class-0>>0.9839231652573337|class-1>>0.9641304347826086\n",
      "Saved model!\n",
      "Best thresholds ===>>> class-0>>0.6729952096939087|class-1>>0.6596769690513611\n",
      "Epoch: 018, Train Loss: 0.0713, Test Loss: 0.0622, Train Acc: 0.8000, Test Acc: 0.8275, Test f1: 0.8563, AUC: class-0>>0.9844451404113164|class-1>>0.965\n",
      "Epoch: 019, Train Loss: 0.0720, Test Loss: 0.0625, Train Acc: 0.8075, Test Acc: 0.8440, Test f1: 0.8777, AUC: class-0>>0.9858022758116713|class-1>>0.965\n",
      "Epoch: 020, Train Loss: 0.0668, Test Loss: 0.0602, Train Acc: 0.8031, Test Acc: 0.8019, Test f1: 0.8377, AUC: class-0>>0.986533041027247|class-1>>0.9651449275362319\n",
      "Epoch: 021, Train Loss: 0.0740, Test Loss: 0.0650, Train Acc: 0.7933, Test Acc: 0.8168, Test f1: 0.8477, AUC: class-0>>0.9853846956884852|class-1>>0.9651449275362319\n",
      "Epoch: 022, Train Loss: 0.0663, Test Loss: 0.0594, Train Acc: 0.8220, Test Acc: 0.8019, Test f1: 0.8377, AUC: class-0>>0.986324250965654|class-1>>0.966159420289855\n",
      "Epoch: 023, Train Loss: 0.0773, Test Loss: 0.0688, Train Acc: 0.8091, Test Acc: 0.8104, Test f1: 0.8382, AUC: class-0>>0.9839231652573337|class-1>>0.9652898550724637\n",
      "Epoch: 024, Train Loss: 0.0758, Test Loss: 0.0694, Train Acc: 0.8191, Test Acc: 0.8332, Test f1: 0.8696, AUC: class-0>>0.9859066708424679|class-1>>0.9658695652173913\n",
      "Epoch: 025, Train Loss: 0.0688, Test Loss: 0.0632, Train Acc: 0.8035, Test Acc: 0.8212, Test f1: 0.8476, AUC: class-0>>0.9867418310888402|class-1>>0.967463768115942\n",
      "Epoch: 026, Train Loss: 0.0652, Test Loss: 0.0627, Train Acc: 0.7981, Test Acc: 0.8087, Test f1: 0.8465, AUC: class-0>>0.9868462261196367|class-1>>0.965\n",
      "Epoch: 027, Train Loss: 0.0732, Test Loss: 0.0745, Train Acc: 0.8014, Test Acc: 0.8249, Test f1: 0.8491, AUC: class-0>>0.9860110658732644|class-1>>0.9651449275362318\n",
      "Epoch: 028, Train Loss: 0.0665, Test Loss: 0.0644, Train Acc: 0.8064, Test Acc: 0.8210, Test f1: 0.8481, AUC: class-0>>0.987890176427602|class-1>>0.9645652173913043\n",
      "Epoch: 029, Train Loss: 0.0690, Test Loss: 0.0633, Train Acc: 0.8110, Test Acc: 0.8518, Test f1: 0.8797, AUC: class-0>>0.9873682012736194|class-1>>0.9660144927536233\n",
      "Epoch: 030, Train Loss: 0.0682, Test Loss: 0.0654, Train Acc: 0.8012, Test Acc: 0.8175, Test f1: 0.8447, AUC: class-0>>0.9885165466123813|class-1>>0.9673188405797102\n",
      "Epoch: 031, Train Loss: 0.0645, Test Loss: 0.0602, Train Acc: 0.8081, Test Acc: 0.8167, Test f1: 0.8489, AUC: class-0>>0.9884121515815847|class-1>>0.9636956521739131\n",
      "Epoch: 032, Train Loss: 0.0617, Test Loss: 0.0577, Train Acc: 0.8068, Test Acc: 0.8173, Test f1: 0.8461, AUC: class-0>>0.9891429167971604|class-1>>0.9655797101449276\n",
      "Epoch: 033, Train Loss: 0.0748, Test Loss: 0.0669, Train Acc: 0.8265, Test Acc: 0.8378, Test f1: 0.8675, AUC: class-0>>0.9884121515815847|class-1>>0.9667391304347825\n",
      "Epoch: 034, Train Loss: 0.0605, Test Loss: 0.0608, Train Acc: 0.8071, Test Acc: 0.8212, Test f1: 0.8480, AUC: class-0>>0.9888297317047707|class-1>>0.9655797101449275\n",
      "Epoch: 035, Train Loss: 0.0673, Test Loss: 0.0653, Train Acc: 0.8060, Test Acc: 0.8378, Test f1: 0.8669, AUC: class-0>>0.9888297317047708|class-1>>0.9642753623188406\n",
      "Epoch: 036, Train Loss: 0.1770, Test Loss: 0.1773, Train Acc: 0.7905, Test Acc: 0.7971, Test f1: 0.8207, AUC: class-0>>0.9533354212339493|class-1>>0.9651449275362319\n",
      "Epoch: 037, Train Loss: 0.0695, Test Loss: 0.0633, Train Acc: 0.8321, Test Acc: 0.8242, Test f1: 0.8533, AUC: class-0>>0.9879945714583985|class-1>>0.965\n",
      "Epoch: 038, Train Loss: 0.0676, Test Loss: 0.0632, Train Acc: 0.8226, Test Acc: 0.8352, Test f1: 0.8599, AUC: class-0>>0.9871594112120263|class-1>>0.9671739130434783\n",
      "Epoch: 039, Train Loss: 0.0596, Test Loss: 0.0598, Train Acc: 0.8162, Test Acc: 0.8141, Test f1: 0.8421, AUC: class-0>>0.9886209416431777|class-1>>0.9660144927536233\n",
      "Epoch: 040, Train Loss: 0.0606, Test Loss: 0.0620, Train Acc: 0.8190, Test Acc: 0.8196, Test f1: 0.8550, AUC: class-0>>0.9875769913352124|class-1>>0.9641304347826086\n",
      "Epoch: 041, Train Loss: 0.0762, Test Loss: 0.0695, Train Acc: 0.7971, Test Acc: 0.8213, Test f1: 0.8463, AUC: class-0>>0.9887253366739743|class-1>>0.9676086956521739\n",
      "Epoch: 042, Train Loss: 0.0622, Test Loss: 0.0592, Train Acc: 0.8081, Test Acc: 0.8199, Test f1: 0.8538, AUC: class-0>>0.9882033615199917|class-1>>0.9667391304347825\n",
      "Epoch: 043, Train Loss: 0.0581, Test Loss: 0.0582, Train Acc: 0.8169, Test Acc: 0.8135, Test f1: 0.8445, AUC: class-0>>0.9889341267355674|class-1>>0.9650000000000001\n",
      "Epoch: 044, Train Loss: 0.0654, Test Loss: 0.0623, Train Acc: 0.8185, Test Acc: 0.8050, Test f1: 0.8440, AUC: class-0>>0.9900824720743293|class-1>>0.9655797101449275\n",
      "Epoch: 045, Train Loss: 0.0599, Test Loss: 0.0609, Train Acc: 0.8119, Test Acc: 0.8346, Test f1: 0.8633, AUC: class-0>>0.9895604969203466|class-1>>0.9657246376811593\n",
      "Epoch: 046, Train Loss: 0.0600, Test Loss: 0.0578, Train Acc: 0.8073, Test Acc: 0.8167, Test f1: 0.8489, AUC: class-0>>0.9896648919511432|class-1>>0.9671014492753622\n",
      "Epoch: 047, Train Loss: 0.0655, Test Loss: 0.0633, Train Acc: 0.8107, Test Acc: 0.8307, Test f1: 0.8617, AUC: class-0>>0.989664891951143|class-1>>0.9660144927536232\n",
      "Epoch: 048, Train Loss: 0.0569, Test Loss: 0.0564, Train Acc: 0.8259, Test Acc: 0.8205, Test f1: 0.8509, AUC: class-0>>0.9889341267355674|class-1>>0.966304347826087\n",
      "Epoch: 049, Train Loss: 0.0565, Test Loss: 0.0562, Train Acc: 0.8150, Test Acc: 0.8167, Test f1: 0.8489, AUC: class-0>>0.9886209416431777|class-1>>0.966159420289855\n",
      "Epoch: 050, Train Loss: 0.0591, Test Loss: 0.0586, Train Acc: 0.8189, Test Acc: 0.8238, Test f1: 0.8557, AUC: class-0>>0.9886209416431777|class-1>>0.9667391304347825\n",
      "Epoch: 051, Train Loss: 0.0643, Test Loss: 0.0612, Train Acc: 0.8100, Test Acc: 0.8199, Test f1: 0.8538, AUC: class-0>>0.9884121515815847|class-1>>0.967463768115942\n",
      "Epoch: 052, Train Loss: 0.0560, Test Loss: 0.0560, Train Acc: 0.8163, Test Acc: 0.8238, Test f1: 0.8557, AUC: class-0>>0.9889341267355674|class-1>>0.967463768115942\n",
      "Epoch: 053, Train Loss: 0.0986, Test Loss: 0.0910, Train Acc: 0.7966, Test Acc: 0.8090, Test f1: 0.8435, AUC: class-0>>0.9860110658732645|class-1>>0.9647101449275363\n",
      "Epoch: 054, Train Loss: 0.0628, Test Loss: 0.0666, Train Acc: 0.8259, Test Acc: 0.8204, Test f1: 0.8514, AUC: class-0>>0.988098966489195|class-1>>0.9661594202898551\n",
      "Epoch: 055, Train Loss: 0.0627, Test Loss: 0.0659, Train Acc: 0.8309, Test Acc: 0.8125, Test f1: 0.8496, AUC: class-0>>0.9887253366739743|class-1>>0.9657246376811595\n",
      "Epoch: 056, Train Loss: 0.0566, Test Loss: 0.0571, Train Acc: 0.8424, Test Acc: 0.8276, Test f1: 0.8576, AUC: class-0>>0.9899780770435327|class-1>>0.9660144927536232\n",
      "Epoch: 057, Train Loss: 0.0616, Test Loss: 0.0660, Train Acc: 0.8359, Test Acc: 0.8133, Test f1: 0.8459, AUC: class-0>>0.988098966489195|class-1>>0.9648550724637682\n",
      "Epoch: 058, Train Loss: 0.0632, Test Loss: 0.0677, Train Acc: 0.8282, Test Acc: 0.8210, Test f1: 0.8488, AUC: class-0>>0.9893517068587536|class-1>>0.9658695652173913\n",
      "Epoch: 059, Train Loss: 0.0596, Test Loss: 0.0599, Train Acc: 0.8354, Test Acc: 0.8347, Test f1: 0.8634, AUC: class-0>>0.9897692869819397|class-1>>0.9661594202898551\n",
      "Epoch: 060, Train Loss: 0.0600, Test Loss: 0.0596, Train Acc: 0.8229, Test Acc: 0.8238, Test f1: 0.8563, AUC: class-0>>0.9905000521975154|class-1>>0.9671739130434783\n",
      "Epoch: 061, Train Loss: 0.0524, Test Loss: 0.0554, Train Acc: 0.8386, Test Acc: 0.8380, Test f1: 0.8682, AUC: class-0>>0.9901868671051257|class-1>>0.9664492753623188\n",
      "Epoch: 062, Train Loss: 0.0525, Test Loss: 0.0549, Train Acc: 0.8407, Test Acc: 0.8380, Test f1: 0.8682, AUC: class-0>>0.9901868671051257|class-1>>0.9668840579710145\n",
      "Epoch: 063, Train Loss: 0.0508, Test Loss: 0.0559, Train Acc: 0.8462, Test Acc: 0.8418, Test f1: 0.8691, AUC: class-0>>0.9900824720743293|class-1>>0.9670289855072464\n",
      "Epoch: 064, Train Loss: 0.0521, Test Loss: 0.0563, Train Acc: 0.8367, Test Acc: 0.8451, Test f1: 0.8740, AUC: class-0>>0.9893517068587534|class-1>>0.9655797101449275\n",
      "Epoch: 065, Train Loss: 0.0566, Test Loss: 0.0612, Train Acc: 0.8247, Test Acc: 0.8380, Test f1: 0.8682, AUC: class-0>>0.9909176323207014|class-1>>0.9667391304347825\n",
      "Epoch: 066, Train Loss: 0.0517, Test Loss: 0.0545, Train Acc: 0.8416, Test Acc: 0.8483, Test f1: 0.8789, AUC: class-0>>0.9891429167971604|class-1>>0.9670289855072464\n",
      "Epoch: 067, Train Loss: 0.0573, Test Loss: 0.0595, Train Acc: 0.8506, Test Acc: 0.8451, Test f1: 0.8735, AUC: class-0>>0.9902912621359223|class-1>>0.9673188405797102\n",
      "Epoch: 068, Train Loss: 0.0500, Test Loss: 0.0559, Train Acc: 0.8576, Test Acc: 0.8236, Test f1: 0.8567, AUC: class-0>>0.9909176323207015|class-1>>0.9661594202898551\n",
      "Epoch: 069, Train Loss: 0.0597, Test Loss: 0.0615, Train Acc: 0.8370, Test Acc: 0.8270, Test f1: 0.8606, AUC: class-0>>0.9905000521975154|class-1>>0.9676086956521739\n",
      "Epoch: 070, Train Loss: 0.0520, Test Loss: 0.0574, Train Acc: 0.8414, Test Acc: 0.8307, Test f1: 0.8632, AUC: class-0>>0.9899780770435327|class-1>>0.966014492753623\n",
      "Epoch: 071, Train Loss: 0.0489, Test Loss: 0.0536, Train Acc: 0.8347, Test Acc: 0.8451, Test f1: 0.8740, AUC: class-0>>0.9907088422591084|class-1>>0.9676086956521739\n",
      "Epoch: 072, Train Loss: 0.0517, Test Loss: 0.0544, Train Acc: 0.8381, Test Acc: 0.8418, Test f1: 0.8696, AUC: class-0>>0.9907088422591084|class-1>>0.9677536231884057\n",
      "Epoch: 073, Train Loss: 0.0595, Test Loss: 0.0618, Train Acc: 0.8478, Test Acc: 0.8412, Test f1: 0.8724, AUC: class-0>>0.989664891951143|class-1>>0.9673188405797102\n",
      "Epoch: 074, Train Loss: 0.0571, Test Loss: 0.0595, Train Acc: 0.8478, Test Acc: 0.8270, Test f1: 0.8606, AUC: class-0>>0.9903956571667188|class-1>>0.9684782608695651\n",
      "Epoch: 075, Train Loss: 0.0499, Test Loss: 0.0548, Train Acc: 0.8522, Test Acc: 0.8378, Test f1: 0.8690, AUC: class-0>>0.9901868671051257|class-1>>0.9677536231884057\n",
      "Epoch: 076, Train Loss: 0.0505, Test Loss: 0.0569, Train Acc: 0.8538, Test Acc: 0.8347, Test f1: 0.8634, AUC: class-0>>0.9905000521975154|class-1>>0.9680434782608697\n",
      "Epoch: 077, Train Loss: 0.0500, Test Loss: 0.0544, Train Acc: 0.8515, Test Acc: 0.8451, Test f1: 0.8740, AUC: class-0>>0.9901868671051258|class-1>>0.9686231884057971\n",
      "Epoch: 078, Train Loss: 0.0582, Test Loss: 0.0597, Train Acc: 0.8428, Test Acc: 0.8341, Test f1: 0.8673, AUC: class-0>>0.9902912621359223|class-1>>0.9689130434782608\n",
      "Epoch: 079, Train Loss: 0.0475, Test Loss: 0.0543, Train Acc: 0.8557, Test Acc: 0.8451, Test f1: 0.8740, AUC: class-0>>0.9909176323207016|class-1>>0.9680434782608696\n",
      "Epoch: 080, Train Loss: 0.0667, Test Loss: 0.0687, Train Acc: 0.8448, Test Acc: 0.8276, Test f1: 0.8581, AUC: class-0>>0.991230817413091|class-1>>0.9677536231884057\n",
      "Epoch: 081, Train Loss: 0.0491, Test Loss: 0.0556, Train Acc: 0.8623, Test Acc: 0.8378, Test f1: 0.8690, AUC: class-0>>0.9893517068587534|class-1>>0.9696376811594203\n",
      "Epoch: 082, Train Loss: 0.0496, Test Loss: 0.0587, Train Acc: 0.8621, Test Acc: 0.8386, Test f1: 0.8642, AUC: class-0>>0.9903956571667188|class-1>>0.9684782608695652\n",
      "Epoch: 083, Train Loss: 0.0470, Test Loss: 0.0544, Train Acc: 0.8627, Test Acc: 0.8378, Test f1: 0.8690, AUC: class-0>>0.9900824720743292|class-1>>0.967463768115942\n",
      "Epoch: 084, Train Loss: 0.0475, Test Loss: 0.0552, Train Acc: 0.8506, Test Acc: 0.8410, Test f1: 0.8739, AUC: class-0>>0.9900824720743293|class-1>>0.9671739130434781\n",
      "Epoch: 085, Train Loss: 0.0489, Test Loss: 0.0565, Train Acc: 0.8484, Test Acc: 0.8307, Test f1: 0.8627, AUC: class-0>>0.9895604969203466|class-1>>0.9673188405797101\n",
      "Epoch: 086, Train Loss: 0.0467, Test Loss: 0.0539, Train Acc: 0.8656, Test Acc: 0.8451, Test f1: 0.8740, AUC: class-0>>0.9905000521975154|class-1>>0.9686231884057971\n",
      "Epoch: 087, Train Loss: 0.0515, Test Loss: 0.0587, Train Acc: 0.8595, Test Acc: 0.8347, Test f1: 0.8634, AUC: class-0>>0.9905000521975154|class-1>>0.9684782608695651\n",
      "Epoch: 088, Train Loss: 0.0473, Test Loss: 0.0547, Train Acc: 0.8638, Test Acc: 0.8451, Test f1: 0.8740, AUC: class-0>>0.990604447228312|class-1>>0.9686231884057971\n",
      "Epoch: 089, Train Loss: 0.0494, Test Loss: 0.0594, Train Acc: 0.8580, Test Acc: 0.8269, Test f1: 0.8616, AUC: class-0>>0.9894561018895501|class-1>>0.967463768115942\n",
      "Epoch: 090, Train Loss: 0.0463, Test Loss: 0.0541, Train Acc: 0.8686, Test Acc: 0.8451, Test f1: 0.8740, AUC: class-0>>0.9902912621359223|class-1>>0.9684782608695651\n",
      "Epoch: 091, Train Loss: 0.0486, Test Loss: 0.0576, Train Acc: 0.8623, Test Acc: 0.8378, Test f1: 0.8690, AUC: class-0>>0.989247311827957|class-1>>0.9674637681159419\n",
      "Epoch: 092, Train Loss: 0.0453, Test Loss: 0.0535, Train Acc: 0.8678, Test Acc: 0.8451, Test f1: 0.8740, AUC: class-0>>0.9905000521975154|class-1>>0.968768115942029\n",
      "Epoch: 093, Train Loss: 0.0466, Test Loss: 0.0555, Train Acc: 0.8670, Test Acc: 0.8378, Test f1: 0.8690, AUC: class-0>>0.9900824720743292|class-1>>0.9683333333333333\n",
      "Epoch: 094, Train Loss: 0.0517, Test Loss: 0.0575, Train Acc: 0.8765, Test Acc: 0.8347, Test f1: 0.8634, AUC: class-0>>0.9899780770435327|class-1>>0.9694927536231884\n",
      "Epoch: 095, Train Loss: 0.0467, Test Loss: 0.0545, Train Acc: 0.8716, Test Acc: 0.8451, Test f1: 0.8740, AUC: class-0>>0.9914396074746843|class-1>>0.9687681159420289\n",
      "Epoch: 096, Train Loss: 0.0443, Test Loss: 0.0532, Train Acc: 0.8775, Test Acc: 0.8451, Test f1: 0.8740, AUC: class-0>>0.9908132372899049|class-1>>0.9686231884057971\n",
      "Epoch: 097, Train Loss: 0.0454, Test Loss: 0.0543, Train Acc: 0.8713, Test Acc: 0.8380, Test f1: 0.8682, AUC: class-0>>0.9905000521975154|class-1>>0.9689130434782608\n",
      "Epoch: 098, Train Loss: 0.0473, Test Loss: 0.0591, Train Acc: 0.8639, Test Acc: 0.8269, Test f1: 0.8616, AUC: class-0>>0.9889341267355675|class-1>>0.9683333333333334\n",
      "Epoch: 099, Train Loss: 0.0446, Test Loss: 0.0550, Train Acc: 0.8693, Test Acc: 0.8269, Test f1: 0.8609, AUC: class-0>>0.9895604969203465|class-1>>0.9686231884057972\n",
      "Epoch: 100, Train Loss: 0.0446, Test Loss: 0.0544, Train Acc: 0.8717, Test Acc: 0.8418, Test f1: 0.8691, AUC: class-0>>0.9901868671051258|class-1>>0.9690579710144928\n",
      "Epoch: 101, Train Loss: 0.0441, Test Loss: 0.0541, Train Acc: 0.8765, Test Acc: 0.8340, Test f1: 0.8674, AUC: class-0>>0.9899780770435327|class-1>>0.9692028985507246\n",
      "Epoch: 102, Train Loss: 0.0431, Test Loss: 0.0540, Train Acc: 0.8620, Test Acc: 0.8340, Test f1: 0.8674, AUC: class-0>>0.9906044472283119|class-1>>0.9692028985507246\n",
      "Epoch: 103, Train Loss: 0.0446, Test Loss: 0.0540, Train Acc: 0.8788, Test Acc: 0.8410, Test f1: 0.8739, AUC: class-0>>0.9901868671051257|class-1>>0.9690579710144928\n",
      "Epoch: 104, Train Loss: 0.0451, Test Loss: 0.0540, Train Acc: 0.8600, Test Acc: 0.8451, Test f1: 0.8740, AUC: class-0>>0.9905000521975154|class-1>>0.9696376811594203\n",
      "Epoch: 105, Train Loss: 0.0429, Test Loss: 0.0539, Train Acc: 0.8843, Test Acc: 0.8384, Test f1: 0.8655, AUC: class-0>>0.98945610188955|class-1>>0.9694927536231884\n",
      "Epoch: 106, Train Loss: 0.0435, Test Loss: 0.0552, Train Acc: 0.8612, Test Acc: 0.8340, Test f1: 0.8674, AUC: class-0>>0.9898736820127362|class-1>>0.9690579710144928\n",
      "Epoch: 107, Train Loss: 0.0428, Test Loss: 0.0543, Train Acc: 0.8624, Test Acc: 0.8457, Test f1: 0.8706, AUC: class-0>>0.9907088422591084|class-1>>0.9696376811594203\n",
      "Epoch: 108, Train Loss: 0.0432, Test Loss: 0.0536, Train Acc: 0.8781, Test Acc: 0.8378, Test f1: 0.8690, AUC: class-0>>0.9902912621359223|class-1>>0.9699275362318841\n",
      "Epoch: 109, Train Loss: 0.0420, Test Loss: 0.0538, Train Acc: 0.8692, Test Acc: 0.8386, Test f1: 0.8650, AUC: class-0>>0.9905000521975154|class-1>>0.9689130434782608\n",
      "Epoch: 110, Train Loss: 0.0436, Test Loss: 0.0553, Train Acc: 0.8761, Test Acc: 0.8340, Test f1: 0.8669, AUC: class-0>>0.9891429167971605|class-1>>0.9700724637681158\n",
      "Epoch: 111, Train Loss: 0.0464, Test Loss: 0.0590, Train Acc: 0.8870, Test Acc: 0.8269, Test f1: 0.8616, AUC: class-0>>0.991022027351498|class-1>>0.9709420289855072\n",
      "Epoch: 112, Train Loss: 0.0422, Test Loss: 0.0547, Train Acc: 0.8794, Test Acc: 0.8269, Test f1: 0.8616, AUC: class-0>>0.9899780770435327|class-1>>0.9694927536231884\n",
      "Epoch: 113, Train Loss: 0.0416, Test Loss: 0.0542, Train Acc: 0.8790, Test Acc: 0.8236, Test f1: 0.8562, AUC: class-0>>0.9900824720743293|class-1>>0.9697826086956521\n",
      "Epoch: 114, Train Loss: 0.0427, Test Loss: 0.0540, Train Acc: 0.8829, Test Acc: 0.8378, Test f1: 0.8685, AUC: class-0>>0.9901868671051257|class-1>>0.9700724637681158\n",
      "Epoch: 115, Train Loss: 0.0412, Test Loss: 0.0535, Train Acc: 0.8768, Test Acc: 0.8313, Test f1: 0.8599, AUC: class-0>>0.9900824720743292|class-1>>0.9702173913043478\n",
      "Epoch: 116, Train Loss: 0.0438, Test Loss: 0.0542, Train Acc: 0.8820, Test Acc: 0.8410, Test f1: 0.8739, AUC: class-0>>0.9902912621359223|class-1>>0.9703623188405797\n",
      "Epoch: 117, Train Loss: 0.0421, Test Loss: 0.0543, Train Acc: 0.8828, Test Acc: 0.8313, Test f1: 0.8595, AUC: class-0>>0.9895604969203465|class-1>>0.9702173913043477\n",
      "Epoch: 118, Train Loss: 0.0416, Test Loss: 0.0544, Train Acc: 0.8781, Test Acc: 0.8705, Test f1: 0.8883, AUC: class-0>>0.990604447228312|class-1>>0.9707971014492752\n",
      "Saved model!\n",
      "Best thresholds ===>>> class-0>>0.6464552879333496|class-1>>0.6334131956100464\n",
      "Epoch: 119, Train Loss: 0.0409, Test Loss: 0.0532, Train Acc: 0.8817, Test Acc: 0.8269, Test f1: 0.8616, AUC: class-0>>0.9899780770435327|class-1>>0.9707971014492753\n",
      "Epoch: 120, Train Loss: 0.0433, Test Loss: 0.0575, Train Acc: 0.8802, Test Acc: 0.8734, Test f1: 0.8935, AUC: class-0>>0.9897692869819397|class-1>>0.9697826086956521\n",
      "Saved model!\n",
      "Best thresholds ===>>> class-0>>0.6220710277557373|class-1>>0.6475628614425659\n",
      "Epoch: 121, Train Loss: 0.0418, Test Loss: 0.0550, Train Acc: 0.8781, Test Acc: 0.8620, Test f1: 0.8909, AUC: class-0>>0.9903956571667187|class-1>>0.9709420289855072\n",
      "Epoch: 122, Train Loss: 0.0425, Test Loss: 0.0549, Train Acc: 0.8840, Test Acc: 0.8269, Test f1: 0.8610, AUC: class-0>>0.9890385217663639|class-1>>0.9706521739130435\n",
      "Epoch: 123, Train Loss: 0.0402, Test Loss: 0.0536, Train Acc: 0.8837, Test Acc: 0.8313, Test f1: 0.8599, AUC: class-0>>0.989664891951143|class-1>>0.9702173913043478\n",
      "Epoch: 124, Train Loss: 0.0455, Test Loss: 0.0601, Train Acc: 0.8847, Test Acc: 0.8657, Test f1: 0.8918, AUC: class-0>>0.9896648919511432|class-1>>0.9693478260869565\n",
      "Epoch: 125, Train Loss: 0.0400, Test Loss: 0.0537, Train Acc: 0.8883, Test Acc: 0.8269, Test f1: 0.8616, AUC: class-0>>0.989664891951143|class-1>>0.9706521739130435\n",
      "Epoch: 126, Train Loss: 0.0403, Test Loss: 0.0542, Train Acc: 0.8934, Test Acc: 0.8269, Test f1: 0.8616, AUC: class-0>>0.9893517068587535|class-1>>0.9707971014492753\n",
      "Epoch: 127, Train Loss: 0.0416, Test Loss: 0.0544, Train Acc: 0.8837, Test Acc: 0.8269, Test f1: 0.8616, AUC: class-0>>0.9905000521975154|class-1>>0.9709420289855072\n",
      "Epoch: 128, Train Loss: 0.0436, Test Loss: 0.0551, Train Acc: 0.8863, Test Acc: 0.8386, Test f1: 0.8650, AUC: class-0>>0.9902912621359222|class-1>>0.9710869565217392\n",
      "Epoch: 129, Train Loss: 0.0398, Test Loss: 0.0540, Train Acc: 0.8872, Test Acc: 0.8313, Test f1: 0.8599, AUC: class-0>>0.9896648919511432|class-1>>0.9709420289855072\n",
      "Epoch: 130, Train Loss: 0.0404, Test Loss: 0.0537, Train Acc: 0.8863, Test Acc: 0.8313, Test f1: 0.8599, AUC: class-0>>0.9900824720743292|class-1>>0.9709420289855072\n",
      "Epoch: 131, Train Loss: 0.0395, Test Loss: 0.0537, Train Acc: 0.8957, Test Acc: 0.8313, Test f1: 0.8599, AUC: class-0>>0.9893517068587534|class-1>>0.971086956521739\n",
      "Epoch: 132, Train Loss: 0.0397, Test Loss: 0.0536, Train Acc: 0.8894, Test Acc: 0.8313, Test f1: 0.8599, AUC: class-0>>0.9897692869819397|class-1>>0.9713768115942027\n",
      "Epoch: 133, Train Loss: 0.0395, Test Loss: 0.0537, Train Acc: 0.8883, Test Acc: 0.8313, Test f1: 0.8599, AUC: class-0>>0.9893517068587534|class-1>>0.9707971014492753\n",
      "Epoch: 134, Train Loss: 0.0395, Test Loss: 0.0539, Train Acc: 0.8946, Test Acc: 0.8313, Test f1: 0.8599, AUC: class-0>>0.9895604969203466|class-1>>0.971086956521739\n",
      "Epoch: 135, Train Loss: 0.0394, Test Loss: 0.0537, Train Acc: 0.8957, Test Acc: 0.8313, Test f1: 0.8599, AUC: class-0>>0.9893517068587534|class-1>>0.971231884057971\n",
      "Epoch: 136, Train Loss: 0.0393, Test Loss: 0.0538, Train Acc: 0.8872, Test Acc: 0.8313, Test f1: 0.8599, AUC: class-0>>0.9891429167971604|class-1>>0.9709420289855072\n",
      "Epoch: 137, Train Loss: 0.0394, Test Loss: 0.0536, Train Acc: 0.8957, Test Acc: 0.8313, Test f1: 0.8599, AUC: class-0>>0.9893517068587534|class-1>>0.971086956521739\n",
      "Epoch: 138, Train Loss: 0.0395, Test Loss: 0.0537, Train Acc: 0.8883, Test Acc: 0.8313, Test f1: 0.8595, AUC: class-0>>0.9893517068587535|class-1>>0.9716666666666666\n",
      "Epoch: 139, Train Loss: 0.0395, Test Loss: 0.0537, Train Acc: 0.8957, Test Acc: 0.8313, Test f1: 0.8599, AUC: class-0>>0.989247311827957|class-1>>0.9713768115942029\n",
      "Epoch: 140, Train Loss: 0.0393, Test Loss: 0.0540, Train Acc: 0.8872, Test Acc: 0.8588, Test f1: 0.8860, AUC: class-0>>0.98945610188955|class-1>>0.9713768115942027\n",
      "Epoch: 141, Train Loss: 0.0392, Test Loss: 0.0542, Train Acc: 0.8872, Test Acc: 0.8312, Test f1: 0.8605, AUC: class-0>>0.989247311827957|class-1>>0.9706521739130434\n",
      "Epoch: 142, Train Loss: 0.0391, Test Loss: 0.0540, Train Acc: 0.8872, Test Acc: 0.8312, Test f1: 0.8605, AUC: class-0>>0.989247311827957|class-1>>0.971086956521739\n",
      "Epoch: 143, Train Loss: 0.0391, Test Loss: 0.0540, Train Acc: 0.8872, Test Acc: 0.8235, Test f1: 0.8573, AUC: class-0>>0.989247311827957|class-1>>0.9709420289855072\n",
      "Epoch: 144, Train Loss: 0.0390, Test Loss: 0.0538, Train Acc: 0.8957, Test Acc: 0.8313, Test f1: 0.8599, AUC: class-0>>0.989247311827957|class-1>>0.9714492753623188\n",
      "Epoch: 145, Train Loss: 0.0393, Test Loss: 0.0538, Train Acc: 0.8883, Test Acc: 0.8352, Test f1: 0.8608, AUC: class-0>>0.98945610188955|class-1>>0.9712318840579709\n",
      "Epoch: 146, Train Loss: 0.0390, Test Loss: 0.0538, Train Acc: 0.8946, Test Acc: 0.8313, Test f1: 0.8599, AUC: class-0>>0.989247311827957|class-1>>0.9713768115942029\n",
      "Epoch: 147, Train Loss: 0.0390, Test Loss: 0.0539, Train Acc: 0.8946, Test Acc: 0.8665, Test f1: 0.8880, AUC: class-0>>0.989247311827957|class-1>>0.9712318840579709\n",
      "Epoch: 148, Train Loss: 0.0391, Test Loss: 0.0541, Train Acc: 0.8946, Test Acc: 0.8665, Test f1: 0.8880, AUC: class-0>>0.989247311827957|class-1>>0.971231884057971\n",
      "Epoch: 149, Train Loss: 0.0389, Test Loss: 0.0539, Train Acc: 0.8946, Test Acc: 0.8312, Test f1: 0.8605, AUC: class-0>>0.989247311827957|class-1>>0.9712318840579709\n",
      "Running cross-validation fold: 03\n",
      "Epoch: 001, Train Loss: 0.0957, Test Loss: 0.1243, Train Acc: 0.7228, Test Acc: 0.7289, Test f1: 0.7409, AUC: class-0>>0.9502314814814814|class-1>>0.8941882050549764\n",
      "Saved model!\n",
      "Best thresholds ===>>> class-0>>0.6456401348114014|class-1>>0.5775299668312073\n",
      "Epoch: 002, Train Loss: 0.1157, Test Loss: 0.1312, Train Acc: 0.7715, Test Acc: 0.7064, Test f1: 0.6904, AUC: class-0>>0.9462331649831651|class-1>>0.9053262887333999\n",
      "Epoch: 003, Train Loss: 0.0731, Test Loss: 0.0991, Train Acc: 0.7782, Test Acc: 0.7042, Test f1: 0.6846, AUC: class-0>>0.9509680134680134|class-1>>0.9148936170212766\n",
      "Epoch: 004, Train Loss: 0.0808, Test Loss: 0.1088, Train Acc: 0.7697, Test Acc: 0.7080, Test f1: 0.6962, AUC: class-0>>0.9580176767676769|class-1>>0.9117521062401828\n",
      "Epoch: 005, Train Loss: 0.1078, Test Loss: 0.1288, Train Acc: 0.7934, Test Acc: 0.7261, Test f1: 0.7142, AUC: class-0>>0.953493265993266|class-1>>0.9088961873482794\n",
      "Epoch: 006, Train Loss: 0.0918, Test Loss: 0.1155, Train Acc: 0.8067, Test Acc: 0.7277, Test f1: 0.7199, AUC: class-0>>0.94760101010101|class-1>>0.9151792089104669\n",
      "Epoch: 007, Train Loss: 0.0990, Test Loss: 0.1240, Train Acc: 0.7672, Test Acc: 0.7227, Test f1: 0.7142, AUC: class-0>>0.9568602693602694|class-1>>0.902898757675282\n",
      "Epoch: 008, Train Loss: 0.0944, Test Loss: 0.1233, Train Acc: 0.7748, Test Acc: 0.6959, Test f1: 0.6725, AUC: class-0>>0.9591750841750842|class-1>>0.9024703698414965\n",
      "Epoch: 009, Train Loss: 0.0786, Test Loss: 0.1038, Train Acc: 0.7936, Test Acc: 0.7194, Test f1: 0.7084, AUC: class-0>>0.9591750841750841|class-1>>0.9080394116807083\n",
      "Epoch: 010, Train Loss: 0.0775, Test Loss: 0.1063, Train Acc: 0.7793, Test Acc: 0.7004, Test f1: 0.6768, AUC: class-0>>0.9632786195286195|class-1>>0.9078966157361131\n",
      "Epoch: 011, Train Loss: 0.0774, Test Loss: 0.0937, Train Acc: 0.7793, Test Acc: 0.7856, Test f1: 0.7988, AUC: class-0>>0.9624368686868687|class-1>>0.893045837498215\n",
      "Saved model!\n",
      "Best thresholds ===>>> class-0>>0.6546507477760315|class-1>>0.6685082316398621\n",
      "Epoch: 012, Train Loss: 0.0684, Test Loss: 0.0996, Train Acc: 0.8005, Test Acc: 0.7046, Test f1: 0.6749, AUC: class-0>>0.9653829966329968|class-1>>0.9137512494645151\n",
      "Epoch: 013, Train Loss: 0.0720, Test Loss: 0.1024, Train Acc: 0.7660, Test Acc: 0.7063, Test f1: 0.6798, AUC: class-0>>0.968013468013468|class-1>>0.9164643724118234\n",
      "Epoch: 014, Train Loss: 0.0758, Test Loss: 0.0995, Train Acc: 0.8355, Test Acc: 0.7424, Test f1: 0.7310, AUC: class-0>>0.9639099326599325|class-1>>0.919320291303727\n",
      "Epoch: 015, Train Loss: 0.0956, Test Loss: 0.1325, Train Acc: 0.8189, Test Acc: 0.7189, Test f1: 0.6989, AUC: class-0>>0.9675925925925927|class-1>>0.9178923318577752\n",
      "Epoch: 016, Train Loss: 0.0771, Test Loss: 0.1110, Train Acc: 0.8076, Test Acc: 0.7840, Test f1: 0.7993, AUC: class-0>>0.9701178451178452|class-1>>0.9204626588604884\n",
      "Saved model!\n",
      "Best thresholds ===>>> class-0>>0.7082718014717102|class-1>>0.5884337425231934\n",
      "Epoch: 017, Train Loss: 0.0970, Test Loss: 0.1308, Train Acc: 0.8110, Test Acc: 0.7243, Test f1: 0.7073, AUC: class-0>>0.9691708754208754|class-1>>0.9188919034699414\n",
      "Epoch: 018, Train Loss: 0.0673, Test Loss: 0.0866, Train Acc: 0.8286, Test Acc: 0.8020, Test f1: 0.8161, AUC: class-0>>0.970223063973064|class-1>>0.9231757818077967\n",
      "Saved model!\n",
      "Best thresholds ===>>> class-0>>0.6631401777267456|class-1>>0.6357750296592712\n",
      "Epoch: 019, Train Loss: 0.0621, Test Loss: 0.0871, Train Acc: 0.8417, Test Acc: 0.7353, Test f1: 0.7224, AUC: class-0>>0.9679082491582491|class-1>>0.9224618020848208\n",
      "Epoch: 020, Train Loss: 0.0715, Test Loss: 0.0938, Train Acc: 0.7851, Test Acc: 0.7873, Test f1: 0.7991, AUC: class-0>>0.9715909090909092|class-1>>0.9271740682564615\n",
      "Epoch: 021, Train Loss: 0.0706, Test Loss: 0.0931, Train Acc: 0.8010, Test Acc: 0.8042, Test f1: 0.8203, AUC: class-0>>0.9743265993265994|class-1>>0.9248893331429386\n",
      "Saved model!\n",
      "Best thresholds ===>>> class-0>>0.6901946067810059|class-1>>0.6354442834854126\n",
      "Epoch: 022, Train Loss: 0.0642, Test Loss: 0.0916, Train Acc: 0.8175, Test Acc: 0.7286, Test f1: 0.7058, AUC: class-0>>0.9692760942760943|class-1>>0.9244609453091532\n",
      "Epoch: 023, Train Loss: 0.0759, Test Loss: 0.1038, Train Acc: 0.7907, Test Acc: 0.7927, Test f1: 0.8045, AUC: class-0>>0.9723274410774411|class-1>>0.9250321290875338\n",
      "Epoch: 024, Train Loss: 0.0593, Test Loss: 0.0808, Train Acc: 0.7957, Test Acc: 0.7944, Test f1: 0.8062, AUC: class-0>>0.9705387205387206|class-1>>0.9286020277024132\n",
      "Epoch: 025, Train Loss: 0.0666, Test Loss: 0.0927, Train Acc: 0.8363, Test Acc: 0.7407, Test f1: 0.7302, AUC: class-0>>0.9727483164983165|class-1>>0.9266028844780809\n",
      "Epoch: 026, Train Loss: 0.0700, Test Loss: 0.0991, Train Acc: 0.8287, Test Acc: 0.7462, Test f1: 0.7365, AUC: class-0>>0.9724326599326599|class-1>>0.9263172925888905\n",
      "Epoch: 027, Train Loss: 0.0785, Test Loss: 0.0950, Train Acc: 0.8365, Test Acc: 0.8020, Test f1: 0.8145, AUC: class-0>>0.9743265993265993|class-1>>0.9298871912037698\n",
      "Epoch: 028, Train Loss: 0.0616, Test Loss: 0.0840, Train Acc: 0.8142, Test Acc: 0.8037, Test f1: 0.8152, AUC: class-0>>0.9743265993265993|class-1>>0.9318863344281022\n",
      "Saved model!\n",
      "Best thresholds ===>>> class-0>>0.6848205327987671|class-1>>0.643238365650177\n",
      "Epoch: 029, Train Loss: 0.0662, Test Loss: 0.0893, Train Acc: 0.8503, Test Acc: 0.7610, Test f1: 0.7613, AUC: class-0>>0.9666456228956228|class-1>>0.930029987148365\n",
      "Epoch: 030, Train Loss: 0.0717, Test Loss: 0.0865, Train Acc: 0.8049, Test Acc: 0.8091, Test f1: 0.8214, AUC: class-0>>0.9681186868686869|class-1>>0.9294588033699843\n",
      "Saved model!\n",
      "Best thresholds ===>>> class-0>>0.6380624175071716|class-1>>0.6785444021224976\n",
      "Epoch: 031, Train Loss: 0.0742, Test Loss: 0.0956, Train Acc: 0.8504, Test Acc: 0.8146, Test f1: 0.8276, AUC: class-0>>0.9601220538720538|class-1>>0.9331714979294587\n",
      "Saved model!\n",
      "Best thresholds ===>>> class-0>>0.6060839295387268|class-1>>0.5888493061065674\n",
      "Epoch: 032, Train Loss: 0.0595, Test Loss: 0.0892, Train Acc: 0.8510, Test Acc: 0.8168, Test f1: 0.8309, AUC: class-0>>0.9730639730639731|class-1>>0.92988719120377\n",
      "Saved model!\n",
      "Best thresholds ===>>> class-0>>0.6622743010520935|class-1>>0.5798158049583435\n",
      "Epoch: 033, Train Loss: 0.0627, Test Loss: 0.0856, Train Acc: 0.8409, Test Acc: 0.8113, Test f1: 0.8261, AUC: class-0>>0.9750631313131314|class-1>>0.9321719263172925\n",
      "Epoch: 034, Train Loss: 0.1288, Test Loss: 0.1557, Train Acc: 0.8356, Test Acc: 0.7898, Test f1: 0.8027, AUC: class-0>>0.975378787878788|class-1>>0.9284592317578181\n",
      "Epoch: 035, Train Loss: 0.0665, Test Loss: 0.0953, Train Acc: 0.8452, Test Acc: 0.8020, Test f1: 0.8161, AUC: class-0>>0.9721170033670034|class-1>>0.9294588033699843\n",
      "Epoch: 036, Train Loss: 0.0667, Test Loss: 0.0915, Train Acc: 0.8183, Test Acc: 0.7965, Test f1: 0.8086, AUC: class-0>>0.9749579124579124|class-1>>0.9307439668713409\n",
      "Epoch: 037, Train Loss: 0.0552, Test Loss: 0.0793, Train Acc: 0.8555, Test Acc: 0.7521, Test f1: 0.7452, AUC: class-0>>0.9731691919191919|class-1>>0.9290304155361988\n",
      "Epoch: 038, Train Loss: 0.0643, Test Loss: 0.0786, Train Acc: 0.8441, Test Acc: 0.8129, Test f1: 0.8256, AUC: class-0>>0.9757996632996633|class-1>>0.9288876195916036\n",
      "Epoch: 039, Train Loss: 0.0684, Test Loss: 0.0886, Train Acc: 0.8211, Test Acc: 0.7593, Test f1: 0.7541, AUC: class-0>>0.9719065656565657|class-1>>0.9330287019848635\n",
      "Epoch: 040, Train Loss: 0.0570, Test Loss: 0.0844, Train Acc: 0.8254, Test Acc: 0.7927, Test f1: 0.8050, AUC: class-0>>0.9766414141414141|class-1>>0.9340282735970299\n",
      "Epoch: 041, Train Loss: 0.0550, Test Loss: 0.0767, Train Acc: 0.8330, Test Acc: 0.8091, Test f1: 0.8210, AUC: class-0>>0.9748526936026936|class-1>>0.9264600885334856\n",
      "Epoch: 042, Train Loss: 0.0661, Test Loss: 0.1012, Train Acc: 0.8452, Test Acc: 0.7560, Test f1: 0.7538, AUC: class-0>>0.9759048821548821|class-1>>0.9294588033699843\n",
      "Epoch: 043, Train Loss: 0.0526, Test Loss: 0.0766, Train Acc: 0.8411, Test Acc: 0.8041, Test f1: 0.8175, AUC: class-0>>0.9748526936026936|class-1>>0.9327431100956732\n",
      "Epoch: 044, Train Loss: 0.0888, Test Loss: 0.1267, Train Acc: 0.8259, Test Acc: 0.7554, Test f1: 0.7450, AUC: class-0>>0.9676978114478114|class-1>>0.9314579465943167\n",
      "Epoch: 045, Train Loss: 0.0522, Test Loss: 0.0790, Train Acc: 0.8545, Test Acc: 0.7588, Test f1: 0.7563, AUC: class-0>>0.9753787878787878|class-1>>0.9324575182064829\n",
      "Epoch: 046, Train Loss: 0.0577, Test Loss: 0.0878, Train Acc: 0.8653, Test Acc: 0.7665, Test f1: 0.7673, AUC: class-0>>0.9761153198653199|class-1>>0.9343138654862202\n",
      "Epoch: 047, Train Loss: 0.0587, Test Loss: 0.0841, Train Acc: 0.8419, Test Acc: 0.7681, Test f1: 0.7660, AUC: class-0>>0.9740109427609428|class-1>>0.9311723547051264\n",
      "Epoch: 048, Train Loss: 0.0573, Test Loss: 0.0747, Train Acc: 0.8492, Test Acc: 0.7991, Test f1: 0.8131, AUC: class-0>>0.9790614478114479|class-1>>0.9220334142510352\n",
      "Epoch: 049, Train Loss: 0.0583, Test Loss: 0.0778, Train Acc: 0.8480, Test Acc: 0.8062, Test f1: 0.8205, AUC: class-0>>0.9787457912457912|class-1>>0.9364558046551478\n",
      "Epoch: 050, Train Loss: 0.0569, Test Loss: 0.0771, Train Acc: 0.8436, Test Acc: 0.8075, Test f1: 0.8203, AUC: class-0>>0.9769570707070707|class-1>>0.9240325574753677\n",
      "Epoch: 051, Train Loss: 0.0525, Test Loss: 0.0776, Train Acc: 0.8581, Test Acc: 0.7778, Test f1: 0.7791, AUC: class-0>>0.9731691919191918|class-1>>0.9367413965443381\n",
      "Epoch: 052, Train Loss: 0.0517, Test Loss: 0.0760, Train Acc: 0.8657, Test Acc: 0.7509, Test f1: 0.7438, AUC: class-0>>0.9788510101010102|class-1>>0.9351706411537912\n",
      "Epoch: 053, Train Loss: 0.0524, Test Loss: 0.0799, Train Acc: 0.8696, Test Acc: 0.7648, Test f1: 0.7654, AUC: class-0>>0.9764309764309764|class-1>>0.9353134370983863\n",
      "Epoch: 054, Train Loss: 0.0523, Test Loss: 0.0755, Train Acc: 0.8601, Test Acc: 0.7626, Test f1: 0.7600, AUC: class-0>>0.9791666666666666|class-1>>0.9348850492646009\n",
      "Epoch: 055, Train Loss: 0.0616, Test Loss: 0.0878, Train Acc: 0.8229, Test Acc: 0.7677, Test f1: 0.7666, AUC: class-0>>0.9780092592592593|class-1>>0.935741824932172\n",
      "Epoch: 056, Train Loss: 0.0531, Test Loss: 0.0807, Train Acc: 0.8602, Test Acc: 0.7719, Test f1: 0.7698, AUC: class-0>>0.9780092592592593|class-1>>0.9360274168213623\n",
      "Epoch: 057, Train Loss: 0.0516, Test Loss: 0.0813, Train Acc: 0.8474, Test Acc: 0.7542, Test f1: 0.7451, AUC: class-0>>0.9785353535353536|class-1>>0.9341710695416251\n",
      "Epoch: 058, Train Loss: 0.0544, Test Loss: 0.0729, Train Acc: 0.8486, Test Acc: 0.8037, Test f1: 0.8157, AUC: class-0>>0.9801136363636364|class-1>>0.935027845209196\n",
      "Epoch: 059, Train Loss: 0.0516, Test Loss: 0.0780, Train Acc: 0.8646, Test Acc: 0.7572, Test f1: 0.7535, AUC: class-0>>0.9792718855218856|class-1>>0.9364558046551479\n",
      "Epoch: 060, Train Loss: 0.0522, Test Loss: 0.0752, Train Acc: 0.8618, Test Acc: 0.7665, Test f1: 0.7673, AUC: class-0>>0.9780092592592592|class-1>>0.9353134370983863\n",
      "Epoch: 061, Train Loss: 0.0558, Test Loss: 0.0739, Train Acc: 0.8537, Test Acc: 0.8091, Test f1: 0.8210, AUC: class-0>>0.97885101010101|class-1>>0.9326003141510781\n",
      "Epoch: 062, Train Loss: 0.0496, Test Loss: 0.0748, Train Acc: 0.8493, Test Acc: 0.8037, Test f1: 0.8157, AUC: class-0>>0.9782196969696969|class-1>>0.9347422533200057\n",
      "Epoch: 063, Train Loss: 0.0580, Test Loss: 0.0847, Train Acc: 0.8572, Test Acc: 0.8020, Test f1: 0.8145, AUC: class-0>>0.9741161616161617|class-1>>0.9373125803227188\n",
      "Epoch: 064, Train Loss: 0.0499, Test Loss: 0.0728, Train Acc: 0.8556, Test Acc: 0.7664, Test f1: 0.7637, AUC: class-0>>0.9804292929292929|class-1>>0.935741824932172\n",
      "Epoch: 065, Train Loss: 0.0511, Test Loss: 0.0815, Train Acc: 0.8444, Test Acc: 0.7626, Test f1: 0.7588, AUC: class-0>>0.9761153198653199|class-1>>0.9388833357132659\n",
      "Epoch: 066, Train Loss: 0.0497, Test Loss: 0.0739, Train Acc: 0.8562, Test Acc: 0.8037, Test f1: 0.8157, AUC: class-0>>0.9808501683501684|class-1>>0.9364558046551478\n",
      "Epoch: 067, Train Loss: 0.0490, Test Loss: 0.0699, Train Acc: 0.8579, Test Acc: 0.8008, Test f1: 0.8149, AUC: class-0>>0.9810606060606061|class-1>>0.9355990289875769\n",
      "Epoch: 068, Train Loss: 0.0534, Test Loss: 0.0710, Train Acc: 0.8707, Test Acc: 0.7974, Test f1: 0.8143, AUC: class-0>>0.9812710437710437|class-1>>0.9251749250321291\n",
      "Epoch: 069, Train Loss: 0.0596, Test Loss: 0.0875, Train Acc: 0.8777, Test Acc: 0.7580, Test f1: 0.7492, AUC: class-0>>0.9790614478114478|class-1>>0.9360274168213623\n",
      "Epoch: 070, Train Loss: 0.0468, Test Loss: 0.0705, Train Acc: 0.8701, Test Acc: 0.8062, Test f1: 0.8205, AUC: class-0>>0.9801136363636364|class-1>>0.936598600599743\n",
      "Epoch: 071, Train Loss: 0.0580, Test Loss: 0.0793, Train Acc: 0.8744, Test Acc: 0.8129, Test f1: 0.8256, AUC: class-0>>0.9812710437710437|class-1>>0.933314293874054\n",
      "Saved model!\n",
      "Best thresholds ===>>> class-0>>0.6835466623306274|class-1>>0.6597968339920044\n",
      "Epoch: 072, Train Loss: 0.0465, Test Loss: 0.0746, Train Acc: 0.8704, Test Acc: 0.7790, Test f1: 0.7776, AUC: class-0>>0.9801136363636365|class-1>>0.9384549478794803\n",
      "Epoch: 073, Train Loss: 0.0458, Test Loss: 0.0738, Train Acc: 0.8816, Test Acc: 0.7795, Test f1: 0.7771, AUC: class-0>>0.9802188552188552|class-1>>0.9403112951592175\n",
      "Epoch: 074, Train Loss: 0.0467, Test Loss: 0.0762, Train Acc: 0.8768, Test Acc: 0.7790, Test f1: 0.7776, AUC: class-0>>0.9785353535353536|class-1>>0.9394545194916465\n",
      "Epoch: 075, Train Loss: 0.0454, Test Loss: 0.0737, Train Acc: 0.8710, Test Acc: 0.7723, Test f1: 0.7681, AUC: class-0>>0.9797979797979798|class-1>>0.9391689276024561\n",
      "Epoch: 076, Train Loss: 0.0527, Test Loss: 0.0843, Train Acc: 0.8711, Test Acc: 0.8031, Test f1: 0.8103, AUC: class-0>>0.9812710437710438|class-1>>0.9384549478794803\n",
      "Epoch: 077, Train Loss: 0.0451, Test Loss: 0.0705, Train Acc: 0.8630, Test Acc: 0.7859, Test f1: 0.7940, AUC: class-0>>0.981797138047138|class-1>>0.9345994573754106\n",
      "Epoch: 078, Train Loss: 0.0488, Test Loss: 0.0821, Train Acc: 0.8643, Test Acc: 0.7723, Test f1: 0.7681, AUC: class-0>>0.9801136363636365|class-1>>0.9384549478794801\n",
      "Epoch: 079, Train Loss: 0.0438, Test Loss: 0.0727, Train Acc: 0.8853, Test Acc: 0.7854, Test f1: 0.7832, AUC: class-0>>0.9819023569023569|class-1>>0.9393117235470514\n",
      "Epoch: 080, Train Loss: 0.0450, Test Loss: 0.0711, Train Acc: 0.8842, Test Acc: 0.7985, Test f1: 0.8073, AUC: class-0>>0.9808501683501684|class-1>>0.9317435384835071\n",
      "Epoch: 081, Train Loss: 0.0579, Test Loss: 0.0836, Train Acc: 0.8709, Test Acc: 0.8079, Test f1: 0.8218, AUC: class-0>>0.9834806397306397|class-1>>0.9360274168213623\n",
      "Epoch: 082, Train Loss: 0.0474, Test Loss: 0.0724, Train Acc: 0.8563, Test Acc: 0.7997, Test f1: 0.8059, AUC: class-0>>0.9816919191919191|class-1>>0.9348850492646008\n",
      "Epoch: 083, Train Loss: 0.0431, Test Loss: 0.0741, Train Acc: 0.8957, Test Acc: 0.7866, Test f1: 0.7861, AUC: class-0>>0.9815867003367004|class-1>>0.940739682993003\n",
      "Epoch: 084, Train Loss: 0.0508, Test Loss: 0.0742, Train Acc: 0.8651, Test Acc: 0.7811, Test f1: 0.7801, AUC: class-0>>0.9804292929292928|class-1>>0.9357418249321718\n",
      "Epoch: 085, Train Loss: 0.0498, Test Loss: 0.0725, Train Acc: 0.8892, Test Acc: 0.8052, Test f1: 0.8138, AUC: class-0>>0.9820075757575758|class-1>>0.936598600599743\n",
      "Epoch: 086, Train Loss: 0.0449, Test Loss: 0.0695, Train Acc: 0.8715, Test Acc: 0.8052, Test f1: 0.8138, AUC: class-0>>0.9816919191919192|class-1>>0.932600314151078\n",
      "Epoch: 087, Train Loss: 0.0440, Test Loss: 0.0717, Train Acc: 0.8604, Test Acc: 0.7998, Test f1: 0.8076, AUC: class-0>>0.9792718855218855|class-1>>0.9351706411537913\n",
      "Epoch: 088, Train Loss: 0.0418, Test Loss: 0.0711, Train Acc: 0.8917, Test Acc: 0.7952, Test f1: 0.8066, AUC: class-0>>0.9795875420875421|class-1>>0.9400257032700272\n",
      "Epoch: 089, Train Loss: 0.0446, Test Loss: 0.0738, Train Acc: 0.8705, Test Acc: 0.7998, Test f1: 0.8076, AUC: class-0>>0.9812710437710437|class-1>>0.9380265600456946\n",
      "Epoch: 090, Train Loss: 0.0456, Test Loss: 0.0731, Train Acc: 0.8791, Test Acc: 0.7706, Test f1: 0.7663, AUC: class-0>>0.9836910774410774|class-1>>0.9371697843781237\n",
      "Epoch: 091, Train Loss: 0.0477, Test Loss: 0.0780, Train Acc: 0.8982, Test Acc: 0.8090, Test f1: 0.8178, AUC: class-0>>0.9813762626262625|class-1>>0.9418820505497645\n",
      "Epoch: 092, Train Loss: 0.0451, Test Loss: 0.0787, Train Acc: 0.8987, Test Acc: 0.7816, Test f1: 0.7787, AUC: class-0>>0.9797979797979798|class-1>>0.9444523775524775\n",
      "Epoch: 093, Train Loss: 0.0437, Test Loss: 0.0776, Train Acc: 0.8811, Test Acc: 0.7799, Test f1: 0.7761, AUC: class-0>>0.9819023569023569|class-1>>0.9421676424389548\n",
      "Epoch: 094, Train Loss: 0.0459, Test Loss: 0.0708, Train Acc: 0.8876, Test Acc: 0.7947, Test f1: 0.8006, AUC: class-0>>0.983270202020202|class-1>>0.9334570898186492\n",
      "Epoch: 095, Train Loss: 0.0536, Test Loss: 0.0842, Train Acc: 0.8619, Test Acc: 0.7888, Test f1: 0.7915, AUC: class-0>>0.9805345117845118|class-1>>0.9418820505497644\n",
      "Epoch: 096, Train Loss: 0.0440, Test Loss: 0.0739, Train Acc: 0.8841, Test Acc: 0.8069, Test f1: 0.8150, AUC: class-0>>0.9828493265993267|class-1>>0.9415964586605741\n",
      "Epoch: 097, Train Loss: 0.0517, Test Loss: 0.0762, Train Acc: 0.8918, Test Acc: 0.8233, Test f1: 0.8310, AUC: class-0>>0.9830597643097644|class-1>>0.9361702127659575\n",
      "Saved model!\n",
      "Best thresholds ===>>> class-0>>0.701808512210846|class-1>>0.6630204916000366\n",
      "Epoch: 098, Train Loss: 0.0452, Test Loss: 0.0747, Train Acc: 0.8726, Test Acc: 0.7821, Test f1: 0.7820, AUC: class-0>>0.9819023569023569|class-1>>0.9375981722119092\n",
      "Epoch: 099, Train Loss: 0.0398, Test Loss: 0.0694, Train Acc: 0.8790, Test Acc: 0.8107, Test f1: 0.8191, AUC: class-0>>0.9834806397306397|class-1>>0.9403112951592174\n",
      "Epoch: 100, Train Loss: 0.0392, Test Loss: 0.0692, Train Acc: 0.8751, Test Acc: 0.7963, Test f1: 0.7994, AUC: class-0>>0.9837962962962963|class-1>>0.9418820505497643\n",
      "Epoch: 101, Train Loss: 0.0403, Test Loss: 0.0682, Train Acc: 0.9180, Test Acc: 0.8062, Test f1: 0.8186, AUC: class-0>>0.9815867003367004|class-1>>0.9343138654862202\n",
      "Epoch: 102, Train Loss: 0.0384, Test Loss: 0.0700, Train Acc: 0.8863, Test Acc: 0.7976, Test f1: 0.8009, AUC: class-0>>0.9837962962962963|class-1>>0.9423104383835499\n",
      "Epoch: 103, Train Loss: 0.0401, Test Loss: 0.0680, Train Acc: 0.8991, Test Acc: 0.8179, Test f1: 0.8257, AUC: class-0>>0.9841119528619529|class-1>>0.9368841924889334\n",
      "Epoch: 104, Train Loss: 0.0394, Test Loss: 0.0679, Train Acc: 0.8986, Test Acc: 0.8045, Test f1: 0.8171, AUC: class-0>>0.9839015151515151|class-1>>0.9394545194916464\n",
      "Epoch: 105, Train Loss: 0.0398, Test Loss: 0.0728, Train Acc: 0.8732, Test Acc: 0.8047, Test f1: 0.8088, AUC: class-0>>0.9822180134680135|class-1>>0.9441667856632872\n",
      "Epoch: 106, Train Loss: 0.0384, Test Loss: 0.0702, Train Acc: 0.9150, Test Acc: 0.8007, Test f1: 0.8124, AUC: class-0>>0.9809553872053872|class-1>>0.9413108667713838\n",
      "Epoch: 107, Train Loss: 0.0376, Test Loss: 0.0684, Train Acc: 0.8961, Test Acc: 0.8179, Test f1: 0.8257, AUC: class-0>>0.9846380471380471|class-1>>0.9441667856632872\n",
      "Epoch: 108, Train Loss: 0.0378, Test Loss: 0.0713, Train Acc: 0.8895, Test Acc: 0.7963, Test f1: 0.7994, AUC: class-0>>0.9833754208754208|class-1>>0.9438811937740968\n",
      "Epoch: 109, Train Loss: 0.0375, Test Loss: 0.0687, Train Acc: 0.8883, Test Acc: 0.8074, Test f1: 0.8159, AUC: class-0>>0.9831649831649832|class-1>>0.9414536627159789\n",
      "Epoch: 110, Train Loss: 0.0466, Test Loss: 0.0841, Train Acc: 0.8883, Test Acc: 0.7888, Test f1: 0.7905, AUC: class-0>>0.9819023569023569|class-1>>0.9441667856632872\n",
      "Epoch: 111, Train Loss: 0.0368, Test Loss: 0.0703, Train Acc: 0.8833, Test Acc: 0.8179, Test f1: 0.8257, AUC: class-0>>0.9820075757575758|class-1>>0.9441667856632873\n",
      "Epoch: 112, Train Loss: 0.0369, Test Loss: 0.0679, Train Acc: 0.8914, Test Acc: 0.8179, Test f1: 0.8257, AUC: class-0>>0.9822180134680134|class-1>>0.9418820505497644\n",
      "Epoch: 113, Train Loss: 0.0376, Test Loss: 0.0728, Train Acc: 0.8858, Test Acc: 0.8047, Test f1: 0.8081, AUC: class-0>>0.98253367003367|class-1>>0.9458803369984292\n",
      "Epoch: 114, Train Loss: 0.0423, Test Loss: 0.0773, Train Acc: 0.8939, Test Acc: 0.7997, Test f1: 0.8026, AUC: class-0>>0.9824284511784511|class-1>>0.9458803369984292\n",
      "Epoch: 115, Train Loss: 0.0378, Test Loss: 0.0730, Train Acc: 0.8872, Test Acc: 0.8064, Test f1: 0.8095, AUC: class-0>>0.9827441077441078|class-1>>0.944880765386263\n",
      "Epoch: 116, Train Loss: 0.0466, Test Loss: 0.0823, Train Acc: 0.9096, Test Acc: 0.8030, Test f1: 0.8065, AUC: class-0>>0.9814814814814815|class-1>>0.9474510923889762\n",
      "Epoch: 117, Train Loss: 0.0362, Test Loss: 0.0681, Train Acc: 0.8920, Test Acc: 0.8233, Test f1: 0.8315, AUC: class-0>>0.9827441077441077|class-1>>0.9420248464943596\n",
      "Saved model!\n",
      "Best thresholds ===>>> class-0>>0.6497149467468262|class-1>>0.633162796497345\n",
      "Epoch: 118, Train Loss: 0.0357, Test Loss: 0.0690, Train Acc: 0.8971, Test Acc: 0.8102, Test f1: 0.8143, AUC: class-0>>0.9836910774410774|class-1>>0.9454519491646437\n",
      "Epoch: 119, Train Loss: 0.0384, Test Loss: 0.0746, Train Acc: 0.8949, Test Acc: 0.7897, Test f1: 0.7947, AUC: class-0>>0.98253367003367|class-1>>0.9471655004997858\n",
      "Epoch: 120, Train Loss: 0.0371, Test Loss: 0.0704, Train Acc: 0.8996, Test Acc: 0.8028, Test f1: 0.8156, AUC: class-0>>0.9830597643097643|class-1>>0.9447379694416679\n",
      "Epoch: 121, Train Loss: 0.0361, Test Loss: 0.0721, Train Acc: 0.8870, Test Acc: 0.8102, Test f1: 0.8143, AUC: class-0>>0.9834806397306397|class-1>>0.9467371126660002\n",
      "Epoch: 122, Train Loss: 0.0352, Test Loss: 0.0685, Train Acc: 0.9035, Test Acc: 0.8102, Test f1: 0.8143, AUC: class-0>>0.9833754208754208|class-1>>0.9454519491646438\n",
      "Epoch: 123, Train Loss: 0.0354, Test Loss: 0.0684, Train Acc: 0.8998, Test Acc: 0.8288, Test f1: 0.8368, AUC: class-0>>0.9830597643097643|class-1>>0.9445951734970728\n",
      "Saved model!\n",
      "Best thresholds ===>>> class-0>>0.6703746318817139|class-1>>0.6169973015785217\n",
      "Epoch: 124, Train Loss: 0.0351, Test Loss: 0.0683, Train Acc: 0.9021, Test Acc: 0.8233, Test f1: 0.8315, AUC: class-0>>0.9836910774410774|class-1>>0.9461659288876196\n",
      "Epoch: 125, Train Loss: 0.0349, Test Loss: 0.0680, Train Acc: 0.8996, Test Acc: 0.8233, Test f1: 0.8315, AUC: class-0>>0.9828493265993267|class-1>>0.9450235613308583\n",
      "Epoch: 126, Train Loss: 0.0364, Test Loss: 0.0716, Train Acc: 0.9035, Test Acc: 0.8102, Test f1: 0.8143, AUC: class-0>>0.9839015151515151|class-1>>0.9477366842781665\n",
      "Epoch: 127, Train Loss: 0.0347, Test Loss: 0.0690, Train Acc: 0.9010, Test Acc: 0.8102, Test f1: 0.8143, AUC: class-0>>0.9830597643097643|class-1>>0.9468799086105955\n",
      "Epoch: 128, Train Loss: 0.0349, Test Loss: 0.0674, Train Acc: 0.8996, Test Acc: 0.8288, Test f1: 0.8368, AUC: class-0>>0.9829545454545454|class-1>>0.9457375410538341\n",
      "Saved model!\n",
      "Best thresholds ===>>> class-0>>0.6573842167854309|class-1>>0.6223334670066833\n",
      "Epoch: 129, Train Loss: 0.0351, Test Loss: 0.0709, Train Acc: 0.9010, Test Acc: 0.8102, Test f1: 0.8143, AUC: class-0>>0.9837962962962963|class-1>>0.9485934599457375\n",
      "Epoch: 130, Train Loss: 0.0345, Test Loss: 0.0694, Train Acc: 0.9035, Test Acc: 0.8102, Test f1: 0.8143, AUC: class-0>>0.9830597643097643|class-1>>0.9474510923889762\n",
      "Epoch: 131, Train Loss: 0.0341, Test Loss: 0.0684, Train Acc: 0.9085, Test Acc: 0.8102, Test f1: 0.8143, AUC: class-0>>0.9834806397306397|class-1>>0.9467371126660002\n",
      "Epoch: 132, Train Loss: 0.0345, Test Loss: 0.0694, Train Acc: 0.9085, Test Acc: 0.8102, Test f1: 0.8143, AUC: class-0>>0.984006734006734|class-1>>0.947308296444381\n",
      "Epoch: 133, Train Loss: 0.0344, Test Loss: 0.0705, Train Acc: 0.9035, Test Acc: 0.8102, Test f1: 0.8143, AUC: class-0>>0.983270202020202|class-1>>0.9483078680565472\n",
      "Epoch: 134, Train Loss: 0.0343, Test Loss: 0.0681, Train Acc: 0.9060, Test Acc: 0.8102, Test f1: 0.8143, AUC: class-0>>0.9837962962962963|class-1>>0.9461659288876196\n",
      "Epoch: 135, Train Loss: 0.0339, Test Loss: 0.0689, Train Acc: 0.9085, Test Acc: 0.8102, Test f1: 0.8143, AUC: class-0>>0.9836910774410774|class-1>>0.9474510923889762\n",
      "Epoch: 136, Train Loss: 0.0338, Test Loss: 0.0681, Train Acc: 0.9085, Test Acc: 0.8102, Test f1: 0.8143, AUC: class-0>>0.9839015151515151|class-1>>0.9467371126660004\n",
      "Epoch: 137, Train Loss: 0.0346, Test Loss: 0.0672, Train Acc: 0.9035, Test Acc: 0.8233, Test f1: 0.8315, AUC: class-0>>0.9837962962962963|class-1>>0.9450235613308583\n",
      "Epoch: 138, Train Loss: 0.0336, Test Loss: 0.0682, Train Acc: 0.9085, Test Acc: 0.8102, Test f1: 0.8143, AUC: class-0>>0.9841119528619529|class-1>>0.9478794802227617\n",
      "Epoch: 139, Train Loss: 0.0339, Test Loss: 0.0698, Train Acc: 0.9035, Test Acc: 0.8102, Test f1: 0.8143, AUC: class-0>>0.984006734006734|class-1>>0.9485934599457376\n",
      "Epoch: 140, Train Loss: 0.0343, Test Loss: 0.0701, Train Acc: 0.9049, Test Acc: 0.8102, Test f1: 0.8143, AUC: class-0>>0.9836910774410774|class-1>>0.9487362558903326\n",
      "Epoch: 141, Train Loss: 0.0335, Test Loss: 0.0688, Train Acc: 0.9085, Test Acc: 0.8102, Test f1: 0.8143, AUC: class-0>>0.984006734006734|class-1>>0.9485934599457376\n",
      "Epoch: 142, Train Loss: 0.0338, Test Loss: 0.0679, Train Acc: 0.9085, Test Acc: 0.8102, Test f1: 0.8143, AUC: class-0>>0.984006734006734|class-1>>0.9467371126660004\n",
      "Epoch: 143, Train Loss: 0.0334, Test Loss: 0.0683, Train Acc: 0.9125, Test Acc: 0.8102, Test f1: 0.8143, AUC: class-0>>0.9839015151515151|class-1>>0.9484506640011423\n",
      "Epoch: 144, Train Loss: 0.0335, Test Loss: 0.0677, Train Acc: 0.9085, Test Acc: 0.8102, Test f1: 0.8143, AUC: class-0>>0.9842171717171717|class-1>>0.9477366842781665\n",
      "Epoch: 145, Train Loss: 0.0335, Test Loss: 0.0684, Train Acc: 0.9111, Test Acc: 0.8102, Test f1: 0.8143, AUC: class-0>>0.9842171717171717|class-1>>0.9487362558903328\n",
      "Epoch: 146, Train Loss: 0.0333, Test Loss: 0.0681, Train Acc: 0.9085, Test Acc: 0.8102, Test f1: 0.8143, AUC: class-0>>0.9841119528619529|class-1>>0.9483078680565471\n",
      "Epoch: 147, Train Loss: 0.0336, Test Loss: 0.0693, Train Acc: 0.9085, Test Acc: 0.8102, Test f1: 0.8143, AUC: class-0>>0.9839015151515151|class-1>>0.9488790518349278\n",
      "Epoch: 148, Train Loss: 0.0332, Test Loss: 0.0682, Train Acc: 0.9136, Test Acc: 0.8102, Test f1: 0.8143, AUC: class-0>>0.9842171717171717|class-1>>0.949021847779523\n",
      "Epoch: 149, Train Loss: 0.0333, Test Loss: 0.0686, Train Acc: 0.9111, Test Acc: 0.8102, Test f1: 0.8143, AUC: class-0>>0.9841119528619529|class-1>>0.9493074396687134\n"
     ]
    }
   ],
   "source": [
    "torch.save(model.state_dict(), os.path.join(weights_path, 'init_distributed.pth'))\n",
    "for i in range(num_fold):\n",
    "    model.load_state_dict(torch.load(os.path.join(weights_path, 'init_distributed.pth')))\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, num_epochs, 0.00001)\n",
    "    optimal_score = 0\n",
    "    print(f'Running cross-validation fold: {i:02d}')\n",
    "    graph_train  = []\n",
    "    graph_test = []\n",
    "    for k in range(num_fold):\n",
    "        if k != i: graph_train += chunks_data[k]\n",
    "        else: graph_test = chunks_data[k]\n",
    "    n_sample = []\n",
    "    for graph in graph_train:\n",
    "        n_sample.append(graph.y.numpy())\n",
    "    n_sample = np.asarray(n_sample)\n",
    "    counts = np.count_nonzero(n_sample, axis=0)\n",
    "    counts = counts.max() / counts\n",
    "    # criterion = torch.nn.BCEWithLogitsLoss(pos_weight=torch.from_numpy(counts)).cuda()\n",
    "    # criterion = torch.nn.BCEWithLogitsLoss().cuda()    \n",
    "    criterion = nn.MSELoss()\n",
    "    for epoch in range(1, num_epochs):\n",
    "        shuffle(graph_train)\n",
    "        train(graph_train)\n",
    "        scheduler.step()    \n",
    "        train_loss, train_acc, _, _, _ = test(graph_train)\n",
    "        test_loss, test_acc, avg_prc, aucs, thresh = test(graph_test)\n",
    "        log_msg = f'Epoch: {epoch:03d}, Train Loss: {train_loss:.4f}, Test Loss: {test_loss:.4f}, Train Acc: {train_acc:.4f}, Test Acc: {test_acc:.4f}, Test f1: {avg_prc:.4f}' + ', AUC: ' + '|'.join('class-{}>>{}'.format(*k) for k in enumerate(aucs))\n",
    "        print(log_msg)\n",
    "        sum_metrics = (avg_prc+test_acc+sum(aucs))/3\n",
    "        if sum_metrics > optimal_score:\n",
    "            optimal_score = sum_metrics\n",
    "            os.makedirs(weights_path, exist_ok=True)\n",
    "            torch.save(model.state_dict(), os.path.join(weights_path, f'{weights_name}_fold{i:02d}.pth'))\n",
    "            print('Saved model!')\n",
    "            print('Best thresholds ===>>> '+ '|'.join('class-{}>>{}'.format(*k) for k in enumerate(thresh)))\n",
    "            with open(os.path.join(weights_path, f'{weights_name}_fold{i:02d}.txt'), \"w\") as text_file:\n",
    "                print('Best thresholds ===>>> '+ '|'.join('class-{}>>{}'.format(*k) for k in enumerate(thresh))+'|||'+log_msg, file=text_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f31d9168",
   "metadata": {},
   "source": [
    "### Read result summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "10904f65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation results of GCN_TMA_Distributed: Acc 0.8936749999999999, f1 0.9052, AUC PDAC 0.9913768154506976, AUC CP 0.9668670088533391\n"
     ]
    }
   ],
   "source": [
    "log_files = glob(os.path.join(weights_path, f'{weights_name}_fold*.txt'))\n",
    "results = []\n",
    "for log_file in log_files:\n",
    "    with open(log_file) as text_file:\n",
    "        lines = text_file.readlines()[0]\n",
    "        results.append([float(i) for i in re.findall(\"\\d+\\.\\d+\", lines)[3:]])\n",
    "results = np.vstack(results).mean(0)\n",
    "print(f'Cross-validation results of {weights_name}: Acc {results[2]}, f1 {results[3]}, AUC PDAC {results[4]}, AUC CP {results[5]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "21802e70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation results of MLP_TMA_Distributed: Acc 0.8529249999999999, f1 0.8712500000000001, AUC PDAC 0.9813363634568791, AUC CP 0.9538783336588694\n"
     ]
    }
   ],
   "source": [
    "log_files = glob(os.path.join(weights_path, f'{weights_name}_fold*.txt'))\n",
    "results = []\n",
    "for log_file in log_files:\n",
    "    with open(log_file) as text_file:\n",
    "        lines = text_file.readlines()[0]\n",
    "        results.append([float(i) for i in re.findall(\"\\d+\\.\\d+\", lines)[3:]])\n",
    "results = np.vstack(results).mean(0)\n",
    "print(f'Cross-validation results of {weights_name}: Acc {results[2]}, f1 {results[3]}, AUC PDAC {results[4]}, AUC CP {results[5]}')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "de88db708fd7569f2666ff37f921dc0f88a8459546879a55f00d65d0006c4c3e"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('pt')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
